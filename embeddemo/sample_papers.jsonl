{"search_keyword": "artificial language learning", "id": "2503.21676v2", "title": "How do language models learn facts? Dynamics, curricula and hallucinations", "abstract": "Large language models accumulate vast knowledge during pre-training, yet the dynamics governing this acquisition remain poorly understood. This work investigates the learning dynamics of language models on a synthetic factual recall task, uncovering three key findings: First, language models learn in three phases, exhibiting a performance plateau before acquiring precise factual knowledge. Mechanistically, this plateau coincides with the formation of attention-based circuits that support recall. Second, the training data distribution significantly impacts learning dynamics, as imbalanced distributions lead to shorter plateaus. Finally, hallucinations emerge simultaneously with knowledge, and integrating new knowledge into the model through fine-tuning is challenging, as it quickly corrupts its existing parametric memories. Our results emphasize the importance of data distribution in knowledge acquisition and suggest novel data scheduling strategies to accelerate neural network training.", "url": "https://arxiv.org/abs/2503.21676v2", "year": 2025, "categories": ["cs.CL", "cs.LG"]}
{"search_keyword": "artificial language learning", "id": "1212.2390v1", "title": "On the complexity of learning a language: An improvement of Block's algorithm", "abstract": "Language learning is thought to be a highly complex process. One of the hurdles in learning a language is to learn the rules of syntax of the language. Rules of syntax are often ordered in that before one rule can applied one must apply another. It has been thought that to learn the order of n rules one must go through all n! permutations. Thus to learn the order of 27 rules would require 27! steps or 1.08889x10^{28} steps. This number is much greater than the number of seconds since the beginning of the universe! In an insightful analysis the linguist Block ([Block 86], pp. 62-63, p.238) showed that with the assumption of transitivity this vast number of learning steps reduces to a mere 377 steps. We present a mathematical analysis of the complexity of Block's algorithm. The algorithm has a complexity of order n^2 given n rules. In addition, we improve Block's results exponentially, by introducing an algorithm that has complexity of order less than n log n.", "url": "https://arxiv.org/abs/1212.2390v1", "year": 2012, "categories": ["cs.CL", "cs.LG"]}
{"search_keyword": "artificial language learning", "id": "1504.06329v1", "title": "Analysis of Stopping Active Learning based on Stabilizing Predictions", "abstract": "Within the natural language processing (NLP) community, active learning has been widely investigated and applied in order to alleviate the annotation bottleneck faced by developers of new NLP systems and technologies. This paper presents the first theoretical analysis of stopping active learning based on stabilizing predictions (SP). The analysis has revealed three elements that are central to the success of the SP method: (1) bounds on Cohen's Kappa agreement between successively trained models impose bounds on differences in F-measure performance of the models; (2) since the stop set does not have to be labeled, it can be made large in practice, helping to guarantee that the results transfer to previously unseen streams of examples at test/application time; and (3) good (low variance) sample estimates of Kappa between successive models can be obtained. Proofs of relationships between the level of Kappa agreement and the difference in performance between consecutive models are presented. Specifically, if the Kappa agreement between two models exceeds a threshold T (where $T>0$), then the difference in F-measure performance between those models is bounded above by $\\frac{4(1-T)}{T}$ in all cases. If precision of the positive conjunction of the models is assumed to be $p$, then the bound can be tightened to $\\frac{4(1-T)}{(p+1)T}$.", "url": "https://arxiv.org/abs/1504.06329v1", "year": 2015, "categories": ["cs.LG", "cs.CL", "stat.ML"]}
{"search_keyword": "artificial language learning", "id": "2110.01831v1", "title": "The Artificial Scientist: Logicist, Emergentist, and Universalist Approaches to Artificial General Intelligence", "abstract": "We attempt to define what is necessary to construct an Artificial Scientist, explore and evaluate several approaches to artificial general intelligence (AGI) which may facilitate this, conclude that a unified or hybrid approach is necessary and explore two theories that satisfy this requirement to some degree.", "url": "https://arxiv.org/abs/2110.01831v1", "year": 2021, "categories": ["cs.AI"]}
{"search_keyword": "artificial language learning", "id": "1807.09844v2", "title": "Modular Mechanistic Networks: On Bridging Mechanistic and Phenomenological Models with Deep Neural Networks in Natural Language Processing", "abstract": "Natural language processing (NLP) can be done using either top-down (theory driven) and bottom-up (data driven) approaches, which we call mechanistic and phenomenological respectively. The approaches are frequently considered to stand in opposition to each other. Examining some recent approaches in deep learning we argue that deep neural networks incorporate both perspectives and, furthermore, that leveraging this aspect of deep learning may help in solving complex problems within language technology, such as modelling language and perception in the domain of spatial cognition.", "url": "https://arxiv.org/abs/1807.09844v2", "year": 2018, "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.NE", "stat.ML"]}
{"search_keyword": "artificial language learning", "id": "2410.08355v3", "title": "Metalic: Meta-Learning In-Context with Protein Language Models", "abstract": "Predicting the biophysical and functional properties of proteins is essential for in silico protein design. Machine learning has emerged as a promising technique for such prediction tasks. However, the relative scarcity of in vitro annotations means that these models often have little, or no, specific data on the desired fitness prediction task. As a result of limited data, protein language models (PLMs) are typically trained on general protein sequence modeling tasks, and then fine-tuned, or applied zero-shot, to protein fitness prediction. When no task data is available, the models make strong assumptions about the correlation between the protein sequence likelihood and fitness scores. In contrast, we propose meta-learning over a distribution of standard fitness prediction tasks, and demonstrate positive transfer to unseen fitness prediction tasks. Our method, called Metalic (Meta-Learning In-Context), uses in-context learning and fine-tuning, when data is available, to adapt to new tasks. Crucially, fine-tuning enables considerable generalization, even though it is not accounted for during meta-training. Our fine-tuned models achieve strong results with 18 times fewer parameters than state-of-the-art models. Moreover, our method sets a new state-of-the-art in low-data settings on ProteinGym, an established fitness-prediction benchmark. Due to data scarcity, we believe meta-learning will play a pivotal role in advancing protein engineering.", "url": "https://arxiv.org/abs/2410.08355v3", "year": 2024, "categories": ["cs.LG"]}
{"search_keyword": "artificial language learning", "id": "1610.00031v1", "title": "Discriminating Similar Languages: Evaluations and Explorations", "abstract": "We present an analysis of the performance of machine learning classifiers on discriminating between similar languages and language varieties. We carried out a number of experiments using the results of the two editions of the Discriminating between Similar Languages (DSL) shared task. We investigate the progress made between the two tasks, estimate an upper bound on possible performance using ensemble and oracle combination, and provide learning curves to help us understand which languages are more challenging. A number of difficult sentences are identified and investigated further with human annotation.", "url": "https://arxiv.org/abs/1610.00031v1", "year": 2016, "categories": ["cs.CL"]}
{"search_keyword": "artificial language learning", "id": "2304.10464v4", "title": "Learning to Plan with Natural Language", "abstract": "Large Language Models (LLMs) have shown remarkable performance in various basic natural language tasks. For completing the complex task, we still need a plan for the task to guide LLMs to generate the specific solutions step by step. LLMs can directly generate task plans, but these plans may still contain factual errors or are incomplete. A high-quality task plan contains correct step-by-step solutions for solving all situations and behavioral instructions for avoiding mistakes. To obtain it, we propose the Learning to Plan method, which involves two phases: (1) In the first learning task plan phase, it iteratively updates the task plan with new step-by-step solutions and behavioral instructions, which are obtained by prompting LLMs to derive from training error feedback. (2) In the subsequent test phase, the LLM uses the learned task plan to guide the inference of LLM on the test set. We demonstrate the effectiveness of our method on the five different reasoning type tasks (8 datasets). Further, our analysis experiment shows that the task plan learned by one LLM can directly guide another LLM to improve its performance, which reveals a new transfer learning paradigm. We release the code at \\url{https://github.com/Eureka6174/LearnNLPlan}", "url": "https://arxiv.org/abs/2304.10464v4", "year": 2023, "categories": ["cs.CL"]}
{"search_keyword": "artificial language learning", "id": "2007.09774v1", "title": "An Overview of Natural Language State Representation for Reinforcement Learning", "abstract": "A suitable state representation is a fundamental part of the learning process in Reinforcement Learning. In various tasks, the state can either be described by natural language or be natural language itself. This survey outlines the strategies used in the literature to build natural language state representations. We appeal for more linguistically interpretable and grounded representations, careful justification of design decisions and evaluation of the effectiveness of different approaches.", "url": "https://arxiv.org/abs/2007.09774v1", "year": 2020, "categories": ["cs.CL"]}
{"search_keyword": "artificial language learning", "id": "cs/9812021v1", "title": "Forgetting Exceptions is Harmful in Language Learning", "abstract": "We show that in language learning, contrary to received wisdom, keeping exceptional training instances in memory can be beneficial for generalization accuracy. We investigate this phenomenon empirically on a selection of benchmark natural language processing tasks: grapheme-to-phoneme conversion, part-of-speech tagging, prepositional-phrase attachment, and base noun phrase chunking. In a first series of experiments we combine memory-based learning with training set editing techniques, in which instances are edited based on their typicality and class prediction strength. Results show that editing exceptional instances (with low typicality or low class prediction strength) tends to harm generalization accuracy. In a second series of experiments we compare memory-based learning and decision-tree learning methods on the same selection of tasks, and find that decision-tree learning often performs worse than memory-based learning. Moreover, the decrease in performance can be linked to the degree of abstraction from exceptions (i.e., pruning or eagerness). We provide explanations for both results in terms of the properties of the natural language processing tasks and the learning algorithms.", "url": "https://arxiv.org/abs/cs/9812021v1", "year": 1998, "categories": ["cs.CL", "cs.LG"]}
{"search_keyword": "memory encoding", "id": "1509.00070v1", "title": "In-Line-Test of Variability and Bit-Error-Rate of HfOx-Based Resistive Memory", "abstract": "Spatial and temporal variability of HfOx-based resistive random access memory (RRAM) are investigated for manufacturing and product designs. Manufacturing variability is characterized at different levels including lots, wafers, and chips. Bit-error-rate (BER) is proposed as a holistic parameter for the write cycle resistance statistics. Using the electrical in-line-test cycle data, a method is developed to derive BERs as functions of the design margin, to provide guidance for technology evaluation and product design. The proposed BER calculation can also be used in the off-line bench test and build-in-self-test (BIST) for adaptive error correction and for the other types of random access memories.", "url": "https://arxiv.org/abs/1509.00070v1", "year": 2015, "categories": ["cs.ET", "cond-mat.mtrl-sci", "physics.data-an"]}
{"search_keyword": "memory encoding", "id": "1308.2881v1", "title": "Opacity of Memory Management in Software Transactional Memory", "abstract": "Opacity of Transactional Memory is proposed to be established by incremental validation. Quiescence in terms of epoch-based memory reclamation is applied to deal with doomed transactions causing memory access violations. This method unfortunately involves increased memory consumption and does not cover reclamations outside of transactions. This paper introduces a different method which combines incremental validation with elements of sandboxing to solve these issues.", "url": "https://arxiv.org/abs/1308.2881v1", "year": 2013, "categories": ["cs.DC", "cs.OS"]}
{"search_keyword": "memory encoding", "id": "2211.02987v1", "title": "Differentiable Neural Computers with Memory Demon", "abstract": "A Differentiable Neural Computer (DNC) is a neural network with an external memory which allows for iterative content modification via read, write and delete operations. We show that information theoretic properties of the memory contents play an important role in the performance of such architectures. We introduce a novel concept of memory demon to DNC architectures which modifies the memory contents implicitly via additive input encoding. The goal of the memory demon is to maximize the expected sum of mutual information of the consecutive external memory contents.", "url": "https://arxiv.org/abs/2211.02987v1", "year": 2022, "categories": ["cs.LG", "cs.NE"]}
{"search_keyword": "memory encoding", "id": "1002.1191v1", "title": "Unidirectional Error Correcting Codes for Memory Systems: A Comparative Study", "abstract": "In order to achieve fault tolerance, highly reliable system often require the ability to detect errors as soon as they occur and prevent the speared of erroneous information throughout the system. Thus, the need for codes capable of detecting and correcting byte errors are extremely important since many memory systems use b-bit-per-chip organization. Redundancy on the chip must be put to make fault-tolerant design available. This paper examined several methods of computer memory systems, and then a proposed technique is designed to choose a suitable method depending on the organization of memory systems. The constructed codes require a minimum number of check bits with respect to codes used previously, then it is optimized to fit the organization of memory systems according to the requirements for data and byte lengths.", "url": "https://arxiv.org/abs/1002.1191v1", "year": 2010, "categories": ["cs.IT"]}
{"search_keyword": "memory encoding", "id": "1508.01127v1", "title": "Encoding CSP into CCS (Extended Version)", "abstract": "We study encodings from CSP into asynchronous CCS with name passing and matching, so in fact, the asynchronous pi-calculus. By doing so, we discuss two different ways to map the multi-way synchronisation mechanism of CSP into the two-way synchronisation mechanism of CCS. Both encodings satisfy the criteria of Gorla except for compositionality, as both use an additional top-level context. Following the work of Parrow and Sj√∂din, the first encoding uses a central coordinator and establishes a variant of weak bisimilarity between source terms and their translations. The second encoding is decentralised, and thus more efficient, but ensures only a form of coupled similarity between source terms and their translations.", "url": "https://arxiv.org/abs/1508.01127v1", "year": 2015, "categories": ["cs.LO"]}
{"search_keyword": "memory encoding", "id": "2004.07997v2", "title": "Random Memory Walk", "abstract": "We present a simple model of a random walk with partial memory, which we call the \\emph{random memory walk}. We introduce this model motivated by the belief that it mimics the behavior of the once-reinforced random walk in high dimensions and with small reinforcement. We establish the transience of the random memory walk in dimensions three and higher, and show that its scaling limit is a Brownian motion.", "url": "https://arxiv.org/abs/2004.07997v2", "year": 2020, "categories": ["math.PR"]}
{"search_keyword": "memory encoding", "id": "1405.2846v7", "title": "Introduction to Dynamic Unary Encoding", "abstract": "Dynamic unary encoding takes unary encoding to the next level. Every n-bit binary string is an encoding of dynamic unary and every n-bit binary string is encodable by dynamic unary. By utilizing both forms of unary code and a single bit of parity information dynamic unary encoding partitions 2^n non-negative integers into n sets of disjoint cycles of n-bit elements. These cycles have been employed as virtual data sets, binary transforms and as a mathematical object. Characterization of both the cycles and of the cycle spectrum is given. Examples of encoding and decoding algorithms are given. Examples of other constructs utilizing the principles of dynamic unary encoding are presented. The cycle as a mathematical object is demonstrated.", "url": "https://arxiv.org/abs/1405.2846v7", "year": 2014, "categories": ["cs.IT", "cs.DS"]}
{"search_keyword": "memory encoding", "id": "quant-ph/0506054v2", "title": "Improvement of stabilizer based entanglement distillation protocols by encoding operators", "abstract": "This paper presents a method for enumerating all encoding operators in the Clifford group for a given stabilizer. Furthermore, we classify encoding operators into the equivalence classes such that EDPs (Entanglement Distillation Protocol) constructed from encoding operators in the same equivalence class have the same performance. By this classification, for a given parameter, the number of candidates for good EDPs is significantly reduced. As a result, we find the best EDP among EDPs constructed from [[4,2]] stabilizer codes. This EDP has a better performance than previously known EDPs over wide range of fidelity.", "url": "https://arxiv.org/abs/quant-ph/0506054v2", "year": 2005, "categories": ["quant-ph"]}
{"search_keyword": "memory encoding", "id": "2308.01175v1", "title": "Memory Encoding Model", "abstract": "We explore a new class of brain encoding model by adding memory-related information as input. Memory is an essential brain mechanism that works alongside visual stimuli. During a vision-memory cognitive task, we found the non-visual brain is largely predictable using previously seen images. Our Memory Encoding Model (Mem) won the Algonauts 2023 visual brain competition even without model ensemble (single model score 66.8, ensemble score 70.8). Our ensemble model without memory input (61.4) can also stand a 3rd place. Furthermore, we observe periodic delayed brain response correlated to 6th-7th prior image, and hippocampus also showed correlated activity timed with this periodicity. We conjuncture that the periodic replay could be related to memory mechanism to enhance the working memory.", "url": "https://arxiv.org/abs/2308.01175v1", "year": 2023, "categories": ["cs.CV"]}
{"search_keyword": "memory encoding", "id": "1308.6782v1", "title": "Graphene Nanotechnology for the Next Generation Nonvolatile Memory", "abstract": "As conventional silicon technology is approaching its fundamental material and physical limits with continuous scaling, there is a growing push to look for new platform to design memory circuits for nanoelectronic applications. In this paper we explore new design concept of nonvolatile memory based on graphene nanotechnology. The investigation focuses on two forms of graphene based field effect transistor (FET) carbon nanotube FET (CNTFET) and graphene nanoribbon FET (GNRFET). The analysis reveals that GNRFET with a high trapping capable oxide layer is suitable for ultra high ensity nonvolatile memory.", "url": "https://arxiv.org/abs/1308.6782v1", "year": 2013, "categories": ["cond-mat.mes-hall"]}
{"search_keyword": "language acquisition", "id": "2503.21676v2", "title": "How do language models learn facts? Dynamics, curricula and hallucinations", "abstract": "Large language models accumulate vast knowledge during pre-training, yet the dynamics governing this acquisition remain poorly understood. This work investigates the learning dynamics of language models on a synthetic factual recall task, uncovering three key findings: First, language models learn in three phases, exhibiting a performance plateau before acquiring precise factual knowledge. Mechanistically, this plateau coincides with the formation of attention-based circuits that support recall. Second, the training data distribution significantly impacts learning dynamics, as imbalanced distributions lead to shorter plateaus. Finally, hallucinations emerge simultaneously with knowledge, and integrating new knowledge into the model through fine-tuning is challenging, as it quickly corrupts its existing parametric memories. Our results emphasize the importance of data distribution in knowledge acquisition and suggest novel data scheduling strategies to accelerate neural network training.", "url": "https://arxiv.org/abs/2503.21676v2", "year": 2025, "categories": ["cs.CL", "cs.LG"]}
{"search_keyword": "language acquisition", "id": "1610.00031v1", "title": "Discriminating Similar Languages: Evaluations and Explorations", "abstract": "We present an analysis of the performance of machine learning classifiers on discriminating between similar languages and language varieties. We carried out a number of experiments using the results of the two editions of the Discriminating between Similar Languages (DSL) shared task. We investigate the progress made between the two tasks, estimate an upper bound on possible performance using ensemble and oracle combination, and provide learning curves to help us understand which languages are more challenging. A number of difficult sentences are identified and investigated further with human annotation.", "url": "https://arxiv.org/abs/1610.00031v1", "year": 2016, "categories": ["cs.CL"]}
{"search_keyword": "language acquisition", "id": "1501.03191v1", "title": "Annotating Cognates and Etymological Origin in Turkic Languages", "abstract": "Turkic languages exhibit extensive and diverse etymological relationships among lexical items. These relationships make the Turkic languages promising for exploring automated translation lexicon induction by leveraging cognate and other etymological information. However, due to the extent and diversity of the types of relationships between words, it is not clear how to annotate such information. In this paper, we present a methodology for annotating cognates and etymological origin in Turkic languages. Our method strives to balance the amount of research effort the annotator expends with the utility of the annotations for supporting research on improving automated translation lexicon induction.", "url": "https://arxiv.org/abs/1501.03191v1", "year": 2015, "categories": ["cs.CL"]}
{"search_keyword": "language acquisition", "id": "2409.02228v1", "title": "Unforgettable Generalization in Language Models", "abstract": "When language models (LMs) are trained to forget (or \"unlearn'') a skill, how precisely does their behavior change? We study the behavior of transformer LMs in which tasks have been forgotten via fine-tuning on randomized labels. Such LMs learn to generate near-random predictions for individual examples in the \"training'' set used for forgetting. Across tasks, however, LMs exhibit extreme variability in whether LM predictions change on examples outside the training set. In some tasks (like entailment classification), forgetting generalizes robustly, and causes models to produce uninformative predictions on new task instances; in other tasks (like physical commonsense reasoning and scientific question answering) forgetting affects only the training examples, and models continue to perform the \"forgotten'' task accurately even for examples very similar to those that appeared in the training set. Dataset difficulty is not predictive of whether a behavior can be forgotten; instead, generalization in forgetting is (weakly) predicted by the confidence of LMs' initial task predictions and the variability of LM representations of training data, with low confidence and low variability both associated with greater generalization. Perhaps most surprisingly, random-label forgetting appears to be somewhat insensitive to the contents of the training set: for example, models trained on science questions with random labels continue to answer other science questions accurately, but begin to produce random labels on entailment classification tasks. Finally, we show that even generalizable forgetting is shallow: linear probes trained on LMs' representations can still perform tasks reliably after forgetting. Our results highlight the difficulty and unpredictability of performing targeted skill removal from models via fine-tuning.", "url": "https://arxiv.org/abs/2409.02228v1", "year": 2024, "categories": ["cs.LG", "cs.CL"]}
{"search_keyword": "language acquisition", "id": "1610.00030v1", "title": "Modeling Language Change in Historical Corpora: The Case of Portuguese", "abstract": "This paper presents a number of experiments to model changes in a historical Portuguese corpus composed of literary texts for the purpose of temporal text classification. Algorithms were trained to classify texts with respect to their publication date taking into account lexical variation represented as word n-grams, and morphosyntactic variation represented by part-of-speech (POS) distribution. We report results of 99.8% accuracy using word unigram features with a Support Vector Machines classifier to predict the publication date of documents in time intervals of both one century and half a century. A feature analysis is performed to investigate the most informative features for this task and how they are linked to language change.", "url": "https://arxiv.org/abs/1610.00030v1", "year": 2016, "categories": ["cs.CL"]}
{"search_keyword": "language acquisition", "id": "2105.03943v2", "title": "gComm: An environment for investigating generalization in Grounded Language Acquisition", "abstract": "gComm is a step towards developing a robust platform to foster research in grounded language acquisition in a more challenging and realistic setting. It comprises a 2-d grid environment with a set of agents (a stationary speaker and a mobile listener connected via a communication channel) exposed to a continuous array of tasks in a partially observable setting. The key to solving these tasks lies in agents developing linguistic abilities and utilizing them for efficiently exploring the environment. The speaker and listener have access to information provided in different modalities, i.e. the speaker's input is a natural language instruction that contains the target and task specifications and the listener's input is its grid-view. Each must rely on the other to complete the assigned task, however, the only way they can achieve the same, is to develop and use some form of communication. gComm provides several tools for studying different forms of communication and assessing their generalization.", "url": "https://arxiv.org/abs/2105.03943v2", "year": 2021, "categories": ["cs.CL", "cs.AI"]}
{"search_keyword": "language acquisition", "id": "2307.14850v6", "title": "Turkish Native Language Identification V2", "abstract": "This paper presents the first application of Native Language Identification (NLI) for the Turkish language. NLI is the task of automatically identifying an individual's native language (L1) based on their writing or speech in a non-native language (L2). While most NLI research has focused on L2 English, our study extends this scope to L2 Turkish by analyzing a corpus of texts written by native speakers of Albanian, Arabic and Persian. We leverage a cleaned version of the Turkish Learner Corpus and demonstrate the effectiveness of syntactic features, comparing a structural Part-of-Speech n-gram model to a hybrid model that retains function words. Our models achieve promising results, and we analyze the most predictive features to reveal L1-specific transfer effects. We make our data and code publicly available for further study.", "url": "https://arxiv.org/abs/2307.14850v6", "year": 2023, "categories": ["cs.CL"]}
{"search_keyword": "language acquisition", "id": "cs/0112004v1", "title": "Part of Speech Tagging in Thai Language Using Support Vector Machine", "abstract": "The elastic-input neuro tagger and hybrid tagger, combined with a neural network and Brill's error-driven learning, have already been proposed for the purpose of constructing a practical tagger using as little training data as possible. When a small Thai corpus is used for training, these taggers have tagging accuracies of 94.4% and 95.5% (accounting only for the ambiguous words in terms of the part of speech), respectively. In this study, in order to construct more accurate taggers we developed new tagging methods using three machine learning methods: the decision-list, maximum entropy, and support vector machine methods. We then performed tagging experiments by using these methods. Our results showed that the support vector machine method has the best precision (96.1%), and that it is capable of improving the accuracy of tagging in the Thai language. Finally, we theoretically examined all these methods and discussed how the improvements were achived.", "url": "https://arxiv.org/abs/cs/0112004v1", "year": 2001, "categories": ["cs.CL"]}
{"search_keyword": "language acquisition", "id": "2403.17811v1", "title": "Are Compressed Language Models Less Subgroup Robust?", "abstract": "To reduce the inference cost of large language models, model compression is increasingly used to create smaller scalable models. However, little is known about their robustness to minority subgroups defined by the labels and attributes of a dataset. In this paper, we investigate the effects of 18 different compression methods and settings on the subgroup robustness of BERT language models. We show that worst-group performance does not depend on model size alone, but also on the compression method used. Additionally, we find that model compression does not always worsen the performance on minority subgroups. Altogether, our analysis serves to further research into the subgroup robustness of model compression.", "url": "https://arxiv.org/abs/2403.17811v1", "year": 2024, "categories": ["cs.LG", "cs.CL"]}
{"search_keyword": "language acquisition", "id": "2402.15010v2", "title": "How Important Is Tokenization in French Medical Masked Language Models?", "abstract": "Subword tokenization has become the prevailing standard in the field of natural language processing (NLP) over recent years, primarily due to the widespread utilization of pre-trained language models. This shift began with Byte-Pair Encoding (BPE) and was later followed by the adoption of SentencePiece and WordPiece. While subword tokenization consistently outperforms character and word-level tokenization, the precise factors contributing to its success remain unclear. Key aspects such as the optimal segmentation granularity for diverse tasks and languages, the influence of data sources on tokenizers, and the role of morphological information in Indo-European languages remain insufficiently explored. This is particularly pertinent for biomedical terminology, characterized by specific rules governing morpheme combinations. Despite the agglutinative nature of biomedical terminology, existing language models do not explicitly incorporate this knowledge, leading to inconsistent tokenization strategies for common terms. In this paper, we seek to delve into the complexities of subword tokenization in French biomedical domain across a variety of NLP tasks and pinpoint areas where further enhancements can be made. We analyze classical tokenization algorithms, including BPE and SentencePiece, and introduce an original tokenization strategy that integrates morpheme-enriched word segmentation into existing tokenization methods.", "url": "https://arxiv.org/abs/2402.15010v2", "year": 2024, "categories": ["cs.CL", "cs.AI", "cs.LG"]}
{"search_keyword": "cognitive load", "id": "1002.3035v1", "title": "Cognition and Emotion: Perspectives of a Closing Gap", "abstract": "The primary tasks of a cognitive system is to survive and to maximize a life-long utility function, like the number of offsprings. A direct computational maximization of life-long utility is however not possible in complex environments, especially in the context, of real-world time constraints. The central role of emotions is to serve as an intermediate layer in the space of policies available to agents and animals, leading to a large dimensional reduction of complexity. We review our current understanding of the functional role of emotions, stressing the role of the neuromodulators mediating emotions for the diffusive homeostatic control system of the brain. We discuss a recent proposal, that emotional diffusive control is characterized, in contrast to neutral diffusive control, by interaction effects, viz by interferences between emotional arousal and reward signaling. Several proposals for the realization of synthetic emotions are discussed in this context, together with key open issues regarding the interplay between emotional motivational drives and diffusive control.", "url": "https://arxiv.org/abs/1002.3035v1", "year": 2010, "categories": ["q-bio.NC"]}
{"search_keyword": "cognitive load", "id": "0901.3028v1", "title": "Cognitive computation with autonomously active neural networks: an emerging field", "abstract": "The human brain is autonomously active. To understand the functional role of this self-sustained neural activity, and its interplay with the sensory data input stream, is an important question in cognitive system research and we review here the present state of theoretical modelling. This review will start with a brief overview of the experimental efforts, together with a discussion of transient vs. self-sustained neural activity in the framework of reservoir computing. The main emphasis will be then on two paradigmal neural network architectures showing continuously ongoing transient-state dynamics: saddle point networks and networks of attractor relics. Self-active neural networks are confronted with two seemingly contrasting demands: a stable internal dynamical state and sensitivity to incoming stimuli. We show, that this dilemma can be solved by networks of attractor relics based on competitive neural dynamics, where the attractor relics compete on one side with each other for transient dominance, and on the other side with the dynamical influence of the input signals. Unsupervised and local Hebbian-style online learning then allows the system to build up correlations between the internal dynamical transient states and the sensory input stream. An emergent cognitive capability results from this set-up. The system performs online, and on its own, a non-linear independent component analysis of the sensory data stream, all the time being continuously and autonomously active. This process maps the independent components of the sensory input onto the attractor relics, which acquire in this way a semantic meaning.", "url": "https://arxiv.org/abs/0901.3028v1", "year": 2009, "categories": ["q-bio.NC"]}
{"search_keyword": "cognitive load", "id": "1801.08829v2", "title": "Symbol Emergence in Cognitive Developmental Systems: a Survey", "abstract": "Humans use signs, e.g., sentences in a spoken language, for communication and thought. Hence, symbol systems like language are crucial for our communication with other agents and adaptation to our real-world environment. The symbol systems we use in our human society adaptively and dynamically change over time. In the context of artificial intelligence (AI) and cognitive systems, the symbol grounding problem has been regarded as one of the central problems related to {\\it symbols}. However, the symbol grounding problem was originally posed to connect symbolic AI and sensorimotor information and did not consider many interdisciplinary phenomena in human communication and dynamic symbol systems in our society, which semiotics considered. In this paper, we focus on the symbol emergence problem, addressing not only cognitive dynamics but also the dynamics of symbol systems in society, rather than the symbol grounding problem. We first introduce the notion of a symbol in semiotics from the humanities, to leave the very narrow idea of symbols in symbolic AI. Furthermore, over the years, it became more and more clear that symbol emergence has to be regarded as a multifaceted problem. Therefore, secondly, we review the history of the symbol emergence problem in different fields, including both biological and artificial systems, showing their mutual relations. We summarize the discussion and provide an integrative viewpoint and comprehensive overview of symbol emergence in cognitive systems. Additionally, we describe the challenges facing the creation of cognitive systems that can be part of symbol emergence systems.", "url": "https://arxiv.org/abs/1801.08829v2", "year": 2018, "categories": ["cs.AI", "cs.RO"]}
{"search_keyword": "cognitive load", "id": "1110.6589v1", "title": "A cognitive diversity framework for radar target classification", "abstract": "Classification of targets by radar has proved to be notoriously difficult with the best systems still yet to attain sufficiently high levels of performance and reliability. In the current contribution we explore a new design of radar based target recognition, where angular diversity is used in a cognitive manner to attain better performance. Performance is bench- marked against conventional classification schemes. The proposed scheme can easily be extended to cognitive target recognition based on multiple diversity strategies.", "url": "https://arxiv.org/abs/1110.6589v1", "year": 2011, "categories": ["cs.AI"]}
{"search_keyword": "cognitive load", "id": "2111.12860v8", "title": "Applied Exoskeleton Technology: A Comprehensive Review of Physical and Cognitive Human-Robot Interaction", "abstract": "Exoskeletons and orthoses are wearable mobile systems providing mechanical benefits to the users. Despite significant improvements in the last decades, the technology is not fully mature to be adopted for strenuous and non-programmed tasks. To accommodate this insufficiency, different aspects of this technology need to be analysed and improved. Numerous studies have tried to address some aspects of exoskeletons, e.g. mechanism design, intent prediction, and control scheme. However, most works have focused on a specific element of design or application without providing a comprehensive review framework. This study aims to analyse and survey the contributing aspects to this technology's improvement and broad adoption. To address this, after introducing assistive devices and exoskeletons, the main design criteria will be investigated from both physical Human-Robot Interaction (HRI) perspectives. In order to establish an intelligent HRI strategy and enable intuitive control for users, cognitive HRI will be investigated after a brief introduction to various approaches to their control strategies. The study will be further developed by outlining several examples of known assistive devices in different categories. And some guidelines for exoskeleton selection and possible mitigation of current limitations will be discussed.", "url": "https://arxiv.org/abs/2111.12860v8", "year": 2021, "categories": ["cs.RO", "cs.HC"]}
{"search_keyword": "cognitive load", "id": "2404.17098v2", "title": "CLARE: Cognitive Load Assessment in REaltime with Multimodal Data", "abstract": "We present a novel multimodal dataset for Cognitive Load Assessment in REal-time (CLARE). The dataset contains physiological and gaze data from 24 participants with self-reported cognitive load scores as ground-truth labels. The dataset consists of four modalities, namely, Electrocardiography (ECG), Electrodermal Activity (EDA), Electroencephalogram (EEG), and Gaze tracking. To map diverse levels of mental load on participants during experiments, each participant completed four nine-minutes sessions on a computer-based operator performance and mental workload task (the MATB-II software) with varying levels of complexity in one minute segments. During the experiment, participants reported their cognitive load every 10 seconds. For the dataset, we also provide benchmark binary classification results with machine learning and deep learning models on two different evaluation schemes, namely, 10-fold and leave-one-subject-out (LOSO) cross-validation. Benchmark results show that for 10-fold evaluation, the convolutional neural network (CNN) based deep learning model achieves the best classification performance with ECG, EDA, and Gaze. In contrast, for LOSO, the best performance is achieved by the deep learning model with ECG, EDA, and EEG.", "url": "https://arxiv.org/abs/2404.17098v2", "year": 2024, "categories": ["cs.HC", "cs.AI"]}
{"search_keyword": "cognitive load", "id": "cs/0412059v1", "title": "Vector Symbolic Architectures answer Jackendoff's challenges for cognitive neuroscience", "abstract": "Jackendoff (2002) posed four challenges that linguistic combinatoriality and rules of language present to theories of brain function. The essence of these problems is the question of how to neurally instantiate the rapid construction and transformation of the compositional structures that are typically taken to be the domain of symbolic processing. He contended that typical connectionist approaches fail to meet these challenges and that the dialogue between linguistic theory and cognitive neuroscience will be relatively unproductive until the importance of these problems is widely recognised and the challenges answered by some technical innovation in connectionist modelling. This paper claims that a little-known family of connectionist models (Vector Symbolic Architectures) are able to meet Jackendoff's challenges.", "url": "https://arxiv.org/abs/cs/0412059v1", "year": 2004, "categories": ["cs.NE", "cs.AI"]}
{"search_keyword": "cognitive load", "id": "1706.03661v2", "title": "DAC-h3: A Proactive Robot Cognitive Architecture to Acquire and Express Knowledge About the World and the Self", "abstract": "This paper introduces a cognitive architecture for a humanoid robot to engage in a proactive, mixed-initiative exploration and manipulation of its environment, where the initiative can originate from both the human and the robot. The framework, based on a biologically-grounded theory of the brain and mind, integrates a reactive interaction engine, a number of state-of-the-art perceptual and motor learning algorithms, as well as planning abilities and an autobiographical memory. The architecture as a whole drives the robot behavior to solve the symbol grounding problem, acquire language capabilities, execute goal-oriented behavior, and express a verbal narrative of its own experience in the world. We validate our approach in human-robot interaction experiments with the iCub humanoid robot, showing that the proposed cognitive architecture can be applied in real time within a realistic scenario and that it can be used with naive users.", "url": "https://arxiv.org/abs/1706.03661v2", "year": 2017, "categories": ["cs.AI", "cs.RO"]}
{"search_keyword": "cognitive load", "id": "2305.09091v2", "title": "AAAI 2022 Fall Symposium: System-1 and System-2 realized within the Common Model of Cognition", "abstract": "Attempts to import dual-system descriptions of System-1 and System-2 into AI have been hindered by a lack of clarity over their distinction. We address this and other issues by situating System-1 and System-2 within the Common Model of Cognition. Results show that what are thought to be distinctive characteristics of System-1 and 2 instead form a spectrum of cognitive properties. The Common Model provides a comprehensive vision of the computational units involved in System-1 and System-2, their underlying mechanisms, and the implications for learning, metacognition, and emotion.", "url": "https://arxiv.org/abs/2305.09091v2", "year": 2023, "categories": ["cs.AI"]}
{"search_keyword": "cognitive load", "id": "2004.04595v4", "title": "Robust Beamforming Design for Intelligent Reflecting Surface Aided Cognitive Radio Systems with Imperfect Cascaded CSI", "abstract": "In this paper, intelligent reflecting surface (IRS) is introduced to enhance the network performance of cognitive radio (CR) systems. Specifically, we investigate robust beamforming design based on both bounded channel state information (CSI) error model and statistical CSI error model for primary user (PU)-related channels in IRS-aided CR systems. We jointly optimize the transmit precoding (TPC) at the secondary user (SU) transmitter (ST) and phase shifts at the IRS to minimize the ST' s total transmit power subject to the quality of service of SUs, the limited interference imposed on the PU and unit-modulus of the reflective beamforming. The successive convex approximation (SCA) method, Schur's complement, General sign-definiteness principle, inverse Chi-square distribution and penalty convex-concave procedure are invoked for dealing with these intricate constraints. The non-convex optimization problems are transformed into several convex subproblems and efficient algorithms are proposed. Simulation results verify the efficiency of the proposed algorithms and reveal the impacts of CSI uncertainties on ST's minimum transmit power and feasibility rate of the optimization problems. Simulation results also show that the number of transmit antennas at the ST and the number of phase shifts at the IRS should be carefully chosen to balance the channel realization feasibility rate and the total transmit power.", "url": "https://arxiv.org/abs/2004.04595v4", "year": 2020, "categories": ["eess.SP"]}
{"search_keyword": "experimental psychology", "id": "1711.01767v1", "title": "Kurt Lewin, psychological constructs and sources of brain cognitive activity", "abstract": "Understanding mind-brain-environment relations is one of the key topics in psychology. Kurt Lewin, inspired by theoretical physics, tried to establish topological and vector psychology analyzing patterns of interaction between the individual and her/his environment. The time is ripe to reformulate his ambitious goals, searching for ways to interpret objectively measured brain processes in terms of suitable psychological constructs. Connecting cognitive and social psychology constructs to neurophenomics, as it is done now in psychiatry, should ground them in physical reality.", "url": "https://arxiv.org/abs/1711.01767v1", "year": 2017, "categories": ["q-bio.NC"]}
{"search_keyword": "experimental psychology", "id": "2009.13758v1", "title": "How do Visualization Designers Think? Design Cognition as a Core Aspect of Visualization Psychology", "abstract": "There are numerous opportunities for engaging in research at the intersection of psychology and visualization. While most opportunities taken up by the VIS community will likely focus on the psychology of users, there are also opportunities for studying the psychology of designers. In this position paper, I argue the importance of studying design cognition as a necessary component of a holistic program of research on visualization psychology. I provide a brief overview of research on design cognition in other disciplines, and discuss opportunities for VIS to build an analogous research program. Doing so can lead to a stronger integration of research and design practice, can provide a better understanding of how to educate and train future designers, and will likely surface both challenges and opportunities for future research.", "url": "https://arxiv.org/abs/2009.13758v1", "year": 2020, "categories": ["cs.HC"]}
{"search_keyword": "experimental psychology", "id": "2010.00488v1", "title": "Developing Effective Community Network Analysis Tools According to Visualization Psychology", "abstract": "Visualization is a useful technology in health science, and especially for community network analysis. Because visualization applications in healthcare are typically risk-averse, health psychologists can play a significant role in ensuring appropriate and effective uses of visualization techniques in healthcare. In this paper, we examine the role of health psychologists in the triangle of \"health science\", \"visualization technology\", and \"visualization psychology\". We conclude that health psychologists can use visualization to aid data intelligence workflows in healthcare and health psychology, while researching into visualization psychology to aid the improvement and optimization of data visualization processes.", "url": "https://arxiv.org/abs/2010.00488v1", "year": 2020, "categories": ["cs.HC", "cs.SI"]}
{"search_keyword": "experimental psychology", "id": "2208.12946v2", "title": "Nondistributivity of human logic and violation of response replicability effect in cognitive psychology", "abstract": "The aim of this paper is to promote quantum logic as one of the basic tools for analyzing human reasoning. We compare it with classical (Boolean) logic and highlight the role of violation of the distributive law for conjunction and disjunction. It is well known that nondistributivity is equivalent to incompatibility of logical variables -- the impossibility to assign jointly the two-valued truth values to these variables. A natural question arises as to whether quantum logical nondistributivity in human logic can be tested experimentally. We show that testing the response replicability effect (RRE) in cognitive psychology is equivalent to testing nondistributivity -- under the prevailing conjecture that the mental state update generated by observation is described as orthogonal projection of the mental state vector (the projective update conjecture of Wang and Busemeyer). A simple test of RRE is suggested. In contrast to the previous works in quantum-like modeling, we proceed in the state-dependent framework; in particular, distributivity, compatibility, and RRE are considered in a fixed mental state. In this framework, we improve the previous result on the impossibility to combine question order and response replicability effects by using (von Neumann-L√ºders) projective measurements.", "url": "https://arxiv.org/abs/2208.12946v2", "year": 2022, "categories": ["q-bio.NC", "quant-ph"]}
{"search_keyword": "experimental psychology", "id": "1405.6427v1", "title": "Applying Quantum Principles to Psychology", "abstract": "This article starts out with a detailed example illustrating the utility of applying quantum probability to psychology. Then it describes several alternative mathematical methods for mapping fundamental quantum concepts (such as state preparation, measurement, state evolution) to fundamental psychological concepts (such as stimulus, response, information processing). For state preparation, we consider both pure states and densities with mixtures. For measurement, we consider projective measurements and positive operator valued measurements. The advantages and disadvantages of each method with respect to applications in psychology are discussed.", "url": "https://arxiv.org/abs/1405.6427v1", "year": 2014, "categories": ["q-bio.NC", "quant-ph"]}
{"search_keyword": "experimental psychology", "id": "2402.03435v1", "title": "Psychological Assessments with Large Language Models: A Privacy-Focused and Cost-Effective Approach", "abstract": "This study explores the use of Large Language Models (LLMs) to analyze text comments from Reddit users, aiming to achieve two primary objectives: firstly, to pinpoint critical excerpts that support a predefined psychological assessment of suicidal risk; and secondly, to summarize the material to substantiate the preassigned suicidal risk level. The work is circumscribed to the use of \"open-source\" LLMs that can be run locally, thereby enhancing data privacy. Furthermore, it prioritizes models with low computational requirements, making it accessible to both individuals and institutions operating on limited computing budgets. The implemented strategy only relies on a carefully crafted prompt and a grammar to guide the LLM's text completion. Despite its simplicity, the evaluation metrics show outstanding results, making it a valuable privacy-focused and cost-effective approach. This work is part of the Computational Linguistics and Clinical Psychology (CLPsych) 2024 shared task.", "url": "https://arxiv.org/abs/2402.03435v1", "year": 2024, "categories": ["cs.CL", "cs.AI", "cs.CY"]}
{"search_keyword": "experimental psychology", "id": "math/0611451v3", "title": "Experimental study of energy-minimizing point configurations on spheres", "abstract": "In this paper we report on massive computer experiments aimed at finding spherical point configurations that minimize potential energy. We present experimental evidence for two new universal optima (consisting of 40 points in 10 dimensions and 64 points in 14 dimensions), as well as evidence that there are no others with at most 64 points. We also describe several other new polytopes, and we present new geometrical descriptions of some of the known universal optima.", "url": "https://arxiv.org/abs/math/0611451v3", "year": 2006, "categories": ["math.MG"]}
{"search_keyword": "experimental psychology", "id": "1807.05684v4", "title": "True Contextuality Beats Direct Influences in Human Decision Making", "abstract": "In quantum physics there are well-known situations when measurements of the same property in different contexts (under different conditions) have the same probability distribution, but cannot be represented by one and the same random variable. Such systems of random variables are called contextual. More generally, true contextuality is observed when different contexts force measurements of the same property (in psychology, responses to the same question) to be more dissimilar random variables than warranted by the difference of their distributions. The difference in distributions is itself a form of context-dependence, but of another nature: it is attributable to direct causal influences exerted by contexts upon the random variables. The Contextuality-by-Default (CbD) theory allows one to separate true contextuality from direct influences in the overall context-dependence. The CbD analysis of numerous previous attempts to demonstrate contextuality in human judgments shows that all context-dependence in them can be accounted for by direct influences, with no true contextuality present. However, contextual systems in human behavior can be found. In this paper we present a series of crowdsourcing experiments that exhibit true contextuality in simple decision making.}{The design of these experiments is an elaboration of one introduced in the \"Snow Queen\" experiment (Decision 5, 193-204, 2018), where contextuality was for the first time demonstrated unequivocally.", "url": "https://arxiv.org/abs/1807.05684v4", "year": 2018, "categories": ["q-bio.NC", "quant-ph"]}
{"search_keyword": "experimental psychology", "id": "2308.01264v2", "title": "Exploring the psychology of LLMs' Moral and Legal Reasoning", "abstract": "Large language models (LLMs) exhibit expert-level performance in tasks across a wide range of different domains. Ethical issues raised by LLMs and the need to align future versions makes it important to know how state of the art models reason about moral and legal issues. In this paper, we employ the methods of experimental psychology to probe into this question. We replicate eight studies from the experimental literature with instances of Google's Gemini Pro, Anthropic's Claude 2.1, OpenAI's GPT-4, and Meta's Llama 2 Chat 70b. We find that alignment with human responses shifts from one experiment to another, and that models differ amongst themselves as to their overall alignment, with GPT-4 taking a clear lead over all other models we tested. Nonetheless, even when LLM-generated responses are highly correlated to human responses, there are still systematic differences, with a tendency for models to exaggerate effects that are present among humans, in part by reducing variance. This recommends caution with regards to proposals of replacing human participants with current state-of-the-art LLMs in psychological research and highlights the need for further research about the distinctive aspects of machine psychology.", "url": "https://arxiv.org/abs/2308.01264v2", "year": 2023, "categories": ["cs.AI", "cs.CL"]}
{"search_keyword": "experimental psychology", "id": "1201.4685v2", "title": "Order Effects in Sequential Measurements of Non-Commuting Psychological Observables", "abstract": "Sequential measurements of non-commuting observables produce order effects that are well-known in quantum physics. But their conceptual basis, a significant measurement interaction, is relevant for far more general situations. We argue that non-commutativity is ubiquitous in psychology where almost every interaction with a mental system changes that system in an uncontrollable fashion. Psychological order effects for sequential measurements are therefore to be expected as a rule. In this paper we focus on the theoretical basis of such effects. We classify several families of order effects theoretically, relate them to psychological observations, and predict effects yet to be discovered empirically. We assess the complexity, related to the predictive power, of particular (Hilbert space) models of order effects and discuss possible limitations of such models.", "url": "https://arxiv.org/abs/1201.4685v2", "year": 2012, "categories": ["physics.data-an", "quant-ph"]}
{"search_keyword": "cognitive science", "id": "2405.04048v1", "title": "Philosophy of Cognitive Science in the Age of Deep Learning", "abstract": "Deep learning has enabled major advances across most areas of artificial intelligence research. This remarkable progress extends beyond mere engineering achievements and holds significant relevance for the philosophy of cognitive science. Deep neural networks have made significant strides in overcoming the limitations of older connectionist models that once occupied the centre stage of philosophical debates about cognition. This development is directly relevant to long-standing theoretical debates in the philosophy of cognitive science. Furthermore, ongoing methodological challenges related to the comparative evaluation of deep neural networks stand to benefit greatly from interdisciplinary collaboration with philosophy and cognitive science. The time is ripe for philosophers to explore foundational issues related to deep learning and cognition; this perspective paper surveys key areas where their contributions can be especially fruitful.", "url": "https://arxiv.org/abs/2405.04048v1", "year": 2024, "categories": ["cs.CL"]}
{"search_keyword": "cognitive science", "id": "0911.3641v1", "title": "The Import and Export of Cognitive Science", "abstract": "From its inception, a large part of the motivation for Cognitive Science has been the need for an interdisciplinary journal for the study of minds and intelligent systems. One threat to the interdisciplinarity of Cognitive Science, both the field and journal, is that it may become, or already be, too dominated by psychologists. In 2005, psychology was a keyword for 51% of submissions, followed distantly by linguistics (17%), artificial intelligence (13%), neuroscience (10%), computer science (9%), and philosophy (8%). The Institute for Scientific Information (ISI) gathers data not only on how individual articles cite one another, but also on macroscopic citation patterns among journals. Journals or sets of journals can be considered as proxies for fields. As fields become established, they often create journals. By studying the patterns of citations among journals that cite and are cited by Cognitive Science, we can better: 1) appreciate the scholarly ecology surrounding the journal and the journals role within this ecology, 2) establish competitor and alternate journals, and 3) determine the natural clustering of fields related to cognitive science.", "url": "https://arxiv.org/abs/0911.3641v1", "year": 2009, "categories": ["cs.DL", "physics.soc-ph"]}
{"search_keyword": "cognitive science", "id": "1211.0310v1", "title": "Large Synoptic Survey Telescope: Dark Energy Science Collaboration", "abstract": "This white paper describes the LSST Dark Energy Science Collaboration (DESC), whose goal is the study of dark energy and related topics in fundamental physics with data from the Large Synoptic Survey Telescope (LSST). It provides an overview of dark energy science and describes the current and anticipated state of the field. It makes the case for the DESC by laying out a robust analytical framework for dark energy science that has been defined by its members and the comprehensive three-year work plan they have developed for implementing that framework. The analysis working groups cover five key probes of dark energy: weak lensing, large scale structure, galaxy clusters, Type Ia supernovae, and strong lensing. The computing working groups span cosmological simulations, galaxy catalogs, photon simulations and a systematic software and computational framework for LSST dark energy data analysis. The technical working groups make the connection between dark energy science and the LSST system. The working groups have close linkages, especially through the use of the photon simulations to study the impact of instrument design and survey strategy on analysis methodology and cosmological parameter estimation. The white paper describes several high priority tasks identified by each of the 16 working groups. Over the next three years these tasks will help prepare for LSST analysis, make synergistic connections with ongoing cosmological surveys and provide the dark energy community with state of the art analysis tools. Members of the community are invited to join the LSST DESC, according to the membership policies described in the white paper. Applications to sign up for associate membership may be made by submitting the Web form at http://www.slac.stanford.edu/exp/lsst/desc/signup.html with a short statement of the work they wish to pursue that is relevant to the LSST DESC.", "url": "https://arxiv.org/abs/1211.0310v1", "year": 2012, "categories": ["astro-ph.CO", "hep-ex"]}
{"search_keyword": "cognitive science", "id": "1002.3035v1", "title": "Cognition and Emotion: Perspectives of a Closing Gap", "abstract": "The primary tasks of a cognitive system is to survive and to maximize a life-long utility function, like the number of offsprings. A direct computational maximization of life-long utility is however not possible in complex environments, especially in the context, of real-world time constraints. The central role of emotions is to serve as an intermediate layer in the space of policies available to agents and animals, leading to a large dimensional reduction of complexity. We review our current understanding of the functional role of emotions, stressing the role of the neuromodulators mediating emotions for the diffusive homeostatic control system of the brain. We discuss a recent proposal, that emotional diffusive control is characterized, in contrast to neutral diffusive control, by interaction effects, viz by interferences between emotional arousal and reward signaling. Several proposals for the realization of synthetic emotions are discussed in this context, together with key open issues regarding the interplay between emotional motivational drives and diffusive control.", "url": "https://arxiv.org/abs/1002.3035v1", "year": 2010, "categories": ["q-bio.NC"]}
{"search_keyword": "cognitive science", "id": "0901.3028v1", "title": "Cognitive computation with autonomously active neural networks: an emerging field", "abstract": "The human brain is autonomously active. To understand the functional role of this self-sustained neural activity, and its interplay with the sensory data input stream, is an important question in cognitive system research and we review here the present state of theoretical modelling. This review will start with a brief overview of the experimental efforts, together with a discussion of transient vs. self-sustained neural activity in the framework of reservoir computing. The main emphasis will be then on two paradigmal neural network architectures showing continuously ongoing transient-state dynamics: saddle point networks and networks of attractor relics. Self-active neural networks are confronted with two seemingly contrasting demands: a stable internal dynamical state and sensitivity to incoming stimuli. We show, that this dilemma can be solved by networks of attractor relics based on competitive neural dynamics, where the attractor relics compete on one side with each other for transient dominance, and on the other side with the dynamical influence of the input signals. Unsupervised and local Hebbian-style online learning then allows the system to build up correlations between the internal dynamical transient states and the sensory input stream. An emergent cognitive capability results from this set-up. The system performs online, and on its own, a non-linear independent component analysis of the sensory data stream, all the time being continuously and autonomously active. This process maps the independent components of the sensory input onto the attractor relics, which acquire in this way a semantic meaning.", "url": "https://arxiv.org/abs/0901.3028v1", "year": 2009, "categories": ["q-bio.NC"]}
{"search_keyword": "cognitive science", "id": "1304.6736v2", "title": "Networks in Cognitive Science", "abstract": "Networks of interconnected nodes have long played a key role in Cognitive Science, from artificial neural net- works to spreading activation models of semantic mem- ory. Recently, however, a new Network Science has been developed, providing insights into the emergence of global, system-scale properties in contexts as diverse as the Internet, metabolic reactions, and collaborations among scientists. Today, the inclusion of network theory into Cognitive Sciences, and the expansion of complex- systems science, promises to significantly change the way in which the organization and dynamics of cognitive and behavioral processes are understood. In this paper, we review recent contributions of network theory at different levels and domains within the Cognitive Sciences.", "url": "https://arxiv.org/abs/1304.6736v2", "year": 2013, "categories": ["physics.soc-ph", "cs.SI", "q-bio.NC"]}
{"search_keyword": "cognitive science", "id": "2310.08803v1", "title": "Advancing Perception in Artificial Intelligence through Principles of Cognitive Science", "abstract": "Although artificial intelligence (AI) has achieved many feats at a rapid pace, there still exist open problems and fundamental shortcomings related to performance and resource efficiency. Since AI researchers benchmark a significant proportion of performance standards through human intelligence, cognitive sciences-inspired AI is a promising domain of research. Studying cognitive science can provide a fresh perspective to building fundamental blocks in AI research, which can lead to improved performance and efficiency. In this review paper, we focus on the cognitive functions of perception, which is the process of taking signals from one's surroundings as input, and processing them to understand the environment. Particularly, we study and compare its various processes through the lens of both cognitive sciences and AI. Through this study, we review all current major theories from various sub-disciplines of cognitive science (specifically neuroscience, psychology and linguistics), and draw parallels with theories and techniques from current practices in AI. We, hence, present a detailed collection of methods in AI for researchers to build AI systems inspired by cognitive science. Further, through the process of reviewing the state of cognitive-inspired AI, we point out many gaps in the current state of AI (with respect to the performance of the human brain), and hence present potential directions for researchers to develop better perception systems in AI.", "url": "https://arxiv.org/abs/2310.08803v1", "year": 2023, "categories": ["cs.AI"]}
{"search_keyword": "cognitive science", "id": "cs/0412059v1", "title": "Vector Symbolic Architectures answer Jackendoff's challenges for cognitive neuroscience", "abstract": "Jackendoff (2002) posed four challenges that linguistic combinatoriality and rules of language present to theories of brain function. The essence of these problems is the question of how to neurally instantiate the rapid construction and transformation of the compositional structures that are typically taken to be the domain of symbolic processing. He contended that typical connectionist approaches fail to meet these challenges and that the dialogue between linguistic theory and cognitive neuroscience will be relatively unproductive until the importance of these problems is widely recognised and the challenges answered by some technical innovation in connectionist modelling. This paper claims that a little-known family of connectionist models (Vector Symbolic Architectures) are able to meet Jackendoff's challenges.", "url": "https://arxiv.org/abs/cs/0412059v1", "year": 2004, "categories": ["cs.NE", "cs.AI"]}
{"search_keyword": "cognitive science", "id": "1801.08829v2", "title": "Symbol Emergence in Cognitive Developmental Systems: a Survey", "abstract": "Humans use signs, e.g., sentences in a spoken language, for communication and thought. Hence, symbol systems like language are crucial for our communication with other agents and adaptation to our real-world environment. The symbol systems we use in our human society adaptively and dynamically change over time. In the context of artificial intelligence (AI) and cognitive systems, the symbol grounding problem has been regarded as one of the central problems related to {\\it symbols}. However, the symbol grounding problem was originally posed to connect symbolic AI and sensorimotor information and did not consider many interdisciplinary phenomena in human communication and dynamic symbol systems in our society, which semiotics considered. In this paper, we focus on the symbol emergence problem, addressing not only cognitive dynamics but also the dynamics of symbol systems in society, rather than the symbol grounding problem. We first introduce the notion of a symbol in semiotics from the humanities, to leave the very narrow idea of symbols in symbolic AI. Furthermore, over the years, it became more and more clear that symbol emergence has to be regarded as a multifaceted problem. Therefore, secondly, we review the history of the symbol emergence problem in different fields, including both biological and artificial systems, showing their mutual relations. We summarize the discussion and provide an integrative viewpoint and comprehensive overview of symbol emergence in cognitive systems. Additionally, we describe the challenges facing the creation of cognitive systems that can be part of symbol emergence systems.", "url": "https://arxiv.org/abs/1801.08829v2", "year": 2018, "categories": ["cs.AI", "cs.RO"]}
{"search_keyword": "cognitive science", "id": "1110.6589v1", "title": "A cognitive diversity framework for radar target classification", "abstract": "Classification of targets by radar has proved to be notoriously difficult with the best systems still yet to attain sufficiently high levels of performance and reliability. In the current contribution we explore a new design of radar based target recognition, where angular diversity is used in a cognitive manner to attain better performance. Performance is bench- marked against conventional classification schemes. The proposed scheme can easily be extended to cognitive target recognition based on multiple diversity strategies.", "url": "https://arxiv.org/abs/1110.6589v1", "year": 2011, "categories": ["cs.AI"]}
{"search_keyword": "psycholinguistics", "id": "2410.02691v3", "title": "On the Proper Treatment of Tokenization in Psycholinguistics", "abstract": "Language models are widely used in computational psycholinguistics to test theories that relate the negative log probability (the surprisal) of a region of interest (a substring of characters) under a language model to its cognitive cost experienced by readers, as operationalized, for example, by gaze duration on the region. However, the application of modern language models to psycholinguistic studies is complicated by the practice of using tokenization as an intermediate step in training a model. Doing so results in a language model over token strings rather than one over character strings. Vexingly, regions of interest are generally misaligned with these token strings. The paper argues that token-level language models should be (approximately) marginalized into character-level language models before they are used in psycholinguistic studies to compute the surprisal of a region of interest; then, the marginalized character-level language model can be used to compute the surprisal of an arbitrary character substring, which we term a focal area, that the experimenter may wish to use as a predictor. Our proposal of marginalizing a token-level model into a character-level one solves this misalignment issue independently of the tokenization scheme. Empirically, we discover various focal areas whose surprisal is a better psychometric predictor than the surprisal of the region of interest itself.", "url": "https://arxiv.org/abs/2410.02691v3", "year": 2024, "categories": ["cs.CL"]}
{"search_keyword": "psycholinguistics", "id": "2508.11828v1", "title": "A Survey of Idiom Datasets for Psycholinguistic and Computational Research", "abstract": "Idioms are figurative expressions whose meanings often cannot be inferred from their individual words, making them difficult to process computationally and posing challenges for human experimental studies. This survey reviews datasets developed in psycholinguistics and computational linguistics for studying idioms, focusing on their content, form, and intended use. Psycholinguistic resources typically contain normed ratings along dimensions such as familiarity, transparency, and compositionality, while computational datasets support tasks like idiomaticity detection/classification, paraphrasing, and cross-lingual modeling. We present trends in annotation practices, coverage, and task framing across 53 datasets. Although recent efforts expanded language coverage and task diversity, there seems to be no relation yet between psycholinguistic and computational research on idioms.", "url": "https://arxiv.org/abs/2508.11828v1", "year": 2025, "categories": ["cs.CL"]}
{"search_keyword": "psycholinguistics", "id": "2402.05455v1", "title": "Large Language Models for Psycholinguistic Plausibility Pretesting", "abstract": "In psycholinguistics, the creation of controlled materials is crucial to ensure that research outcomes are solely attributed to the intended manipulations and not influenced by extraneous factors. To achieve this, psycholinguists typically pretest linguistic materials, where a common pretest is to solicit plausibility judgments from human evaluators on specific sentences. In this work, we investigate whether Language Models (LMs) can be used to generate these plausibility judgements. We investigate a wide range of LMs across multiple linguistic structures and evaluate whether their plausibility judgements correlate with human judgements. We find that GPT-4 plausibility judgements highly correlate with human judgements across the structures we examine, whereas other LMs correlate well with humans on commonly used syntactic structures. We then test whether this correlation implies that LMs can be used instead of humans for pretesting. We find that when coarse-grained plausibility judgements are needed, this works well, but when fine-grained judgements are necessary, even GPT-4 does not provide satisfactory discriminative power.", "url": "https://arxiv.org/abs/2402.05455v1", "year": 2024, "categories": ["cs.CL"]}
{"search_keyword": "psycholinguistics", "id": "0712.0451v1", "title": "A Reactive Tabu Search Algorithm for Stimuli Generation in Psycholinguistics", "abstract": "The generation of meaningless \"words\" matching certain statistical and/or linguistic criteria is frequently needed for experimental purposes in Psycholinguistics. Such stimuli receive the name of pseudowords or nonwords in the Cognitive Neuroscience literatue. The process for building nonwords sometimes has to be based on linguistic units such as syllables or morphemes, resulting in a numerical explosion of combinations when the size of the nonwords is increased. In this paper, a reactive tabu search scheme is proposed to generate nonwords of variables size. The approach builds pseudowords by using a modified Metaheuristic algorithm based on a local search procedure enhanced by a feedback-based scheme. Experimental results show that the new algorithm is a practical and effective tool for nonword generation.", "url": "https://arxiv.org/abs/0712.0451v1", "year": 2007, "categories": ["cs.AI", "cs.CC", "cs.DM", "cs.LG"]}
{"search_keyword": "psycholinguistics", "id": "2303.00077v1", "title": "Beyond the limitations of any imaginable mechanism: large language models and psycholinguistics", "abstract": "Large language models are not detailed models of human linguistic processing. They are, however, extremely successful at their primary task: providing a model for language. For this reason and because there are no animal models for language, large language models are important in psycholinguistics: they are useful as a practical tool, as an illustrative comparative, and philosophically, as a basis for recasting the relationship between language and thought.", "url": "https://arxiv.org/abs/2303.00077v1", "year": 2023, "categories": ["cs.CL", "cs.AI"]}
{"search_keyword": "psycholinguistics", "id": "cmp-lg/9606016v5", "title": "A Probabilistic Disambiguation Method Based on Psycholinguistic Principles", "abstract": "We address the problem of structural disambiguation in syntactic parsing. In psycholinguistics, a number of principles of disambiguation have been proposed, notably the Lexical Preference Rule (LPR), the Right Association Principle (RAP), and the Attach Low and Parallel Principle (ALPP) (an extension of RAP). We argue that in order to improve disambiguation results it is necessary to implement these principles on the basis of a probabilistic methodology. We define a `three-word probability' for implementing LPR, and a `length probability' for implementing RAP and ALPP. Furthermore, we adopt the `back-off' method to combine these two types of probabilities. Our experimental results indicate our method to be effective, attaining an accuracy of 89.2%.", "url": "https://arxiv.org/abs/cmp-lg/9606016v5", "year": 1996, "categories": ["cs.CL"]}
{"search_keyword": "psycholinguistics", "id": "2506.22439v1", "title": "Psycholinguistic Word Features: a New Approach for the Evaluation of LLMs Alignment with Humans", "abstract": "The evaluation of LLMs has so far focused primarily on how well they can perform different tasks such as reasoning, question-answering, paraphrasing, or translating. For most of these tasks, performance can be measured with objective metrics, such as the number of correct answers. However, other language features are not easily quantified. For example, arousal, concreteness, or gender associated with a given word, as well as the extent to which we experience words with senses and relate them to a specific sense. Those features have been studied for many years by psycholinguistics, conducting large-scale experiments with humans to produce ratings for thousands of words. This opens an opportunity to evaluate how well LLMs align with human ratings on these word features, taking advantage of existing studies that cover many different language features in a large number of words. In this paper, we evaluate the alignment of a representative group of LLMs with human ratings on two psycholinguistic datasets: the Glasgow and Lancaster norms. These datasets cover thirteen features over thousands of words. The results show that alignment is \\textcolor{black}{generally} better in the Glasgow norms evaluated (arousal, valence, dominance, concreteness, imageability, familiarity, and gender) than on the Lancaster norms evaluated (introceptive, gustatory, olfactory, haptic, auditory, and visual). This suggests a potential limitation of current LLMs in aligning with human sensory associations for words, which may be due to their lack of embodied cognition present in humans and illustrates the usefulness of evaluating LLMs with psycholinguistic datasets.", "url": "https://arxiv.org/abs/2506.22439v1", "year": 2025, "categories": ["cs.CL", "cs.AI"]}
{"search_keyword": "psycholinguistics", "id": "1906.04229v1", "title": "Psycholinguistics meets Continual Learning: Measuring Catastrophic Forgetting in Visual Question Answering", "abstract": "We study the issue of catastrophic forgetting in the context of neural multimodal approaches to Visual Question Answering (VQA). Motivated by evidence from psycholinguistics, we devise a set of linguistically-informed VQA tasks, which differ by the types of questions involved (Wh-questions and polar questions). We test what impact task difficulty has on continual learning, and whether the order in which a child acquires question types facilitates computational models. Our results show that dramatic forgetting is at play and that task difficulty and order matter. Two well-known current continual learning methods mitigate the problem only to a limiting degree.", "url": "https://arxiv.org/abs/1906.04229v1", "year": 2019, "categories": ["cs.CL"]}
{"search_keyword": "psycholinguistics", "id": "2106.04963v1", "title": "Psycholinguistic Tripartite Graph Network for Personality Detection", "abstract": "Most of the recent work on personality detection from online posts adopts multifarious deep neural networks to represent the posts and builds predictive models in a data-driven manner, without the exploitation of psycholinguistic knowledge that may unveil the connections between one's language usage and his psychological traits. In this paper, we propose a psycholinguistic knowledge-based tripartite graph network, TrigNet, which consists of a tripartite graph network and a BERT-based graph initializer. The graph network injects structural psycholinguistic knowledge from LIWC, a computerized instrument for psycholinguistic analysis, by constructing a heterogeneous tripartite graph. The graph initializer is employed to provide initial embeddings for the graph nodes. To reduce the computational cost in graph learning, we further propose a novel flow graph attention network (GAT) that only transmits messages between neighboring parties in the tripartite graph. Benefiting from the tripartite graph, TrigNet can aggregate post information from a psychological perspective, which is a novel way of exploiting domain knowledge. Extensive experiments on two datasets show that TrigNet outperforms the existing state-of-art model by 3.47 and 2.10 points in average F1. Moreover, the flow GAT reduces the FLOPS and Memory measures by 38% and 32%, respectively, in comparison to the original GAT in our setting.", "url": "https://arxiv.org/abs/2106.04963v1", "year": 2021, "categories": ["cs.CL"]}
{"search_keyword": "psycholinguistics", "id": "2409.00855v1", "title": "LanguaShrink: Reducing Token Overhead with Psycholinguistics", "abstract": "As large language models (LLMs) improve their capabilities in handling complex tasks, the issues of computational cost and efficiency due to long prompts are becoming increasingly prominent. To accelerate model inference and reduce costs, we propose an innovative prompt compression framework called LanguaShrink. Inspired by the observation that LLM performance depends on the density and position of key information in the input prompts, LanguaShrink leverages psycholinguistic principles and the Ebbinghaus memory curve to achieve task-agnostic prompt compression. This effectively reduces prompt length while preserving essential information. We referred to the training method of OpenChat.The framework introduces part-of-speech priority compression and data distillation techniques, using smaller models to learn compression targets and employing a KL-regularized reinforcement learning strategy for training.\\cite{wang2023openchat} Additionally, we adopt a chunk-based compression algorithm to achieve adjustable compression rates. We evaluate our method on multiple datasets, including LongBench, ZeroScrolls, Arxiv Articles, and a newly constructed novel test set. Experimental results show that LanguaShrink maintains semantic similarity while achieving up to 26 times compression. Compared to existing prompt compression methods, LanguaShrink improves end-to-end latency by 1.43 times.", "url": "https://arxiv.org/abs/2409.00855v1", "year": 2024, "categories": ["cs.CL", "stat.ML"]}
