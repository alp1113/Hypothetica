{
  "topic": "rag",
  "timestamp": "20251013_180815",
  "total_papers": 8,
  "papers": [
    {
      "arxiv_id": "2407.21059v1",
      "title": "Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable   Frameworks",
      "authors": [
        "Yunfan Gao",
        "Yun Xiong",
        "Meng Wang",
        "Haofen Wang"
      ],
      "published": "2024-07-26T03:45:30Z",
      "updated": "2024-07-26T03:45:30Z",
      "summary": "Retrieval-augmented Generation (RAG) has markedly enhanced the capabilities of Large Language Models (LLMs) in tackling knowledge-intensive tasks. The increasing demands of application scenarios have driven the evolution of RAG, leading to the integration of advanced retrievers, LLMs and other complementary technologies, which in turn has amplified the intricacy of RAG systems. However, the rapid advancements are outpacing the foundational RAG paradigm, with many methods struggling to be unified under the process of \"retrieve-then-generate\". In this context, this paper examines the limitations of the existing RAG paradigm and introduces the modular RAG framework. By decomposing complex RAG systems into independent modules and specialized operators, it facilitates a highly reconfigurable framework. Modular RAG transcends the traditional linear architecture, embracing a more advanced design that integrates routing, scheduling, and fusion mechanisms. Drawing on extensive research, this paper further identifies prevalent RAG patterns-linear, conditional, branching, and looping-and offers a comprehensive analysis of their respective implementation nuances. Modular RAG presents innovative opportunities for the conceptualization and deployment of RAG systems. Finally, the paper explores the potential emergence of new operators and paradigms, establishing a solid theoretical foundation and a practical roadmap for the continued evolution and practical deployment of RAG technologies.",
      "html_url": "https://arxiv.org/html/2407.21059v1",
      "sections": {
        "introduction": "Large Language Models (LLMs) have demonstrated remarkable capabilities, yet they still face numerous challenges, such as hallucination and the lag in information updates [1]. Retrieval-augmented Generation (RAG), by accessing external knowledge bases, provides LLMs with important contextual information, significantly enhancing their performance on knowledge-intensive tasks [2]. Currently, RAG, as an enhancement method, has been widely applied in various practical application scenarios, including knowledge question answering, recommendation systems, customer service, and personal assistants. [3, 4, 5, 6]\n\nLarge Language Models (LLMs) have demonstrated remarkable capabilities, yet they still face numerous challenges, such as hallucination and the lag in information updates [1]. Retrieval-augmented Generation (RAG), by accessing external knowledge bases, provides LLMs with important contextual information, significantly enhancing their performance on knowledge-intensive tasks [2]. Currently, RAG, as an enhancement method, has been widely applied in various practical application scenarios, including knowledge question answering, recommendation systems, customer service, and personal assistants. [3, 4, 5, 6]\n\nDuring the nascent stages of RAG , its core framework is constituted by indexing, retrieval, and generation, a paradigm referred to as Naive RAG [7]. However, as the complexity of tasks and the demands of applications have escalated, the limitations of Naive RAG have become increasingly apparent. As depicted in Figure 1, it predominantly hinges on the straightforward similarity of chunks, result in poor performance when confronted with complex queries and chunks with substantial variability. The primary challenges of Naive RAG include: 1) Shallow Understanding of Queries. The semantic similarity between a query and document chunk is not always highly consistent. Relying solely on similarity calculations for retrieval lacks an in-depth exploration of the relationship between the query and the document [8]. 2) Retrieval Redundancy and Noise. Feeding all retrieved chunks directly into LLMs is not always beneficial. Research indicates that an excess of redundant and noisy information may interfere with the LLM’s identification of key information, thereby increasing the risk of generating erroneous and hallucinated responses. [9]\n\nDuring the nascent stages of RAG , its core framework is constituted by indexing, retrieval, and generation, a paradigm referred to as Naive RAG [7]. However, as the complexity of tasks and the demands of applications have escalated, the limitations of Naive RAG have become increasingly apparent. As depicted in Figure 1, it predominantly hinges on the straightforward similarity of chunks, result in poor performance when confronted with complex queries and chunks with substantial variability. The primary challenges of Naive RAG include: 1) Shallow Understanding of Queries. The semantic similarity between a query and document chunk is not always highly consistent. Relying solely on similarity calculations for retrieval lacks an in-depth exploration of the relationship between the query and the document [8]. 2) Retrieval Redundancy and Noise. Feeding all retrieved chunks directly into LLMs is not always beneficial. Research indicates that an excess of redundant and noisy information may interfere with the LLM’s identification of key information, thereby increasing the risk of generating erroneous and hallucinated responses. [9]\n\nTo overcome the aforementioned limitations, Advanced RAG paradigm focuses on optimizing the retrieval phase, aiming to enhance retrieval efficiency and strengthen the utilization of retrieved chunks. As shown in Figure 1 ,typical strategies involve pre-retrieval processing and post-retrieval processing. For instance, query rewriting is used to make the queries more clear and specific, thereby increasing the accuracy of retrieval [10], and the reranking of retrieval results is employed to enhance the LLM’s ability to identify and utilize key information [11].\n\nTo overcome the aforementioned limitations, Advanced RAG paradigm focuses on optimizing the retrieval phase, aiming to enhance retrieval efficiency and strengthen the utilization of retrieved chunks. As shown in Figure 1 ,typical strategies involve pre-retrieval processing and post-retrieval processing. For instance, query rewriting is used to make the queries more clear and specific, thereby increasing the accuracy of retrieval [10], and the reranking of retrieval results is employed to enhance the LLM’s ability to identify and utilize key information [11].\n\nDespite the improvements in the practicality of Advanced RAG, there remains a gap between its capabilities and real-world application requirements. On one hand, as RAG technology advances, user expectations rise, demands continue to evolve, and application settings become more complex. For instance, the integration of heterogeneous data and the new demands for system transparency, control, and maintainability. On the other hand, the growth in application demands has further propelled the evolution of RAG technology.\n\nDespite the improvements in the practicality of Advanced RAG, there remains a gap between its capabilities and real-world application requirements. On one hand, as RAG technology advances, user expectations rise, demands continue to evolve, and application settings become more complex. For instance, the integration of heterogeneous data and the new demands for system transparency, control, and maintainability. On the other hand, the growth in application demands has further propelled the evolution of RAG technology.\n\nAs shown in Figure 2, to achieve more accurate and efficient task execution, modern RAG systems are progressively integrating more sophisticated function, such as organizing more refined index base in the form of knowledge graphs, integrating structured data through query construction methods, and employing fine-tuning techniques to enable encoders to better adapt to domain-specific documents.\n\nAs shown in Figure 2, to achieve more accurate and efficient task execution, modern RAG systems are progressively integrating more sophisticated function, such as organizing more refined index base in the form of knowledge graphs, integrating structured data through query construction methods, and employing fine-tuning techniques to enable encoders to better adapt to domain-specific documents.\n\nIn terms of process design, the current RAG system has surpassed the traditional linear retrieval-generation paradigm. Researchers use iterative retrieval [12] to obtain richer context, recursive retrieval [13] to handle complex queries, and adaptive retrieval [14] to provide overall autonomy and flexibility. This flexibility in the process significantly enhances the expressive power and adaptability of RAG systems, enabling them to better adapt to various application scenarios. However, this also makes the orchestration and scheduling of workflows more complex, posing greater challenges to system design. Specifically, RAG currently faces the following new challenges:\n\nIn terms of process design, the current RAG system has surpassed the traditional linear retrieval-generation paradigm. Researchers use iterative retrieval [12] to obtain richer context, recursive retrieval [13] to handle complex queries, and adaptive retrieval [14] to provide overall autonomy and flexibility. This flexibility in the process significantly enhances the expressive power and adaptability of RAG systems, enabling them to better adapt to various application scenarios. However, this also makes the orchestration and scheduling of workflows more complex, posing greater challenges to system design. Specifically, RAG currently faces the following new challenges:\n\nComplex data sources integration. RAG are no longer confined to a single type of unstructured text data source but have expanded to include various data types, such as semi-structured data like tables and structured data like knowledge graphs [15]. Access to heterogeneous data from multiple sources can provide the system with a richer knowledge background, and more reliable knowledge verification capabilities [16].\n\nComplex data sources integration. RAG are no longer confined to a single type of unstructured text data source but have expanded to include various data types, such as semi-structured data like tables and structured data like knowledge graphs [15]. Access to heterogeneous data from multiple sources can provide the system with a richer knowledge background, and more reliable knowledge verification capabilities [16].\n\nNew demands for system interpretability, controllability, and maintainability. With the increasing complexity of systems, system maintenance and debugging have become more challenging. Additionally, when issues arise, it is essential to quickly pinpoint the specific components that require optimization.\n\nNew demands for system interpretability, controllability, and maintainability. With the increasing complexity of systems, system maintenance and debugging have become more challenging. Additionally, when issues arise, it is essential to quickly pinpoint the specific components that require optimization.\n\nComponent selection and optimization. More neural networks are involved in the RAG system, necessitating the selection of appropriate components to meet the needs of specific tasks and resource configurations. Moreover, additional components enhance the effectiveness of RAG but also bring new collaborative work requirements [17]. Ensuring that these models perform as intended and work efficiently together to enhance the overall system performance is crucial.\n\nComponent selection and optimization. More neural networks are involved in the RAG system, necessitating the selection of appropriate components to meet the needs of specific tasks and resource configurations. Moreover, additional components enhance the effectiveness of RAG but also bring new collaborative work requirements [17]. Ensuring that these models perform as intended and work efficiently together to enhance the overall system performance is crucial.\n\nWorkflow orchestration and scheduling. Components may need to be executed in a specific order, processed in parallel under certain conditions, or even judged by the LLM based on different outputs. Reasonable planning of the workflow is essential for improving system efficiency and achieving the desired outcomes [18].\n\nWorkflow orchestration and scheduling. Components may need to be executed in a specific order, processed in parallel under certain conditions, or even judged by the LLM based on different outputs. Reasonable planning of the workflow is essential for improving system efficiency and achieving the desired outcomes [18].\n\nTo address the design, management, and maintenance challenges posed by the increasing complexity of RAG systems, and to meet the ever-growing and diverse demands and expectations, this paper proposes Modular RAG architecture. In modern computing systems, modularization is becoming a trend. It can enhance the system’s scalability and maintainability and achieve efficient task execution through process control.\n\nTo address the design, management, and maintenance challenges posed by the increasing complexity of RAG systems, and to meet the ever-growing and diverse demands and expectations, this paper proposes Modular RAG architecture. In modern computing systems, modularization is becoming a trend. It can enhance the system’s scalability and maintainability and achieve efficient task execution through process control.\n\nThe Modular RAG system consists of multiple independent yet tightly coordinated modules, each responsible for handling specific functions or tasks. This architecture is divided into three levels: the top level focuses on the critical stages of RAG, where each stage is treated as an independent module. This level not only inherits the main processes from the Advanced RAG paradigm but also introduces an orchestration module to control the coordination of RAG processes. The middle level is composed of sub-modules within each module, further refining and optimizing the functions. The bottom level consists of basic units of operation—operators. Within the Modular RAG framework, RAG systems can be represented in the form of computational graphs, where nodes represent specific operators. The comparison of the three paradigms is shown in the Figure 3. Modular RAG evolves based on the previous development of RAG. The relationships among these three paradigms are ones of inheritance and development. Advanced RAG is a special case of Modular RAG, while Naive RAG is a special case of Advanced RAG.\n\nThe Modular RAG system consists of multiple independent yet tightly coordinated modules, each responsible for handling specific functions or tasks. This architecture is divided into three levels: the top level focuses on the critical stages of RAG, where each stage is treated as an independent module. This level not only inherits the main processes from the Advanced RAG paradigm but also introduces an orchestration module to control the coordination of RAG processes. The middle level is composed of sub-modules within each module, further refining and optimizing the functions. The bottom level consists of basic units of operation—operators. Within the Modular RAG framework, RAG systems can be represented in the form of computational graphs, where nodes represent specific operators. The comparison of the three paradigms is shown in the Figure 3. Modular RAG evolves based on the previous development of RAG. The relationships among these three paradigms are ones of inheritance and development. Advanced RAG is a special case of Modular RAG, while Naive RAG is a special case of Advanced RAG.\n\nThe advantages of Modular RAG are significant, as it enhances the flexibility and scalability of RAG systems. Users can flexibly combine different modules and operators according to the requirements of data sources and task scenarios. In summary, the contributions of this paper are as follows: • This paper proposes a new paradigm called modular RAG, which employs a three-tier architectural design comprising modules, sub-modules, and operators to define the RAG system in a unified and structured manner. This design not only enhances the system’s flexibility and scalability but also, through the independent design of operators, strengthens the system’s maintainability and comprehensibility. • Under the framework of Modular RAG, the orchestration of modules and operators forms the RAG Flow, which can flexibly express current RAG methods. This paper has further summarized six typical flow patterns and specific methods have been analyzed to reveal the universality of modular RAG in practical scenarios. • The Modular RAG framework offers exceptional flexibility and extensibility. This paper delves into the new opportunities brought by Modular RAG and provides a thorough discussion on the adaptation and expansion of new methods in different application scenarios, offering guidance for future research directions and practical exploration.\n\nThe advantages of Modular RAG are significant, as it enhances the flexibility and scalability of RAG systems. Users can flexibly combine different modules and operators according to the requirements of data sources and task scenarios. In summary, the contributions of this paper are as follows:\n\nThis paper proposes a new paradigm called modular RAG, which employs a three-tier architectural design comprising modules, sub-modules, and operators to define the RAG system in a unified and structured manner. This design not only enhances the system’s flexibility and scalability but also, through the independent design of operators, strengthens the system’s maintainability and comprehensibility.\n\nThis paper proposes a new paradigm called modular RAG, which employs a three-tier architectural design comprising modules, sub-modules, and operators to define the RAG system in a unified and structured manner. This design not only enhances the system’s flexibility and scalability but also, through the independent design of operators, strengthens the system’s maintainability and comprehensibility.\n\nUnder the framework of Modular RAG, the orchestration of modules and operators forms the RAG Flow, which can flexibly express current RAG methods. This paper has further summarized six typical flow patterns and specific methods have been analyzed to reveal the universality of modular RAG in practical scenarios.\n\nUnder the framework of Modular RAG, the orchestration of modules and operators forms the RAG Flow, which can flexibly express current RAG methods. This paper has further summarized six typical flow patterns and specific methods have been analyzed to reveal the universality of modular RAG in practical scenarios.\n\nThe Modular RAG framework offers exceptional flexibility and extensibility. This paper delves into the new opportunities brought by Modular RAG and provides a thorough discussion on the adaptation and expansion of new methods in different application scenarios, offering guidance for future research directions and practical exploration.\n\nThe Modular RAG framework offers exceptional flexibility and extensibility. This paper delves into the new opportunities brought by Modular RAG and provides a thorough discussion on the adaptation and expansion of new methods in different application scenarios, offering guidance for future research directions and practical exploration.",
        "methodology": "Modular RAG paradigm demonstrates exceptional compatibility with new developments. To gain a deeper understanding of this, we list three typical scalability cases, which clearly shows that Modular RAG paradigm provides robust support and flexibility for the innovation and development of RAG technology.\n\nModular RAG paradigm demonstrates exceptional compatibility with new developments. To gain a deeper understanding of this, we list three typical scalability cases, which clearly shows that Modular RAG paradigm provides robust support and flexibility for the innovation and development of RAG technology.\n\nIn this scenario, no new modules or operators are proposed; rather, specific problems are addressed through the combination of existing modules.DR-RAG [59] employs a two-stage retrieval strategy and classifier selection mechanism, incorporating a branching retrieval structure. In the first stage, retrieving chunks relevant to the query. In the second stage, the query is combined individually with each chunk retrieved in the first stage, and a parallel secondary retrieval is conducted. The retrieved content is then input into a classifier to filter out the most relevant dynamic documents. This ensures that the retrieved documents are highly relevant to the query while reducing redundant information. DR-RAG improved retrieval method significantly enhances the accuracy and efficiency of answers, bolstering RAG’s performance in multi-hop question-answering scenarios.\n\nIn this scenario, no new modules or operators are proposed; rather, specific problems are addressed through the combination of existing modules.DR-RAG [59] employs a two-stage retrieval strategy and classifier selection mechanism, incorporating a branching retrieval structure. In the first stage, retrieving chunks relevant to the query. In the second stage, the query is combined individually with each chunk retrieved in the first stage, and a parallel secondary retrieval is conducted. The retrieved content is then input into a classifier to filter out the most relevant dynamic documents. This ensures that the retrieved documents are highly relevant to the query while reducing redundant information. DR-RAG improved retrieval method significantly enhances the accuracy and efficiency of answers, bolstering RAG’s performance in multi-hop question-answering scenarios.\n\nThis refers to redesigning the processes for retrieval and generation to address more complex scenarios without proposing new modules. The core idea of PlanRAG [18] lies in its introduction of a preliminary planning stage, a crucial step that occurs before retrieval and generation. Initially, the system employs a judge module to assess whether the current context necessitates the formulation of a new plan or adjustments to an existing one. When encountering a problem for the first time, the system initiates the planning process, while in subsequent interactions, it decides whether to execute re-planning based on previous plans and retrieved data.\n\nThis refers to redesigning the processes for retrieval and generation to address more complex scenarios without proposing new modules. The core idea of PlanRAG [18] lies in its introduction of a preliminary planning stage, a crucial step that occurs before retrieval and generation. Initially, the system employs a judge module to assess whether the current context necessitates the formulation of a new plan or adjustments to an existing one. When encountering a problem for the first time, the system initiates the planning process, while in subsequent interactions, it decides whether to execute re-planning based on previous plans and retrieved data.\n\nNext, the system devises an execution plan tailored to the query, treating this process as a logical decomposition of complex queries. Specifically, PlanRAG uses a query expansion module to extend and refine the query. For each derived sub-query, the system conducts targeted retrieval. Following retrieval, another judge module evaluates the current results to decide whether further retrieval is required or if it should return to the planning stage for re-planning. Through this strategy, PlanRAG is able to handle complex decision-making problems that require multi-step data analysis more efficiently.\n\nNext, the system devises an execution plan tailored to the query, treating this process as a logical decomposition of complex queries. Specifically, PlanRAG uses a query expansion module to extend and refine the query. For each derived sub-query, the system conducts targeted retrieval. Following retrieval, another judge module evaluates the current results to decide whether further retrieval is required or if it should return to the planning stage for re-planning. Through this strategy, PlanRAG is able to handle complex decision-making problems that require multi-step data analysis more efficiently.\n\nNew operators often introduce novel flow design, exemplified by Multi-Head RAG [60]. Existing RAG solutions do not focus on queries that may require retrieving multiple documents with significantly different content. Such queries are common but difficult to handle because embeddings of these documents may be far apart in the embedding space. Multi-Head RAG addresses this by designing a new retriever that uses the activations of the multi-head attention layers of the Transformer, rather than the decoder layers, as keys for retrieving multifaceted documents. Different attention heads can learn to capture different aspects of the data. By using the corresponding activation results, embeddings that represent different aspects of the data items and the query can be generated, thereby enhancing the retrieval accuracy for complex queries.\n\nNew operators often introduce novel flow design, exemplified by Multi-Head RAG [60]. Existing RAG solutions do not focus on queries that may require retrieving multiple documents with significantly different content. Such queries are common but difficult to handle because embeddings of these documents may be far apart in the embedding space. Multi-Head RAG addresses this by designing a new retriever that uses the activations of the multi-head attention layers of the Transformer, rather than the decoder layers, as keys for retrieving multifaceted documents. Different attention heads can learn to capture different aspects of the data. By using the corresponding activation results, embeddings that represent different aspects of the data items and the query can be generated, thereby enhancing the retrieval accuracy for complex queries.",
        "conclusion": "RAG is emerging as a pivotal technology for LLM applications. As technological landscapes evolve and the intricacies of application requirements escalate, RAG systems are being enhanced by integrating a diverse suite of technologies, thereby achieving a higher level of complexity and functionality. This paper introduces the innovative paradigm of Modular RAG. This approach systematically disassembles the complex architecture of RAG systems into well-defined, discrete functional modules. Each module is meticulously characterized by its specific operational functions, ensuring clarity and precision. Therefore, the entire system is composed of those modules and operators, akin to Lego bricks. By conducting an in-depth analysis of numerous studies, the paper also distills common RAG design patterns and scrutinizes key case studies to illustrate these patterns in practice.\n\nRAG is emerging as a pivotal technology for LLM applications. As technological landscapes evolve and the intricacies of application requirements escalate, RAG systems are being enhanced by integrating a diverse suite of technologies, thereby achieving a higher level of complexity and functionality. This paper introduces the innovative paradigm of Modular RAG. This approach systematically disassembles the complex architecture of RAG systems into well-defined, discrete functional modules. Each module is meticulously characterized by its specific operational functions, ensuring clarity and precision. Therefore, the entire system is composed of those modules and operators, akin to Lego bricks. By conducting an in-depth analysis of numerous studies, the paper also distills common RAG design patterns and scrutinizes key case studies to illustrate these patterns in practice.\n\nModular RAG not only offers a structured framework for the design and application of RAG systems but also enables a scenario-based customization of these systems. The modularity inherent in this design facilitates ease of tracking and debugging, significantly enhancing the maintainability and scalability of RAG systems. Furthermore, Modular RAG opens up new avenues for the future progression of RAG technology. It encourages the innovation of novel functional modules and the crafting of innovative workflows, thereby driving forward the frontiers of RAG systems.\n\nModular RAG not only offers a structured framework for the design and application of RAG systems but also enables a scenario-based customization of these systems. The modularity inherent in this design facilitates ease of tracking and debugging, significantly enhancing the maintainability and scalability of RAG systems. Furthermore, Modular RAG opens up new avenues for the future progression of RAG technology. It encourages the innovation of novel functional modules and the crafting of innovative workflows, thereby driving forward the frontiers of RAG systems."
      }
    },
    {
      "arxiv_id": "2505.13006v1",
      "title": "Evaluating the Performance of RAG Methods for Conversational AI in the   Airport Domain",
      "authors": [
        "Yuyang Li",
        "Philip J. M. Kerbusch",
        "Raimon H. R. Pruim",
        "Tobias Käfer"
      ],
      "published": "2025-05-19T11:46:30Z",
      "updated": "2025-05-19T11:46:30Z",
      "summary": "Airports from the top 20 in terms of annual passengers are highly dynamic environments with thousands of flights daily, and they aim to increase the degree of automation. To contribute to this, we implemented a Conversational AI system that enables staff in an airport to communicate with flight information systems. This system not only answers standard airport queries but also resolves airport terminology, jargon, abbreviations, and dynamic questions involving reasoning. In this paper, we built three different Retrieval-Augmented Generation (RAG) methods, including traditional RAG, SQL RAG, and Knowledge Graph-based RAG (Graph RAG). Experiments showed that traditional RAG achieved 84.84% accuracy using BM25 + GPT-4 but occasionally produced hallucinations, which is risky to airport safety. In contrast, SQL RAG and Graph RAG achieved 80.85% and 91.49% accuracy respectively, with significantly fewer hallucinations. Moreover, Graph RAG was especially effective for questions that involved reasoning. Based on our observations, we thus recommend SQL RAG and Graph RAG are better for airport environments, due to fewer hallucinations and the ability to handle dynamic questions.",
      "html_url": "https://arxiv.org/html/2505.13006v1",
      "sections": {
        "introduction": "Amsterdam Airport Schiphol, one of the top 20 airports in the world, ranked by annual passenger numbers, handles thousands of flights each day. These airports rely on staff like gate planners and apron controllers to access and update data across systems. For these employees, traditional database queries can be complex and time-consuming for some employees who are not query experts when they need flight information. A conversational AI system with a natural language query (NLQ) interface allows all employees to interact with systems naturally, asking questions like, “Which fights are at ramp D07?” and receiving instant answers. This improves productivity, and streamlines workflows, especially in high-pressure areas like at the gate, where less educated workers require access to up-to-date information. By replacing strict query formats with intuitive, real-time responses, conversational AI enhances decision-making and efficiency, making it a suitable solution for dynamic environments such as airports.\n\nAmsterdam Airport Schiphol, one of the top 20 airports in the world, ranked by annual passenger numbers, handles thousands of flights each day. These airports rely on staff like gate planners and apron controllers to access and update data across systems. For these employees, traditional database queries can be complex and time-consuming for some employees who are not query experts when they need flight information. A conversational AI system with a natural language query (NLQ) interface allows all employees to interact with systems naturally, asking questions like, “Which fights are at ramp D07?” and receiving instant answers. This improves productivity, and streamlines workflows, especially in high-pressure areas like at the gate, where less educated workers require access to up-to-date information. By replacing strict query formats with intuitive, real-time responses, conversational AI enhances decision-making and efficiency, making it a suitable solution for dynamic environments such as airports.\n\nBuilding such a system is challenging because flight data is stored by experts in tables using aviation abbreviations. We need our system to understand these datasets to answer questions from the airport domain. Additionally, ensuring aviation safety is a major concern; the system must be safe and enable employees to perform accurate operations. We address those challenges using two research questions.\n\nBuilding such a system is challenging because flight data is stored by experts in tables using aviation abbreviations. We need our system to understand these datasets to answer questions from the airport domain. Additionally, ensuring aviation safety is a major concern; the system must be safe and enable employees to perform accurate operations. We address those challenges using two research questions.\n\nThe first question is how to handle flight data so that our system can answer different questions. We divided the questions into three types: • Straightforward questions: Questions that can be directly answered from the flight data. • Questions involving specialized airport jargon, abbreviations, and incomplete queries: Operators often use shorthand or omit context. Flight “KL0123” might be referred to as “0123” or “123,” while gate “C05” might be shortened to “C5.” Abbreviations like “KLM” for “KLM Royal Dutch Airlines” or “Delta” for “Delta Air Lines” are also common. Operators frequently ask short, incomplete questions, e. g., “Which flights are at D04?” or “What is the gate for that Delta airline?” Without resolving missing details such, these questions cannot be answered. • Dynamic questions: Questions that involve additional calculations and reasoning, especially related to time. Examples include “What is the connecting flight’s onramp time for DL1000?” or “What is DL1000’s next flight from the same ramp?” These queries require reasoning through connections between flights and retrieving specific details.\n\nThe first question is how to handle flight data so that our system can answer different questions. We divided the questions into three types:\n\nStraightforward questions: Questions that can be directly answered from the flight data.\n\nStraightforward questions: Questions that can be directly answered from the flight data.\n\nQuestions involving specialized airport jargon, abbreviations, and incomplete queries: Operators often use shorthand or omit context. Flight “KL0123” might be referred to as “0123” or “123,” while gate “C05” might be shortened to “C5.” Abbreviations like “KLM” for “KLM Royal Dutch Airlines” or “Delta” for “Delta Air Lines” are also common. Operators frequently ask short, incomplete questions, e. g., “Which flights are at D04?” or “What is the gate for that Delta airline?” Without resolving missing details such, these questions cannot be answered.\n\nQuestions involving specialized airport jargon, abbreviations, and incomplete queries: Operators often use shorthand or omit context. Flight “KL0123” might be referred to as “0123” or “123,” while gate “C05” might be shortened to “C5.” Abbreviations like “KLM” for “KLM Royal Dutch Airlines” or “Delta” for “Delta Air Lines” are also common. Operators frequently ask short, incomplete questions, e. g., “Which flights are at D04?” or “What is the gate for that Delta airline?” Without resolving missing details such, these questions cannot be answered.\n\nDynamic questions: Questions that involve additional calculations and reasoning, especially related to time. Examples include “What is the connecting flight’s onramp time for DL1000?” or “What is DL1000’s next flight from the same ramp?” These queries require reasoning through connections between flights and retrieving specific details.\n\nDynamic questions: Questions that involve additional calculations and reasoning, especially related to time. Examples include “What is the connecting flight’s onramp time for DL1000?” or “What is DL1000’s next flight from the same ramp?” These queries require reasoning through connections between flights and retrieving specific details.\n\nThe second research question is about how to reduce hallucinations Xu et al. (2024) for the safety of aviation operations. Hallucinations occur when LLMs generate information not based on facts or their training data. In high-safety environments such as airports, however the output should be factual and not imaginative Jacobs and Jaschke (2024). For example, if the system gives wrong gate numbers, flight schedules, or safety instructions, this might disrupt aviation operations, cause delays, or even risk passenger safety. Thus, accurate responses are important.\n\nThe second research question is about how to reduce hallucinations Xu et al. (2024) for the safety of aviation operations. Hallucinations occur when LLMs generate information not based on facts or their training data. In high-safety environments such as airports, however the output should be factual and not imaginative Jacobs and Jaschke (2024). For example, if the system gives wrong gate numbers, flight schedules, or safety instructions, this might disrupt aviation operations, cause delays, or even risk passenger safety. Thus, accurate responses are important.\n\nIn this case study, we examine three Retrieval-Augmented Generation (RAG) techniques for the airport environment: Traditional RAG Lewis et al. (2021) Retrieves relevant information from the flight database and uses LLMs to generate answers based on the retrieved data and original questions. SQL RAG Guo et al. (2023) stores all datasets in an SQL database and converts natural language questions (NLQ) into structured SQL queries. Knowledge Graph-based Retrieval-Augmented Generation (Graph RAG) Edge et al. (2024) aims to improve the performance of LLM tasks by applying RAG techniques to Knowledge Graphs (KGs), requiring the original datasets to be stored in the knowledge graph. A key challenge is retrieving the correct flight information from thousands of flights while minimizing hallucinations.\n\nIn this case study, we examine three Retrieval-Augmented Generation (RAG) techniques for the airport environment: Traditional RAG Lewis et al. (2021) Retrieves relevant information from the flight database and uses LLMs to generate answers based on the retrieved data and original questions. SQL RAG Guo et al. (2023) stores all datasets in an SQL database and converts natural language questions (NLQ) into structured SQL queries. Knowledge Graph-based Retrieval-Augmented Generation (Graph RAG) Edge et al. (2024) aims to improve the performance of LLM tasks by applying RAG techniques to Knowledge Graphs (KGs), requiring the original datasets to be stored in the knowledge graph. A key challenge is retrieving the correct flight information from thousands of flights while minimizing hallucinations.\n\nThe paper is structured as follows: We first survey related work (Sec. 2), then present our dataset (Sec. 3), followed by a high-level description of our experiments (Sec. 4). We then present the results for the research questions (Sec. 5), and lastly conclude (Sec. 6). In the Appendix A, we provide further details, especially on the question generation and classification, next to our prompting.\n\nThe paper is structured as follows: We first survey related work (Sec. 2), then present our dataset (Sec. 3), followed by a high-level description of our experiments (Sec. 4). We then present the results for the research questions (Sec. 5), and lastly conclude (Sec. 6). In the Appendix A, we provide further details, especially on the question generation and classification, next to our prompting.",
        "methodology": "Section not found (searched for: methodology, method, approach)",
        "conclusion": "Our evaluation of three RAG methods shows that of the traditional RAG methods, BM25+GPT-4 is more efficient than other methods, because of the terminology used in the airport. However, traditional RAG can produce hallucinations, which poses a safety risk. SQL RAG and Graph RAG produce fewer hallucinations, and Graph RAG on top has higher accuracy. Our overall system effectively handles specialized airport terminology through question classification and prompt engineering; specifically, we address airport jargon and abbreviations. Graph RAG is particularly effective in handling reasoning tasks and questions about dynamic data, making it efficient in the airport domain.\n\nOur evaluation of three RAG methods shows that of the traditional RAG methods, BM25+GPT-4 is more efficient than other methods, because of the terminology used in the airport. However, traditional RAG can produce hallucinations, which poses a safety risk. SQL RAG and Graph RAG produce fewer hallucinations, and Graph RAG on top has higher accuracy. Our overall system effectively handles specialized airport terminology through question classification and prompt engineering; specifically, we address airport jargon and abbreviations. Graph RAG is particularly effective in handling reasoning tasks and questions about dynamic data, making it efficient in the airport domain."
      }
    },
    {
      "arxiv_id": "2506.09542v1",
      "title": "KG-Infused RAG: Augmenting Corpus-Based RAG with External Knowledge   Graphs",
      "authors": [
        "Dingjun Wu",
        "Yukun Yan",
        "Zhenghao Liu",
        "Zhiyuan Liu",
        "Maosong Sun"
      ],
      "published": "2025-06-11T09:20:02Z",
      "updated": "2025-06-11T09:20:02Z",
      "summary": "Retrieval-Augmented Generation (RAG) improves factual accuracy by grounding responses in external knowledge. However, existing methods typically rely on a single source, either unstructured text or structured knowledge. Moreover, they lack cognitively inspired mechanisms for activating relevant knowledge. To address these issues, we propose KG-Infused RAG, a framework that integrates KGs into RAG systems to implement spreading activation, a cognitive process that enables concept association and inference. KG-Infused RAG retrieves KG facts, expands the query accordingly, and enhances generation by combining corpus passages with structured facts, enabling interpretable, multi-source retrieval grounded in semantic structure. We further improve KG-Infused RAG via preference learning on sampled key stages in the pipeline. Experiments on five QA benchmarks show that KG-Infused RAG consistently outperforms vanilla RAG (by 3.8% to 13.8%). Additionally, when integrated into Self-RAG, KG-Infused RAG brings further performance gains, demonstrating its effectiveness and versatility as a plug-and-play enhancement module for corpus-based RAG methods.",
      "html_url": "https://arxiv.org/html/2506.09542v1",
      "sections": {
        "introduction": "Large language models (LLMs) have shown strong performance in question answering tasks [1, 10, 30, 5], yet they remain susceptible to factual errors and hallucinations [19, 21, 8]. Retrieval-Augmented Generation (RAG) mitigates these issues by grounding generation in external sources such as text corpus or knowledge graphs (KGs) [17, 25, 2, 3, 28]. However, existing RAG methods typically rely on a single retrieval source: corpus-based methods retrieve passages [8, 22, 25, 2], while KG-based methods extract facts from an external graph [3, 28]. This single-source constraint limits their ability to integrate complementary evidence from both unstructured and structured knowledge. Recent hybrid approaches attempt to bridge this gap by constructing ad-hoc KGs from text using LLMs [6, 11, 32], but they often suffer from high computation costs and potential factual inaccuracies due to imperfect entity and relation extraction.\n\nLarge language models (LLMs) have shown strong performance in question answering tasks [1, 10, 30, 5], yet they remain susceptible to factual errors and hallucinations [19, 21, 8]. Retrieval-Augmented Generation (RAG) mitigates these issues by grounding generation in external sources such as text corpus or knowledge graphs (KGs) [17, 25, 2, 3, 28]. However, existing RAG methods typically rely on a single retrieval source: corpus-based methods retrieve passages [8, 22, 25, 2], while KG-based methods extract facts from an external graph [3, 28]. This single-source constraint limits their ability to integrate complementary evidence from both unstructured and structured knowledge. Recent hybrid approaches attempt to bridge this gap by constructing ad-hoc KGs from text using LLMs [6, 11, 32], but they often suffer from high computation costs and potential factual inaccuracies due to imperfect entity and relation extraction.\n\nMore fundamentally, current RAG methods lack the ability to retrieve knowledge by activating semantically related concepts based on prior knowledge structures. This cognitive process, known as spreading activation [4], originates from cognitive psychology and explains how humans access relevant information by traversing semantic associations, even when it is not explicitly mentioned. In RAG, this mechanism can be implemented through the use of KGs, where a seed concept (e.g., an entity in the question) activates connected nodes (e.g., related entities or facts), guiding the retrieval toward semantically relevant evidence. This enables richer knowledge coverage, improves recall of long-tail or implicit facts, and provides a more interpretable reasoning path compared to relying solely on surface-form matching or parametric memory in LLMs.\n\nMore fundamentally, current RAG methods lack the ability to retrieve knowledge by activating semantically related concepts based on prior knowledge structures. This cognitive process, known as spreading activation [4], originates from cognitive psychology and explains how humans access relevant information by traversing semantic associations, even when it is not explicitly mentioned. In RAG, this mechanism can be implemented through the use of KGs, where a seed concept (e.g., an entity in the question) activates connected nodes (e.g., related entities or facts), guiding the retrieval toward semantically relevant evidence. This enables richer knowledge coverage, improves recall of long-tail or implicit facts, and provides a more interpretable reasoning path compared to relying solely on surface-form matching or parametric memory in LLMs.\n\nTo efficiently implement spreading activation, we leverage existing human-curated KGs (e.g., Wikidata), which provide high-quality, structured, and long-tail factual knowledge. Unlike prior work that constructs ad-hoc KGs [6, 11, 32] from text using LLMs—often prone to factual errors and computational costs—those curated KGs offer a stable and cost-effective foundation for semantically guided retrieval. Using KGs as external memory networks enable RAG systems to traverse meaningful concept associations, improving both recall and interpretability of the retrieved evidence.\n\nTo efficiently implement spreading activation, we leverage existing human-curated KGs (e.g., Wikidata), which provide high-quality, structured, and long-tail factual knowledge. Unlike prior work that constructs ad-hoc KGs [6, 11, 32] from text using LLMs—often prone to factual errors and computational costs—those curated KGs offer a stable and cost-effective foundation for semantically guided retrieval. Using KGs as external memory networks enable RAG systems to traverse meaningful concept associations, improving both recall and interpretability of the retrieved evidence.\n\nBased on this insight, we introduce Knowledge Graph Infused Retrieval-Augmented Generation (KG-Infused RAG), a framework that integrates KGs into the RAG pipeline to implement spreading activation. KG-Infused RAG consists of three stages: (1) retrieving relevant KG facts via spreading activation; (2) expanding the query with these facts to improve corpus retrieval; (3) generating answers conditioned on passages enriched with KG facts. This design enables multi-source retrieval that combines KG facts and corpus passages, leveraging KG structure to guide spreading activation. By aligning retrieval with human-like spreading activation, KG-Infused RAG supports more accurate, interpretable, and fact-grounded QA. Moreover, it can be integrated into corpus-based RAG methods (e.g., Self-RAG) to further boost performance.\n\nBased on this insight, we introduce Knowledge Graph Infused Retrieval-Augmented Generation (KG-Infused RAG), a framework that integrates KGs into the RAG pipeline to implement spreading activation. KG-Infused RAG consists of three stages: (1) retrieving relevant KG facts via spreading activation; (2) expanding the query with these facts to improve corpus retrieval; (3) generating answers conditioned on passages enriched with KG facts. This design enables multi-source retrieval that combines KG facts and corpus passages, leveraging KG structure to guide spreading activation. By aligning retrieval with human-like spreading activation, KG-Infused RAG supports more accurate, interpretable, and fact-grounded QA. Moreover, it can be integrated into corpus-based RAG methods (e.g., Self-RAG) to further boost performance.\n\nTo further improve KG-Infused RAG, we apply preference learning on sampled key pipeline stages for targeted tuning. Experiments on multiple benchmarks demonstrate consistent performance gains and show that KG-Infused RAG significantly enhances retrieval quality compared to vanilla RAG.\n\nTo further improve KG-Infused RAG, we apply preference learning on sampled key pipeline stages for targeted tuning. Experiments on multiple benchmarks demonstrate consistent performance gains and show that KG-Infused RAG significantly enhances retrieval quality compared to vanilla RAG.\n\nOur main contributions are summarized as follows: 1. We introduce KG-Infused RAG, a framework that incorporates human-curated knowledge graphs (KGs) into RAG to emulate spreading activation for semantically guided retrieval and generation. 2. KG-Infused RAG supports multi-source retrieval and can be integrated into corpus-based RAG methods (e.g., Self-RAG) as a plug-and-play module. 3. We develop a data sampling method to improve preference learning and model performance. 4. Experiments on five QA benchmarks show that KG-Infused RAG consistently improves performance, with absolute gains of 3.8% to 13.8% over vanilla RAG and substantial improvements when combined with Self-RAG.\n\nOur main contributions are summarized as follows:\n\nWe introduce KG-Infused RAG, a framework that incorporates human-curated knowledge graphs (KGs) into RAG to emulate spreading activation for semantically guided retrieval and generation.\n\nWe introduce KG-Infused RAG, a framework that incorporates human-curated knowledge graphs (KGs) into RAG to emulate spreading activation for semantically guided retrieval and generation.\n\nKG-Infused RAG supports multi-source retrieval and can be integrated into corpus-based RAG methods (e.g., Self-RAG) as a plug-and-play module.\n\nKG-Infused RAG supports multi-source retrieval and can be integrated into corpus-based RAG methods (e.g., Self-RAG) as a plug-and-play module.\n\nWe develop a data sampling method to improve preference learning and model performance.\n\nWe develop a data sampling method to improve preference learning and model performance.\n\nExperiments on five QA benchmarks show that KG-Infused RAG consistently improves performance, with absolute gains of 3.8% to 13.8% over vanilla RAG and substantial improvements when combined with Self-RAG.\n\nExperiments on five QA benchmarks show that KG-Infused RAG consistently improves performance, with absolute gains of 3.8% to 13.8% over vanilla RAG and substantial improvements when combined with Self-RAG.",
        "methodology": "Given a question q𝑞qitalic_q, a KG 𝒢𝒢\\mathcal{G}caligraphic_G, and a corpus 𝒟csubscript𝒟𝑐\\mathcal{D}_{c}caligraphic_D start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT, the goal is to generate an answer a𝑎aitalic_a by retrieving structured facts from 𝒢𝒢\\mathcal{G}caligraphic_G and textual passages from 𝒟csubscript𝒟𝑐\\mathcal{D}_{c}caligraphic_D start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT, and integrating them for final answer generation.\n\nGiven a question q𝑞qitalic_q, a KG 𝒢𝒢\\mathcal{G}caligraphic_G, and a corpus 𝒟csubscript𝒟𝑐\\mathcal{D}_{c}caligraphic_D start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT, the goal is to generate an answer a𝑎aitalic_a by retrieving structured facts from 𝒢𝒢\\mathcal{G}caligraphic_G and textual passages from 𝒟csubscript𝒟𝑐\\mathcal{D}_{c}caligraphic_D start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT, and integrating them for final answer generation.\n\nLet 𝒢q⊂𝒢subscript𝒢𝑞𝒢\\mathcal{G}_{q}\\subset\\mathcal{G}caligraphic_G start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT ⊂ caligraphic_G denote the retrieved KG subgraph composed of query-relevant triples, and 𝒫q⊂𝒟csubscript𝒫𝑞subscript𝒟𝑐\\mathcal{P}_{q}\\subset\\mathcal{D}_{c}caligraphic_P start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT ⊂ caligraphic_D start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT the corresponding retrieved passages. As the KG can be viewed as a semantic network of concept-level associations, the entities in 𝒢qsubscript𝒢𝑞\\mathcal{G}_{q}caligraphic_G start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT serve as seeds for activation propagation over the KG. Formally, the task can be framed as: answer=LLM⁢(q,𝒢q,𝒫q)answerLLM𝑞subscript𝒢𝑞subscript𝒫𝑞\\text{answer}=\\text{LLM}\\left(q,\\mathcal{G}_{q},\\mathcal{P}_{q}\\right)answer = LLM ( italic_q , caligraphic_G start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT , caligraphic_P start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT ) (1)\n\nLet 𝒢q⊂𝒢subscript𝒢𝑞𝒢\\mathcal{G}_{q}\\subset\\mathcal{G}caligraphic_G start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT ⊂ caligraphic_G denote the retrieved KG subgraph composed of query-relevant triples, and 𝒫q⊂𝒟csubscript𝒫𝑞subscript𝒟𝑐\\mathcal{P}_{q}\\subset\\mathcal{D}_{c}caligraphic_P start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT ⊂ caligraphic_D start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT the corresponding retrieved passages. As the KG can be viewed as a semantic network of concept-level associations, the entities in 𝒢qsubscript𝒢𝑞\\mathcal{G}_{q}caligraphic_G start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT serve as seeds for activation propagation over the KG. Formally, the task can be framed as:\n\nWe propose KG-Infused RAG, a framework inspired by the cognitive process of spreading activation, where activation spreads from a central concept to semantically associated ones. By leveraging the semantic structure of KGs, KG-Infused RAG expands relevant knowledge via multi-hop associations, enabling more effective retrieval and integration of KG facts and textual passages. The overall process is illustrated in Figure 1, consisting of three main modules:\n\nWe propose KG-Infused RAG, a framework inspired by the cognitive process of spreading activation, where activation spreads from a central concept to semantically associated ones. By leveraging the semantic structure of KGs, KG-Infused RAG expands relevant knowledge via multi-hop associations, enabling more effective retrieval and integration of KG facts and textual passages. The overall process is illustrated in Figure 1, consisting of three main modules:\n\n1) KG-Guided Spreading Activation. Starting from query-relevant entities, we iteratively expand semantically related triples to build a task-specific subgraph 𝒢qsubscript𝒢𝑞\\mathcal{G}_{q}caligraphic_G start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT, which is then summarized for downstream use.\n\n1) KG-Guided Spreading Activation. Starting from query-relevant entities, we iteratively expand semantically related triples to build a task-specific subgraph 𝒢qsubscript𝒢𝑞\\mathcal{G}_{q}caligraphic_G start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT, which is then summarized for downstream use.\n\n2) KG-Based Query Expansion (KG-Based QE). The LLM leverages both the original query and the expanded KG subgraph to generate an expanded query, improving retrieval over the corpus 𝒟csubscript𝒟𝑐\\mathcal{D}_{c}caligraphic_D start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT.\n\n2) KG-Based Query Expansion (KG-Based QE). The LLM leverages both the original query and the expanded KG subgraph to generate an expanded query, improving retrieval over the corpus 𝒟csubscript𝒟𝑐\\mathcal{D}_{c}caligraphic_D start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT.\n\n3) KG-Augmented Answer Generation (KG-Aug Gen). The LLM generates the answer by integrating retrieved passages and KG summaries, ensuring fact-grounded and interpretable generation.\n\n3) KG-Augmented Answer Generation (KG-Aug Gen). The LLM generates the answer by integrating retrieved passages and KG summaries, ensuring fact-grounded and interpretable generation.\n\nDetails of all prompts used in the framework are in Appendix D.1.\n\nDetails of all prompts used in the framework are in Appendix D.1.\n\nWe prepare our KG 𝒢𝒢\\mathcal{G}caligraphic_G by extracting and preprocessing triples from Wikidata5M [27], a large-scale knowledge graph dataset derived from Wikidata and Wikipedia. Wikidata5M spans multiple domains, making it suitable for open-domain QA tasks. The KG used in KG-Infused RAG is defined as: 𝒢={⟨e,r,d⟩∣e∈ℰ,r∈ℛ,d∈𝒟e}𝒢conditional-set𝑒𝑟𝑑formulae-sequence𝑒ℰformulae-sequence𝑟ℛ𝑑subscript𝒟𝑒\\mathcal{G}=\\left\\{\\langle e,r,d\\rangle\\mid e\\in\\mathcal{E},r\\in\\mathcal{R},d% \\in\\mathcal{D}_{e}\\right\\}caligraphic_G = { ⟨ italic_e , italic_r , italic_d ⟩ ∣ italic_e ∈ caligraphic_E , italic_r ∈ caligraphic_R , italic_d ∈ caligraphic_D start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT } (2)\n\nWe prepare our KG 𝒢𝒢\\mathcal{G}caligraphic_G by extracting and preprocessing triples from Wikidata5M [27], a large-scale knowledge graph dataset derived from Wikidata and Wikipedia. Wikidata5M spans multiple domains, making it suitable for open-domain QA tasks. The KG used in KG-Infused RAG is defined as:\n\nwhere ℰℰ\\mathcal{E}caligraphic_E, ℛℛ\\mathcal{R}caligraphic_R, and 𝒟esubscript𝒟𝑒\\mathcal{D}_{e}caligraphic_D start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT denote the set of entities, relations, and entity descriptions, respectively. We preprocess Wikidata5M to meet our requirements for the KG, resulting in Wikidata5M-KG with approximately ~21M triples. The preprocessing details and statistics are in Appendix B.\n\nwhere ℰℰ\\mathcal{E}caligraphic_E, ℛℛ\\mathcal{R}caligraphic_R, and 𝒟esubscript𝒟𝑒\\mathcal{D}_{e}caligraphic_D start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT denote the set of entities, relations, and entity descriptions, respectively. We preprocess Wikidata5M to meet our requirements for the KG, resulting in Wikidata5M-KG with approximately ~21M triples. The preprocessing details and statistics are in Appendix B.\n\nThis stage constructs a query-specific subgraph 𝒢qsubscript𝒢𝑞\\mathcal{G}_{q}caligraphic_G start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT by simulating spreading activation over the KG, starting from query-relevant entities and propagating through related facts. The retrieved structured knowledge complements corpus evidence and supports downstream retrieval and generation.\n\nThis stage constructs a query-specific subgraph 𝒢qsubscript𝒢𝑞\\mathcal{G}_{q}caligraphic_G start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT by simulating spreading activation over the KG, starting from query-relevant entities and propagating through related facts. The retrieved structured knowledge complements corpus evidence and supports downstream retrieval and generation.\n\nWe first retrieve the top-kesubscript𝑘𝑒k_{e}italic_k start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT entities most relevant to the query q𝑞qitalic_q as the starting points for spreading activation over the KG. Each entity eisubscript𝑒𝑖e_{i}italic_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is associated with a textual description di∈𝒟esubscript𝑑𝑖subscript𝒟𝑒d_{i}\\in\\mathcal{D}_{e}italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∈ caligraphic_D start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT. The similarity between the query and each entity is computed via inner product: sim⁢(q,di)=q⋅disim𝑞subscript𝑑𝑖⋅𝑞subscript𝑑𝑖\\text{sim}(q,d_{i})=q\\cdot d_{i}sim ( italic_q , italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) = italic_q ⋅ italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT (3) We then select the top-kesubscript𝑘𝑒k_{e}italic_k start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT entities whose descriptions have the highest similarity scores: Eq0={ei∈ℰ∣di∈TopKke⁢(𝒟e;sim⁢(q,di))}superscriptsubscript𝐸𝑞0conditional-setsubscript𝑒𝑖ℰsubscript𝑑𝑖subscriptTopKsubscript𝑘𝑒subscript𝒟𝑒sim𝑞subscript𝑑𝑖E_{q}^{0}=\\{e_{i}\\in\\mathcal{E}\\mid d_{i}\\in\\text{TopK}_{k_{e}}(\\mathcal{D}_{e% };\\text{sim}(q,d_{i}))\\}italic_E start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT = { italic_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∈ caligraphic_E ∣ italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∈ TopK start_POSTSUBSCRIPT italic_k start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( caligraphic_D start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT ; sim ( italic_q , italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) ) } (4)\n\nWe first retrieve the top-kesubscript𝑘𝑒k_{e}italic_k start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT entities most relevant to the query q𝑞qitalic_q as the starting points for spreading activation over the KG. Each entity eisubscript𝑒𝑖e_{i}italic_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is associated with a textual description di∈𝒟esubscript𝑑𝑖subscript𝒟𝑒d_{i}\\in\\mathcal{D}_{e}italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∈ caligraphic_D start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT. The similarity between the query and each entity is computed via inner product:\n\nWe then select the top-kesubscript𝑘𝑒k_{e}italic_k start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT entities whose descriptions have the highest similarity scores:\n\nHere, TopKke⁢(𝒟e;sim)subscriptTopKsubscript𝑘𝑒subscript𝒟𝑒sim\\text{TopK}_{k_{e}}(\\mathcal{D}_{e};\\text{sim})TopK start_POSTSUBSCRIPT italic_k start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( caligraphic_D start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT ; sim ) denotes the set of kesubscript𝑘𝑒k_{e}italic_k start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT descriptions in 𝒟esubscript𝒟𝑒\\mathcal{D}_{e}caligraphic_D start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT with the highest similarity to q𝑞qitalic_q, and eisubscript𝑒𝑖e_{i}italic_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is the entity associated with disubscript𝑑𝑖d_{i}italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT. The resulting entity set Eq0={e1,e2,…,eke}superscriptsubscript𝐸𝑞0subscript𝑒1subscript𝑒2…subscript𝑒subscript𝑘𝑒E_{q}^{0}=\\{e_{1},e_{2},\\dots,e_{k_{e}}\\}italic_E start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT = { italic_e start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_e start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , italic_e start_POSTSUBSCRIPT italic_k start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT end_POSTSUBSCRIPT } initializes the spreading activation process over the KG.\n\nHere, TopKke⁢(𝒟e;sim)subscriptTopKsubscript𝑘𝑒subscript𝒟𝑒sim\\text{TopK}_{k_{e}}(\\mathcal{D}_{e};\\text{sim})TopK start_POSTSUBSCRIPT italic_k start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( caligraphic_D start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT ; sim ) denotes the set of kesubscript𝑘𝑒k_{e}italic_k start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT descriptions in 𝒟esubscript𝒟𝑒\\mathcal{D}_{e}caligraphic_D start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT with the highest similarity to q𝑞qitalic_q, and eisubscript𝑒𝑖e_{i}italic_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is the entity associated with disubscript𝑑𝑖d_{i}italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT. The resulting entity set Eq0={e1,e2,…,eke}superscriptsubscript𝐸𝑞0subscript𝑒1subscript𝑒2…subscript𝑒subscript𝑘𝑒E_{q}^{0}=\\{e_{1},e_{2},\\dots,e_{k_{e}}\\}italic_E start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT = { italic_e start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_e start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , italic_e start_POSTSUBSCRIPT italic_k start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT end_POSTSUBSCRIPT } initializes the spreading activation process over the KG.\n\nStarting from the seed entities Eq0superscriptsubscript𝐸𝑞0E_{q}^{0}italic_E start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT, we perform iterative spreading activation over 𝒢𝒢\\mathcal{G}caligraphic_G, where each round i𝑖iitalic_i expands from the currently activated entities Eqisuperscriptsubscript𝐸𝑞𝑖E_{q}^{i}italic_E start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT to retrieve new query-relevant triples. Each round consists of:\n\nStarting from the seed entities Eq0superscriptsubscript𝐸𝑞0E_{q}^{0}italic_E start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT, we perform iterative spreading activation over 𝒢𝒢\\mathcal{G}caligraphic_G, where each round i𝑖iitalic_i expands from the currently activated entities Eqisuperscriptsubscript𝐸𝑞𝑖E_{q}^{i}italic_E start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT to retrieve new query-relevant triples. Each round consists of:\n\n1. Triple Selection. For each entity in the current set Eqisuperscriptsubscript𝐸𝑞𝑖E_{q}^{i}italic_E start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT, we retrieve its 1-hop neighbors from 𝒢𝒢\\mathcal{G}caligraphic_G. An LLM is prompted to select a subset of triples relevant to the query q𝑞qitalic_q. The selected triples for round i𝑖iitalic_i are denoted as {(e,r,e′)i}subscript𝑒𝑟superscript𝑒′𝑖\\{(e,r,e^{\\prime})_{i}\\}{ ( italic_e , italic_r , italic_e start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ) start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT }, where e𝑒eitalic_e, r𝑟ritalic_r, and e′superscript𝑒′e^{\\prime}italic_e start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT denote the head entity, relation, and tail entity in a triple respectively. 2. Activation Memory Construction and Updating. We maintain an Activation Memory ℳ={𝒢qa⁢c⁢t,Eqa⁢c⁢t}ℳsuperscriptsubscript𝒢𝑞𝑎𝑐𝑡superscriptsubscript𝐸𝑞𝑎𝑐𝑡\\mathcal{M}=\\left\\{\\mathcal{G}_{q}^{act},E_{q}^{act}\\right\\}caligraphic_M = { caligraphic_G start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_a italic_c italic_t end_POSTSUPERSCRIPT , italic_E start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_a italic_c italic_t end_POSTSUPERSCRIPT } to track the multi-round activation process, where the subscript a⁢c⁢t𝑎𝑐𝑡actitalic_a italic_c italic_t denotes the activation stage. Here, 𝒢qa⁢c⁢tsuperscriptsubscript𝒢𝑞𝑎𝑐𝑡\\mathcal{G}_{q}^{act}caligraphic_G start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_a italic_c italic_t end_POSTSUPERSCRIPT is the accumulated subgraph of all query-relevant triples retrieved so far, and Eqa⁢c⁢tsuperscriptsubscript𝐸𝑞𝑎𝑐𝑡E_{q}^{act}italic_E start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_a italic_c italic_t end_POSTSUPERSCRIPT is the set of entities activated up to the current round. This memory helps aggregate knowledge while preventing redundant reactivation of entities. At each round i𝑖iitalic_i, given the newly retrieved triples 𝒢qisuperscriptsubscript𝒢𝑞𝑖\\mathcal{G}_{q}^{i}caligraphic_G start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT and the associated entity Eqisuperscriptsubscript𝐸𝑞𝑖E_{q}^{i}italic_E start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT, we update the memory as follows: 𝒢qa⁢c⁢t←𝒢qa⁢c⁢t∪𝒢qiandEqa⁢c⁢t←Eqa⁢c⁢t∪Eqiformulae-sequence←superscriptsubscript𝒢𝑞𝑎𝑐𝑡superscriptsubscript𝒢𝑞𝑎𝑐𝑡superscriptsubscript𝒢𝑞𝑖and←superscriptsubscript𝐸𝑞𝑎𝑐𝑡superscriptsubscript𝐸𝑞𝑎𝑐𝑡superscriptsubscript𝐸𝑞𝑖\\mathcal{G}_{q}^{act}\\leftarrow\\mathcal{G}_{q}^{act}\\cup\\mathcal{G}_{q}^{i}\\ % \\ \\ \\text{and}\\ \\ \\ E_{q}^{act}\\leftarrow E_{q}^{act}\\cup E_{q}^{i}caligraphic_G start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_a italic_c italic_t end_POSTSUPERSCRIPT ← caligraphic_G start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_a italic_c italic_t end_POSTSUPERSCRIPT ∪ caligraphic_G start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT and italic_E start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_a italic_c italic_t end_POSTSUPERSCRIPT ← italic_E start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_a italic_c italic_t end_POSTSUPERSCRIPT ∪ italic_E start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT (5) where ∪\\cup∪ denote the set union operation. 3. Next-Round Activation Entities. At each round i𝑖iitalic_i, we determine the next-round entity set Eqi+1superscriptsubscript𝐸𝑞𝑖1E_{q}^{i+1}italic_E start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i + 1 end_POSTSUPERSCRIPT from the current triples 𝒢qi={(e,r,e′)i}superscriptsubscript𝒢𝑞𝑖subscript𝑒𝑟superscript𝑒′𝑖\\mathcal{G}_{q}^{i}=\\{(e,r,e^{\\prime})_{i}\\}caligraphic_G start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT = { ( italic_e , italic_r , italic_e start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ) start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } by extracting their tail entities {e′}superscript𝑒′\\{e^{\\prime}\\}{ italic_e start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT } as candidates for further activation. To prevent revisiting entities, we exclude those already in the activated entity set Eqa⁢c⁢tsuperscriptsubscript𝐸𝑞𝑎𝑐𝑡E_{q}^{act}italic_E start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_a italic_c italic_t end_POSTSUPERSCRIPT. Formally, Eqi+1=(⋃(e,r,e′)∈𝒢qi{e′})∖Eqa⁢c⁢tsuperscriptsubscript𝐸𝑞𝑖1subscript𝑒𝑟superscript𝑒′superscriptsubscript𝒢𝑞𝑖superscript𝑒′superscriptsubscript𝐸𝑞𝑎𝑐𝑡E_{q}^{i+1}=\\left(\\bigcup_{(e,r,e^{\\prime})\\in\\mathcal{G}_{q}^{i}}\\{e^{\\prime}% \\}\\right)\\setminus E_{q}^{act}italic_E start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i + 1 end_POSTSUPERSCRIPT = ( ⋃ start_POSTSUBSCRIPT ( italic_e , italic_r , italic_e start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ) ∈ caligraphic_G start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT end_POSTSUBSCRIPT { italic_e start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT } ) ∖ italic_E start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_a italic_c italic_t end_POSTSUPERSCRIPT (6) where the set difference ∖\\setminus∖ excludes previously activated entities, ensuring Eqi+1superscriptsubscript𝐸𝑞𝑖1E_{q}^{i+1}italic_E start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i + 1 end_POSTSUPERSCRIPT contains only newly activated entities to guide the next activation round.\n\nTriple Selection. For each entity in the current set Eqisuperscriptsubscript𝐸𝑞𝑖E_{q}^{i}italic_E start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT, we retrieve its 1-hop neighbors from 𝒢𝒢\\mathcal{G}caligraphic_G. An LLM is prompted to select a subset of triples relevant to the query q𝑞qitalic_q. The selected triples for round i𝑖iitalic_i are denoted as {(e,r,e′)i}subscript𝑒𝑟superscript𝑒′𝑖\\{(e,r,e^{\\prime})_{i}\\}{ ( italic_e , italic_r , italic_e start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ) start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT }, where e𝑒eitalic_e, r𝑟ritalic_r, and e′superscript𝑒′e^{\\prime}italic_e start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT denote the head entity, relation, and tail entity in a triple respectively.\n\nTriple Selection. For each entity in the current set Eqisuperscriptsubscript𝐸𝑞𝑖E_{q}^{i}italic_E start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT, we retrieve its 1-hop neighbors from 𝒢𝒢\\mathcal{G}caligraphic_G. An LLM is prompted to select a subset of triples relevant to the query q𝑞qitalic_q. The selected triples for round i𝑖iitalic_i are denoted as {(e,r,e′)i}subscript𝑒𝑟superscript𝑒′𝑖\\{(e,r,e^{\\prime})_{i}\\}{ ( italic_e , italic_r , italic_e start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ) start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT }, where e𝑒eitalic_e, r𝑟ritalic_r, and e′superscript𝑒′e^{\\prime}italic_e start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT denote the head entity, relation, and tail entity in a triple respectively.\n\nActivation Memory Construction and Updating. We maintain an Activation Memory ℳ={𝒢qa⁢c⁢t,Eqa⁢c⁢t}ℳsuperscriptsubscript𝒢𝑞𝑎𝑐𝑡superscriptsubscript𝐸𝑞𝑎𝑐𝑡\\mathcal{M}=\\left\\{\\mathcal{G}_{q}^{act},E_{q}^{act}\\right\\}caligraphic_M = { caligraphic_G start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_a italic_c italic_t end_POSTSUPERSCRIPT , italic_E start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_a italic_c italic_t end_POSTSUPERSCRIPT } to track the multi-round activation process, where the subscript a⁢c⁢t𝑎𝑐𝑡actitalic_a italic_c italic_t denotes the activation stage. Here, 𝒢qa⁢c⁢tsuperscriptsubscript𝒢𝑞𝑎𝑐𝑡\\mathcal{G}_{q}^{act}caligraphic_G start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_a italic_c italic_t end_POSTSUPERSCRIPT is the accumulated subgraph of all query-relevant triples retrieved so far, and Eqa⁢c⁢tsuperscriptsubscript𝐸𝑞𝑎𝑐𝑡E_{q}^{act}italic_E start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_a italic_c italic_t end_POSTSUPERSCRIPT is the set of entities activated up to the current round. This memory helps aggregate knowledge while preventing redundant reactivation of entities. At each round i𝑖iitalic_i, given the newly retrieved triples 𝒢qisuperscriptsubscript𝒢𝑞𝑖\\mathcal{G}_{q}^{i}caligraphic_G start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT and the associated entity Eqisuperscriptsubscript𝐸𝑞𝑖E_{q}^{i}italic_E start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT, we update the memory as follows: 𝒢qa⁢c⁢t←𝒢qa⁢c⁢t∪𝒢qiandEqa⁢c⁢t←Eqa⁢c⁢t∪Eqiformulae-sequence←superscriptsubscript𝒢𝑞𝑎𝑐𝑡superscriptsubscript𝒢𝑞𝑎𝑐𝑡superscriptsubscript𝒢𝑞𝑖and←superscriptsubscript𝐸𝑞𝑎𝑐𝑡superscriptsubscript𝐸𝑞𝑎𝑐𝑡superscriptsubscript𝐸𝑞𝑖\\mathcal{G}_{q}^{act}\\leftarrow\\mathcal{G}_{q}^{act}\\cup\\mathcal{G}_{q}^{i}\\ % \\ \\ \\text{and}\\ \\ \\ E_{q}^{act}\\leftarrow E_{q}^{act}\\cup E_{q}^{i}caligraphic_G start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_a italic_c italic_t end_POSTSUPERSCRIPT ← caligraphic_G start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_a italic_c italic_t end_POSTSUPERSCRIPT ∪ caligraphic_G start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT and italic_E start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_a italic_c italic_t end_POSTSUPERSCRIPT ← italic_E start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_a italic_c italic_t end_POSTSUPERSCRIPT ∪ italic_E start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT (5) where ∪\\cup∪ denote the set union operation.\n\nActivation Memory Construction and Updating. We maintain an Activation Memory ℳ={𝒢qa⁢c⁢t,Eqa⁢c⁢t}ℳsuperscriptsubscript𝒢𝑞𝑎𝑐𝑡superscriptsubscript𝐸𝑞𝑎𝑐𝑡\\mathcal{M}=\\left\\{\\mathcal{G}_{q}^{act},E_{q}^{act}\\right\\}caligraphic_M = { caligraphic_G start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_a italic_c italic_t end_POSTSUPERSCRIPT , italic_E start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_a italic_c italic_t end_POSTSUPERSCRIPT } to track the multi-round activation process, where the subscript a⁢c⁢t𝑎𝑐𝑡actitalic_a italic_c italic_t denotes the activation stage. Here, 𝒢qa⁢c⁢tsuperscriptsubscript𝒢𝑞𝑎𝑐𝑡\\mathcal{G}_{q}^{act}caligraphic_G start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_a italic_c italic_t end_POSTSUPERSCRIPT is the accumulated subgraph of all query-relevant triples retrieved so far, and Eqa⁢c⁢tsuperscriptsubscript𝐸𝑞𝑎𝑐𝑡E_{q}^{act}italic_E start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_a italic_c italic_t end_POSTSUPERSCRIPT is the set of entities activated up to the current round. This memory helps aggregate knowledge while preventing redundant reactivation of entities. At each round i𝑖iitalic_i, given the newly retrieved triples 𝒢qisuperscriptsubscript𝒢𝑞𝑖\\mathcal{G}_{q}^{i}caligraphic_G start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT and the associated entity Eqisuperscriptsubscript𝐸𝑞𝑖E_{q}^{i}italic_E start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT, we update the memory as follows:\n\nwhere ∪\\cup∪ denote the set union operation.\n\nNext-Round Activation Entities. At each round i𝑖iitalic_i, we determine the next-round entity set Eqi+1superscriptsubscript𝐸𝑞𝑖1E_{q}^{i+1}italic_E start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i + 1 end_POSTSUPERSCRIPT from the current triples 𝒢qi={(e,r,e′)i}superscriptsubscript𝒢𝑞𝑖subscript𝑒𝑟superscript𝑒′𝑖\\mathcal{G}_{q}^{i}=\\{(e,r,e^{\\prime})_{i}\\}caligraphic_G start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT = { ( italic_e , italic_r , italic_e start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ) start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } by extracting their tail entities {e′}superscript𝑒′\\{e^{\\prime}\\}{ italic_e start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT } as candidates for further activation. To prevent revisiting entities, we exclude those already in the activated entity set Eqa⁢c⁢tsuperscriptsubscript𝐸𝑞𝑎𝑐𝑡E_{q}^{act}italic_E start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_a italic_c italic_t end_POSTSUPERSCRIPT. Formally, Eqi+1=(⋃(e,r,e′)∈𝒢qi{e′})∖Eqa⁢c⁢tsuperscriptsubscript𝐸𝑞𝑖1subscript𝑒𝑟superscript𝑒′superscriptsubscript𝒢𝑞𝑖superscript𝑒′superscriptsubscript𝐸𝑞𝑎𝑐𝑡E_{q}^{i+1}=\\left(\\bigcup_{(e,r,e^{\\prime})\\in\\mathcal{G}_{q}^{i}}\\{e^{\\prime}% \\}\\right)\\setminus E_{q}^{act}italic_E start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i + 1 end_POSTSUPERSCRIPT = ( ⋃ start_POSTSUBSCRIPT ( italic_e , italic_r , italic_e start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ) ∈ caligraphic_G start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT end_POSTSUBSCRIPT { italic_e start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT } ) ∖ italic_E start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_a italic_c italic_t end_POSTSUPERSCRIPT (6) where the set difference ∖\\setminus∖ excludes previously activated entities, ensuring Eqi+1superscriptsubscript𝐸𝑞𝑖1E_{q}^{i+1}italic_E start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i + 1 end_POSTSUPERSCRIPT contains only newly activated entities to guide the next activation round.\n\nNext-Round Activation Entities. At each round i𝑖iitalic_i, we determine the next-round entity set Eqi+1superscriptsubscript𝐸𝑞𝑖1E_{q}^{i+1}italic_E start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i + 1 end_POSTSUPERSCRIPT from the current triples 𝒢qi={(e,r,e′)i}superscriptsubscript𝒢𝑞𝑖subscript𝑒𝑟superscript𝑒′𝑖\\mathcal{G}_{q}^{i}=\\{(e,r,e^{\\prime})_{i}\\}caligraphic_G start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT = { ( italic_e , italic_r , italic_e start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ) start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } by extracting their tail entities {e′}superscript𝑒′\\{e^{\\prime}\\}{ italic_e start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT } as candidates for further activation. To prevent revisiting entities, we exclude those already in the activated entity set Eqa⁢c⁢tsuperscriptsubscript𝐸𝑞𝑎𝑐𝑡E_{q}^{act}italic_E start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_a italic_c italic_t end_POSTSUPERSCRIPT. Formally,\n\nwhere the set difference ∖\\setminus∖ excludes previously activated entities, ensuring Eqi+1superscriptsubscript𝐸𝑞𝑖1E_{q}^{i+1}italic_E start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i + 1 end_POSTSUPERSCRIPT contains only newly activated entities to guide the next activation round.\n\nThe activation process terminates when either the predefined maximum number of activation rounds k𝑘kitalic_k is reached or no new entities remain, i.e., Eqi+1=∅superscriptsubscript𝐸𝑞𝑖1E_{q}^{i+1}=\\emptysetitalic_E start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i + 1 end_POSTSUPERSCRIPT = ∅. This iterative expansion progressively retrieves query-relevant triples from 𝒢𝒢\\mathcal{G}caligraphic_G, covering paths from 1-hop to k𝑘kitalic_k-hop neighborhoods.\n\nThe activation process terminates when either the predefined maximum number of activation rounds k𝑘kitalic_k is reached or no new entities remain, i.e., Eqi+1=∅superscriptsubscript𝐸𝑞𝑖1E_{q}^{i+1}=\\emptysetitalic_E start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i + 1 end_POSTSUPERSCRIPT = ∅. This iterative expansion progressively retrieves query-relevant triples from 𝒢𝒢\\mathcal{G}caligraphic_G, covering paths from 1-hop to k𝑘kitalic_k-hop neighborhoods.\n\nWe prompt the LLM to summarize the expanded KG subgraph 𝒢qa⁢c⁢tsuperscriptsubscript𝒢𝑞𝑎𝑐𝑡\\mathcal{G}_{q}^{act}caligraphic_G start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_a italic_c italic_t end_POSTSUPERSCRIPT, accumulated in the Activation Memory ℳℳ\\mathcal{M}caligraphic_M, into a natural language summary 𝒮𝒢,qa⁢c⁢tsuperscriptsubscript𝒮𝒢𝑞𝑎𝑐𝑡\\mathcal{S}_{\\mathcal{G},{q}}^{act}caligraphic_S start_POSTSUBSCRIPT caligraphic_G , italic_q end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_a italic_c italic_t end_POSTSUPERSCRIPT. This summarization serves two purposes: (1) It condenses discrete graph-structured facts into a coherent natural language narrative, revealing semantic paths and concept interactions underlying the query. (2) It converts structured knowledge into natural language, making it more accessible and usable for the LLM in subsequent stages.\n\nWe prompt the LLM to summarize the expanded KG subgraph 𝒢qa⁢c⁢tsuperscriptsubscript𝒢𝑞𝑎𝑐𝑡\\mathcal{G}_{q}^{act}caligraphic_G start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_a italic_c italic_t end_POSTSUPERSCRIPT, accumulated in the Activation Memory ℳℳ\\mathcal{M}caligraphic_M, into a natural language summary 𝒮𝒢,qa⁢c⁢tsuperscriptsubscript𝒮𝒢𝑞𝑎𝑐𝑡\\mathcal{S}_{\\mathcal{G},{q}}^{act}caligraphic_S start_POSTSUBSCRIPT caligraphic_G , italic_q end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_a italic_c italic_t end_POSTSUPERSCRIPT. This summarization serves two purposes: (1) It condenses discrete graph-structured facts into a coherent natural language narrative, revealing semantic paths and concept interactions underlying the query. (2) It converts structured knowledge into natural language, making it more accessible and usable for the LLM in subsequent stages.\n\nThe goal of this stage is to generate an expanded query q′superscript𝑞′q^{\\prime}italic_q start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT that complements the original query q𝑞qitalic_q by incorporating structured knowledge retrieved from the KG. The expansion broadens retrieval coverage and enhances the relevance of corpus evidence.\n\nThe goal of this stage is to generate an expanded query q′superscript𝑞′q^{\\prime}italic_q start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT that complements the original query q𝑞qitalic_q by incorporating structured knowledge retrieved from the KG. The expansion broadens retrieval coverage and enhances the relevance of corpus evidence.\n\nWe prompt the LLM with both the original query q𝑞qitalic_q and the KG subgraph summary 𝒮𝒢,qa⁢c⁢tsuperscriptsubscript𝒮𝒢𝑞𝑎𝑐𝑡\\mathcal{S}_{\\mathcal{G},q}^{act}caligraphic_S start_POSTSUBSCRIPT caligraphic_G , italic_q end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_a italic_c italic_t end_POSTSUPERSCRIPT constructed via spreading activation. Conditioned on the prompt, the model performs associative reasoning to transform the query—either simplifying it based on retrieved facts (e.g., replacing intermediate entities), or extending it with new, KG-grounded content inferred through LLM knowledge. Formally, q′=LLMprompt⁢(q,𝒮𝒢,qa⁢c⁢t)superscript𝑞′subscriptLLMprompt𝑞superscriptsubscript𝒮𝒢𝑞𝑎𝑐𝑡q^{\\prime}=\\text{LLM}_{\\text{prompt}}\\left(q,\\mathcal{S}_{\\mathcal{G},{q}}^{% act}\\right)italic_q start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT = LLM start_POSTSUBSCRIPT prompt end_POSTSUBSCRIPT ( italic_q , caligraphic_S start_POSTSUBSCRIPT caligraphic_G , italic_q end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_a italic_c italic_t end_POSTSUPERSCRIPT ) (7) We then perform dual-query retrieval over the corpus using both q𝑞qitalic_q and q′superscript𝑞′q^{\\prime}italic_q start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT, and merge the results: 𝒟c,(q,q′)=𝒟c,q∪𝒟c,q′subscript𝒟𝑐𝑞superscript𝑞′subscript𝒟𝑐𝑞subscript𝒟𝑐superscript𝑞′\\mathcal{D}_{c,(q,q^{\\prime})}=\\mathcal{D}_{c,q}\\cup\\mathcal{D}_{c,q^{\\prime}}caligraphic_D start_POSTSUBSCRIPT italic_c , ( italic_q , italic_q start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ) end_POSTSUBSCRIPT = caligraphic_D start_POSTSUBSCRIPT italic_c , italic_q end_POSTSUBSCRIPT ∪ caligraphic_D start_POSTSUBSCRIPT italic_c , italic_q start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT end_POSTSUBSCRIPT (8) where 𝒟c,qsubscript𝒟𝑐𝑞\\mathcal{D}_{c,q}caligraphic_D start_POSTSUBSCRIPT italic_c , italic_q end_POSTSUBSCRIPT and 𝒟c,q′subscript𝒟𝑐superscript𝑞′\\mathcal{D}_{c,q^{\\prime}}caligraphic_D start_POSTSUBSCRIPT italic_c , italic_q start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT end_POSTSUBSCRIPT denote the passage sets retrieved by q𝑞qitalic_q and q′superscript𝑞′q^{\\prime}italic_q start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT, respectively.\n\nWe prompt the LLM with both the original query q𝑞qitalic_q and the KG subgraph summary 𝒮𝒢,qa⁢c⁢tsuperscriptsubscript𝒮𝒢𝑞𝑎𝑐𝑡\\mathcal{S}_{\\mathcal{G},q}^{act}caligraphic_S start_POSTSUBSCRIPT caligraphic_G , italic_q end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_a italic_c italic_t end_POSTSUPERSCRIPT constructed via spreading activation. Conditioned on the prompt, the model performs associative reasoning to transform the query—either simplifying it based on retrieved facts (e.g., replacing intermediate entities), or extending it with new, KG-grounded content inferred through LLM knowledge. Formally,\n\nWe then perform dual-query retrieval over the corpus using both q𝑞qitalic_q and q′superscript𝑞′q^{\\prime}italic_q start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT, and merge the results:\n\nwhere 𝒟c,qsubscript𝒟𝑐𝑞\\mathcal{D}_{c,q}caligraphic_D start_POSTSUBSCRIPT italic_c , italic_q end_POSTSUBSCRIPT and 𝒟c,q′subscript𝒟𝑐superscript𝑞′\\mathcal{D}_{c,q^{\\prime}}caligraphic_D start_POSTSUBSCRIPT italic_c , italic_q start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT end_POSTSUBSCRIPT denote the passage sets retrieved by q𝑞qitalic_q and q′superscript𝑞′q^{\\prime}italic_q start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT, respectively.\n\nThis stage integrates corpus and KG evidence to generate the final answer. We enhance retrieved passages with structured facts to support more informed and accurate reasoning.\n\nThis stage integrates corpus and KG evidence to generate the final answer. We enhance retrieved passages with structured facts to support more informed and accurate reasoning.\n\nWe first prompt the LLM to summarize the retrieved passages 𝒟c,(q,q′)subscript𝒟𝑐𝑞superscript𝑞′\\mathcal{D}_{c,(q,q^{\\prime})}caligraphic_D start_POSTSUBSCRIPT italic_c , ( italic_q , italic_q start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ) end_POSTSUBSCRIPT, producing a query-focused passage note 𝒮𝒟,qsubscript𝒮𝒟𝑞\\mathcal{S}_{\\mathcal{D},q}caligraphic_S start_POSTSUBSCRIPT caligraphic_D , italic_q end_POSTSUBSCRIPT that distills key information relevant to answering q𝑞qitalic_q: 𝒮𝒟,q=LLMprompt⁢(q,𝒟c,(q,q′))subscript𝒮𝒟𝑞subscriptLLMprompt𝑞subscript𝒟𝑐𝑞superscript𝑞′\\mathcal{S}_{\\mathcal{D},q}=\\text{LLM}_{\\text{prompt}}\\left(q,\\mathcal{D}_{c,(% q,q^{\\prime})}\\right)caligraphic_S start_POSTSUBSCRIPT caligraphic_D , italic_q end_POSTSUBSCRIPT = LLM start_POSTSUBSCRIPT prompt end_POSTSUBSCRIPT ( italic_q , caligraphic_D start_POSTSUBSCRIPT italic_c , ( italic_q , italic_q start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ) end_POSTSUBSCRIPT ) (9)\n\nWe first prompt the LLM to summarize the retrieved passages 𝒟c,(q,q′)subscript𝒟𝑐𝑞superscript𝑞′\\mathcal{D}_{c,(q,q^{\\prime})}caligraphic_D start_POSTSUBSCRIPT italic_c , ( italic_q , italic_q start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ) end_POSTSUBSCRIPT, producing a query-focused passage note 𝒮𝒟,qsubscript𝒮𝒟𝑞\\mathcal{S}_{\\mathcal{D},q}caligraphic_S start_POSTSUBSCRIPT caligraphic_D , italic_q end_POSTSUBSCRIPT that distills key information relevant to answering q𝑞qitalic_q:",
        "conclusion": "In this work, we identify key limitations of existing RAG systems: single-source retrieval and and the absence of a human-like spreading activation process. To address these issues, we propose KG-Infused RAG, a framework that incorporates human-curated KGs to guide query expansion and knowledge augmentation via spreading activation. Extensive experiments show that KG-Infused RAG consistently improves QA performance and can be integrated as a plug-in to enhance corpus-based RAG methods like Self-RAG.\n\nIn this work, we identify key limitations of existing RAG systems: single-source retrieval and and the absence of a human-like spreading activation process. To address these issues, we propose KG-Infused RAG, a framework that incorporates human-curated KGs to guide query expansion and knowledge augmentation via spreading activation. Extensive experiments show that KG-Infused RAG consistently improves QA performance and can be integrated as a plug-in to enhance corpus-based RAG methods like Self-RAG."
      }
    },
    {
      "arxiv_id": "2508.13828v1",
      "title": "Revisiting RAG Ensemble: A Theoretical and Mechanistic Analysis of   Multi-RAG System Collaboration",
      "authors": [
        "Yifei Chen",
        "Guanting Dong",
        "Yutao Zhu",
        "Zhicheng Dou"
      ],
      "published": "2025-08-19T13:38:54Z",
      "updated": "2025-08-19T13:38:54Z",
      "summary": "Retrieval-Augmented Generation (RAG) technology has been widely applied in recent years. However, despite the emergence of various RAG frameworks, a single RAG framework still cannot adapt well to a broad range of downstream tasks. Therefore, how to leverage the advantages of multiple RAG systems has become an area worth exploring. To address this issue, we have conducted a comprehensive and systematic investigation into ensemble methods based on RAG systems. Specifically, we have analyzed the RAG ensemble framework from both theoretical and mechanistic analysis perspectives. From the theoretical analysis, we provide the first explanation of the RAG ensemble framework from the perspective of information entropy. In terms of mechanism analysis, we have explored the RAG ensemble framework from both the pipeline and module levels. We carefully select four different pipelines (Branching, Iterative, Loop, and Agentic) and three different modules (Generator, Retriever, and Reranker) to solve seven different research questions. The experiments show that aggregating multiple RAG systems is both generalizable and robust, whether at the pipeline level or the module level. Our work lays the foundation for similar research on the multi-RAG system ensemble.",
      "html_url": "https://arxiv.org/html/2508.13828v1",
      "sections": {
        "introduction": "The emergence of Large Language Models (LLMs) has profoundly revolutionized many real-world tasks that rely on natural language (Ouyang et al., 2022; Brown et al., 2020; Zhao et al., 2023). However, when dealing with knowledge-intensive tasks, LLMs relying solely on their parametric knowledge often suffer from factual inconsistencies or hallucinations. To address these limitations, Retrieval-Augmented Generation (RAG) methods have been proposed, augmenting LLMs with dynamically retrieved external knowledge. This integration enhances response accuracy and reliability by grounding outputs in verifiable information sources.\n\nThe emergence of Large Language Models (LLMs) has profoundly revolutionized many real-world tasks that rely on natural language (Ouyang et al., 2022; Brown et al., 2020; Zhao et al., 2023). However, when dealing with knowledge-intensive tasks, LLMs relying solely on their parametric knowledge often suffer from factual inconsistencies or hallucinations. To address these limitations, Retrieval-Augmented Generation (RAG) methods have been proposed, augmenting LLMs with dynamically retrieved external knowledge. This integration enhances response accuracy and reliability by grounding outputs in verifiable information sources.\n\nAs research in this field advances, more and more RAG methods have been proposed. Component Module RAG inserts various modules into the standard pipeline to better complete the retrieval task. For instance, the LongLLMLingua and RECOMP methods refine the retrieved knowledge with a refiner, and the SKR and Adaptive RAG methods distinguish the difficulty of questions by introducing a Judger (Jiang et al., 2024; Xu et al., 2024; Wang et al., 2023a; Jeong et al., 2024). Pipeline Module RAG optimizes the whole process to improve the accuracy and efficiency. For example, RePlug method is suitable for tasks with varying difficulty levels, and methods such as Iter-RetGen and Self-RAG are suitable for solving multi-hop problems (Shi et al., 2024; Shao et al., 2023; Asai et al., 2024). With the development of agent technology, the application of Agentic RAG technology is becoming increasingly widespread (Tao et al., 2025; Wu et al., 2025b, c; Li et al., 2025c). For example, Search-o1 and WebThinker combine search and reasoning and perform well in deep search tasks (Li et al., 2025a, b). However, given the inherent complexity of tasks and the heterogeneity of RAG workflows, developing a universal RAG framework that generalizes effectively across diverse applications remains a significant challenge.\n\nAs research in this field advances, more and more RAG methods have been proposed. Component Module RAG inserts various modules into the standard pipeline to better complete the retrieval task. For instance, the LongLLMLingua and RECOMP methods refine the retrieved knowledge with a refiner, and the SKR and Adaptive RAG methods distinguish the difficulty of questions by introducing a Judger (Jiang et al., 2024; Xu et al., 2024; Wang et al., 2023a; Jeong et al., 2024). Pipeline Module RAG optimizes the whole process to improve the accuracy and efficiency. For example, RePlug method is suitable for tasks with varying difficulty levels, and methods such as Iter-RetGen and Self-RAG are suitable for solving multi-hop problems (Shi et al., 2024; Shao et al., 2023; Asai et al., 2024). With the development of agent technology, the application of Agentic RAG technology is becoming increasingly widespread (Tao et al., 2025; Wu et al., 2025b, c; Li et al., 2025c). For example, Search-o1 and WebThinker combine search and reasoning and perform well in deep search tasks (Li et al., 2025a, b). However, given the inherent complexity of tasks and the heterogeneity of RAG workflows, developing a universal RAG framework that generalizes effectively across diverse applications remains a significant challenge.\n\nTo further investigate this limitation, we analyze the framework classification proposed in FlashRAG (Jin et al., 2024) and select four representative RAG methods, each corresponding to a distinct pipeline type: Branching, Iterative, Loop, and Agentic. As illustrated in Figure 1, we evaluate these methods on four benchmark tasks and observe the following key findings:\n\nTo further investigate this limitation, we analyze the framework classification proposed in FlashRAG (Jin et al., 2024) and select four representative RAG methods, each corresponding to a distinct pipeline type: Branching, Iterative, Loop, and Agentic. As illustrated in Figure 1, we evaluate these methods on four benchmark tasks and observe the following key findings:\n\n(1) Lack of generalizability in single RAG pipelines. The upper part of Figure 1 presents the aggregated performance of each method across three datasets. Notably, Branching-based approaches (e.g., RePlug (Shi et al., 2024)) underperform in multi-hop reasoning tasks but excel in multiple-choice settings. A similar phenomenon can be observed in the Iterative method, reinforcing that each pipeline exhibits task-dependent performance biases .\n\n(1) Lack of generalizability in single RAG pipelines. The upper part of Figure 1 presents the aggregated performance of each method across three datasets. Notably, Branching-based approaches (e.g., RePlug (Shi et al., 2024)) underperform in multi-hop reasoning tasks but excel in multiple-choice settings. A similar phenomenon can be observed in the Iterative method, reinforcing that each pipeline exhibits task-dependent performance biases .\n\n(2) Divergence in answer perplexity levels across pipelines. By quantifying the perplexity of generated answers, we observe distinct density distributions for each method (lower part of Figure 1). The more concentrated the distribution, the more stable the generated results. For instance, Loop-based methods yield lower answer perplexity (greater stability) on 2Wiki, but higher perplexity on WikiASP, reflecting task-specific confidence disparities.\n\n(2) Divergence in answer perplexity levels across pipelines. By quantifying the perplexity of generated answers, we observe distinct density distributions for each method (lower part of Figure 1). The more concentrated the distribution, the more stable the generated results. For instance, Loop-based methods yield lower answer perplexity (greater stability) on 2Wiki, but higher perplexity on WikiASP, reflecting task-specific confidence disparities.\n\nThese results collectively demonstrate that single-RAG systems struggle with task generalization, whether measured by performance or output perplexity. This motivates our core research question: How can we aggregate multiple RAG systems to enhance generalization capability for complex, heterogeneous tasks?\n\nThese results collectively demonstrate that single-RAG systems struggle with task generalization, whether measured by performance or output perplexity. This motivates our core research question: How can we aggregate multiple RAG systems to enhance generalization capability for complex, heterogeneous tasks?\n\nTo address this issue, one intuitive approach is to perform adaptive fine-tuning on the model to enhance its ability in RAG tasks. However, such methods may interfere with the model’s inherent capabilities and come with higher training costs. Another common strategy is to treat the model as a router, selecting the optimal single RAG system’s answer and discarding the remaining systems. However, we consider that the unselected answers may still contain valuable information for the task. Recent research has begun to explore component ensemble methods. Some studies suggest that meta-search engines, by aggregating results from multiple search engines, can provide more relevant information (Lu et al., 2005; Renda and Straccia, 2003). Additionally, numerous studies focus on model-level ensemble strategies. We argue that, compared to routing methods, ensemble strategy can better make full use of the useful information in each subsystem, improving the quality of the final results. However, existing methods mainly focus on multi-component ensemble on single level, while RAG tasks involve more complex input flows and system structures. Unfortunately, both in terms of theoretical modeling and mechanism explanation, there is still a significant lack of systematic research on ensemble across multiple RAG systems, which significantly limits its development and application.\n\nTo address this issue, one intuitive approach is to perform adaptive fine-tuning on the model to enhance its ability in RAG tasks. However, such methods may interfere with the model’s inherent capabilities and come with higher training costs. Another common strategy is to treat the model as a router, selecting the optimal single RAG system’s answer and discarding the remaining systems. However, we consider that the unselected answers may still contain valuable information for the task. Recent research has begun to explore component ensemble methods. Some studies suggest that meta-search engines, by aggregating results from multiple search engines, can provide more relevant information (Lu et al., 2005; Renda and Straccia, 2003). Additionally, numerous studies focus on model-level ensemble strategies. We argue that, compared to routing methods, ensemble strategy can better make full use of the useful information in each subsystem, improving the quality of the final results. However, existing methods mainly focus on multi-component ensemble on single level, while RAG tasks involve more complex input flows and system structures. Unfortunately, both in terms of theoretical modeling and mechanism explanation, there is still a significant lack of systematic research on ensemble across multiple RAG systems, which significantly limits its development and application.\n\nTo address this challenge, in this paper, we conduct a comprehensive and systematic study of ensemble methods based on RAG systems. Specifically, we perform an in-depth analysis of the RAG system ensemble method from theoretical analysis and mechanism analysis: (1) From a theoretical analysis perspective: We model the RAG system ensemble method on a non-Euclidean manifold. Through detailed derivation, we clarify the effectiveness of RAG system ensemble from the perspective of information entropy increase. As we know, this is the first work to model a system ensemble task from the perspective of information entropy. (2) From a mechanism analysis perspective: To achieve a comprehensive exploration of RAG ensemble, we conduct in-depth investigations of seven different research questions from both the pipeline and module levels. At the system level, we carefully select four different RAG pipelines (Branching, Iterative, Loop, and Agentic) for ensemble research. Additionally, we conduct ensemble experiments on closed-source RAG frameworks to further explore the characteristics of RAG ensemble. At the module level, we conduct experimental research on the retriever, reranker, and generator of the standard RAG framework. We carefully select three retrievers and five generation models for the experiments, and delve into the characteristics of applying generative rerankers to ensemble tasks. Moreover, our experiments cover a wide range of task sets, including single-hop tasks, multi-hop tasks, multiple-choice tasks, summarization tasks, and tasks in vertical domains, all of which have detailed ensemble analysis. Our main findings include: • RAG ensemble demonstrates clear advantages in both the framework type and the granularity of the ensemble. This reflects the good generalizability of the RAG ensemble method. • In a significant portion of ensemble tasks, the RAG ensemble method exhibits scaling-up characteristics, meaning that increasing the external information has a notable positive impact on the final ensemble result. However, this characteristic also depends on the model’s strong resistance to information interference. • The ensemble model shows a preference for certain groups of input information, and this preference becomes more pronounced as task difficulty increases.\n\nTo address this challenge, in this paper, we conduct a comprehensive and systematic study of ensemble methods based on RAG systems. Specifically, we perform an in-depth analysis of the RAG system ensemble method from theoretical analysis and mechanism analysis:\n\nFrom a theoretical analysis perspective: We model the RAG system ensemble method on a non-Euclidean manifold. Through detailed derivation, we clarify the effectiveness of RAG system ensemble from the perspective of information entropy increase. As we know, this is the first work to model a system ensemble task from the perspective of information entropy.\n\nFrom a theoretical analysis perspective: We model the RAG system ensemble method on a non-Euclidean manifold. Through detailed derivation, we clarify the effectiveness of RAG system ensemble from the perspective of information entropy increase. As we know, this is the first work to model a system ensemble task from the perspective of information entropy.\n\nFrom a mechanism analysis perspective: To achieve a comprehensive exploration of RAG ensemble, we conduct in-depth investigations of seven different research questions from both the pipeline and module levels. At the system level, we carefully select four different RAG pipelines (Branching, Iterative, Loop, and Agentic) for ensemble research. Additionally, we conduct ensemble experiments on closed-source RAG frameworks to further explore the characteristics of RAG ensemble. At the module level, we conduct experimental research on the retriever, reranker, and generator of the standard RAG framework. We carefully select three retrievers and five generation models for the experiments, and delve into the characteristics of applying generative rerankers to ensemble tasks. Moreover, our experiments cover a wide range of task sets, including single-hop tasks, multi-hop tasks, multiple-choice tasks, summarization tasks, and tasks in vertical domains, all of which have detailed ensemble analysis.\n\nFrom a mechanism analysis perspective: To achieve a comprehensive exploration of RAG ensemble, we conduct in-depth investigations of seven different research questions from both the pipeline and module levels. At the system level, we carefully select four different RAG pipelines (Branching, Iterative, Loop, and Agentic) for ensemble research. Additionally, we conduct ensemble experiments on closed-source RAG frameworks to further explore the characteristics of RAG ensemble. At the module level, we conduct experimental research on the retriever, reranker, and generator of the standard RAG framework. We carefully select three retrievers and five generation models for the experiments, and delve into the characteristics of applying generative rerankers to ensemble tasks. Moreover, our experiments cover a wide range of task sets, including single-hop tasks, multi-hop tasks, multiple-choice tasks, summarization tasks, and tasks in vertical domains, all of which have detailed ensemble analysis.\n\nOur main findings include:\n\nRAG ensemble demonstrates clear advantages in both the framework type and the granularity of the ensemble. This reflects the good generalizability of the RAG ensemble method.\n\nRAG ensemble demonstrates clear advantages in both the framework type and the granularity of the ensemble. This reflects the good generalizability of the RAG ensemble method.\n\nIn a significant portion of ensemble tasks, the RAG ensemble method exhibits scaling-up characteristics, meaning that increasing the external information has a notable positive impact on the final ensemble result. However, this characteristic also depends on the model’s strong resistance to information interference.\n\nIn a significant portion of ensemble tasks, the RAG ensemble method exhibits scaling-up characteristics, meaning that increasing the external information has a notable positive impact on the final ensemble result. However, this characteristic also depends on the model’s strong resistance to information interference.\n\nThe ensemble model shows a preference for certain groups of input information, and this preference becomes more pronounced as task difficulty increases.\n\nThe ensemble model shows a preference for certain groups of input information, and this preference becomes more pronounced as task difficulty increases.",
        "methodology": "In this section, we introduce the framework of RAG ensemble, and then we theoretically analyze why combining multiple RAG systems can be effective.\n\nIn this section, we introduce the framework of RAG ensemble, and then we theoretically analyze why combining multiple RAG systems can be effective.\n\nGiven a set of RAG systems {𝒮1,𝒮2,⋯,𝒮n}\\{\\mathcal{S}_{1},\\mathcal{S}_{2},\\cdots,\\mathcal{S}_{n}\\}, the ensemble framework aims to synthesize the inputs and outputs of these systems to generate a response YY. Specifically, each standard RAG system 𝒮i\\mathcal{S}_{i} includes a retriever RiR_{i} and a generator GiG_{i}.111Many advanced RAG systems have designed several additional modules to improve the systems’ performance. In theoretical analysis, we only consider the two fundamental components, without loss of generality, to make the formulation more clear. Henceforth, we will follow this principle to simplify the modules for better understanding. Upon receiving a user input XX, the retriever RiR_{i} retrieves relevant external knowledge, denoted as Di=Ri​(X)D_{i}=R_{i}(X). Then, the generator GiG_{i} generates a response Yi=Gi​(X,Di)Y_{i}=G_{i}(X,D_{i}). When multiple RAG systems exist, all inputs and outputs can be collected as follows: (1) S\\displaystyle{S} ={S1,S2,⋯,Sn},\\displaystyle=\\{{S}_{1},{S}_{2},\\cdots,{S}_{n}\\}, (2) Si\\displaystyle{S}_{i} ={Yi,Di}.\\displaystyle=\\{Y_{i},D_{i}\\}. Finally, an ensemble model is employed to generate the final response as follows: (3) Y=fϕ​(X,S),\\displaystyle Y=f_{\\phi}(X,S), where fϕ​(⋅)f_{\\phi}(\\cdot) denotes the ensemble model parameterized by ϕ\\phi.\n\nGiven a set of RAG systems {𝒮1,𝒮2,⋯,𝒮n}\\{\\mathcal{S}_{1},\\mathcal{S}_{2},\\cdots,\\mathcal{S}_{n}\\}, the ensemble framework aims to synthesize the inputs and outputs of these systems to generate a response YY. Specifically, each standard RAG system 𝒮i\\mathcal{S}_{i} includes a retriever RiR_{i} and a generator GiG_{i}.111Many advanced RAG systems have designed several additional modules to improve the systems’ performance. In theoretical analysis, we only consider the two fundamental components, without loss of generality, to make the formulation more clear. Henceforth, we will follow this principle to simplify the modules for better understanding. Upon receiving a user input XX, the retriever RiR_{i} retrieves relevant external knowledge, denoted as Di=Ri​(X)D_{i}=R_{i}(X). Then, the generator GiG_{i} generates a response Yi=Gi​(X,Di)Y_{i}=G_{i}(X,D_{i}). When multiple RAG systems exist, all inputs and outputs can be collected as follows:\n\nFinally, an ensemble model is employed to generate the final response as follows:\n\nwhere fϕ​(⋅)f_{\\phi}(\\cdot) denotes the ensemble model parameterized by ϕ\\phi.\n\nThe core idea of RAG ensemble lies in the ability of the model to aggregate the information from multiple RAG systems. This paper mainly focuses on the simplest method, which directly embeds the raw information from multiple RAG systems into a prompt and then inputs it to an LLM for ensemble. This process can be represented as follows: (4) Y=fϕ​(Prompt​(X,S)).\\displaystyle Y=f_{\\phi}(\\text{Prompt}(X,{S})).\n\nThe core idea of RAG ensemble lies in the ability of the model to aggregate the information from multiple RAG systems. This paper mainly focuses on the simplest method, which directly embeds the raw information from multiple RAG systems into a prompt and then inputs it to an LLM for ensemble. This process can be represented as follows:\n\nThe prompt we use for basic RAG ensemble is as follows:\n\nThe prompt we use for basic RAG ensemble is as follows:\n\nRAG Ensemble Prompt Here is a question and some external data from {num} systems’ information: System 1: {system 1’s information} System 2: {system 2’s information} System 3: {system 3’s information} …… Question: {question} Your task is to answer the question based on the given information. You should first output your reasoning process and then provide the final answer. The output format of reasoning process and final answer should be enclosed within ¡think¿ ¡/think¿ and ¡answer¿ ¡/answer¿ tags, respectively, and the final answer should be enclosed within boxed with latex format, i.e., “¡think¿ reasoning process here ¡/think¿¡answer¿a​f​i​n​a​l​a​n​s​w​e​r​h​e​r​e ¡/answer¿”. Only output your reasoning process in ¡think¿¡/think¿ and your answer in ¡answer¿ boxed{} ¡/answer¿, and do not output any other words.\n\nIn the following, we will provide a detailed theoretical analysis of the rationale behind RAG ensemble.\n\nIn the following, we will provide a detailed theoretical analysis of the rationale behind RAG ensemble.\n\n(a) Before knowledge refinement (b) After knowledge refinement\n\n(a) Before knowledge refinement\n\n(b) After knowledge refinement\n\nIn this section, we provide a detailed analysis of RAG ensemble from the perspective of theoretical modeling. Given a probability distribution, information entropy describes the degree of uncertainty we have about an event. The higher the entropy, the more uncertain the system is, and the more information it contains. Mathematically, the entropy H​(X)H(X) of a continuous random variable XX with probability distribution P​(x)P(x) can be shown as: (5) H​(X)=−∫p​(x)​log⁡p​(x)​𝑑x.H(X)=-\\int p(x)\\log p(x)\\,dx.\n\nIn this section, we provide a detailed analysis of RAG ensemble from the perspective of theoretical modeling. Given a probability distribution, information entropy describes the degree of uncertainty we have about an event. The higher the entropy, the more uncertain the system is, and the more information it contains. Mathematically, the entropy H​(X)H(X) of a continuous random variable XX with probability distribution P​(x)P(x) can be shown as:\n\nInspired by the information bottleneck method (Tishby et al., 2000; Zhu et al., 2024a) in information theory, we consider that the key of the RAG ensemble framework lies in its ensemble of information from multiple individual RAG systems, reducing the information entropy of the final answer. As shown in Figure 3, this process can be modeled on a non-Euclidean sphere.\n\nInspired by the information bottleneck method (Tishby et al., 2000; Zhu et al., 2024a) in information theory, we consider that the key of the RAG ensemble framework lies in its ensemble of information from multiple individual RAG systems, reducing the information entropy of the final answer. As shown in Figure 3, this process can be modeled on a non-Euclidean sphere.\n\nAs shown in Figure 3(a), the entire white sphere represents the potential region for generating answers (i.e., the informational entropy of the answer), the gray area represents the useful information contained in the prompt, and each green circular area represents the external information introduced by an individual RAG system (including both useful and useless external knowledge). In Figure 3(b), the purple area represents the useful information extracted by generator from all external knowledge. We consider that due to the introduction of useful knowledge from both the prompt and all individual RAG systems, the uncertainty in generating the answer is reduced. That is, RAG ensemble can aggregate useful information from multiple systems to generate a correct output. The useful knowledge of individual RAG systems and the useful knowledge after extraction from all systems can be obtained as follows: (6) ei\\displaystyle e_{i} =gϕ​[q,(Si)],\\displaystyle=g_{\\phi}\\left[q,(S_{i})\\right], (7) e∗\\displaystyle e^{*} =gϕ​[q,(S1,S2,…,Sn)],\\displaystyle=g_{\\phi}\\left[q,(S_{1},S_{2},\\ldots,S_{n})\\right], where eie_{i} represents individual RAG’s useful knowledge, e∗e^{*} represents all the useful knowledge extracted, and gϕg_{\\phi} represents the process that the model ϕ\\phi extracts useful information. Then, the process by which ensemble model reduces the information entropy of the generated answer can be expressed by the following formula: H​(Y|X,K)\\displaystyle H(Y|X,K) =H​(Y|X,e∗)\\displaystyle=H(Y|X,e^{*}) =H​(Y)−I​(X,e∗;Y)⏟useful information,\\displaystyle=H(Y)-\\underbrace{I(X,e^{*};Y)}_{\\text{useful information}}, I​(X,e∗;Y)\\displaystyle I(X,e^{*};Y) =I​(X,K;Y)⏟all information−I​(euseless)⏟useless information,\\displaystyle=\\underbrace{I(X,K;Y)}_{\\text{all information}}-\\underbrace{I(e_{\\text{useless}})}_{\\text{useless information}}, where KK represents all the external knowledge provided, H​(Y|X,K)H(Y|X,K) represents the conditional entropy of the generated response when both the user prompt and external knowledge are introduced, H​(Y)H(Y) denotes the informational entropy of generating response. Moreover, H​(Y|X,K)=H​(Y|X,e∗)H(Y|X,K)=H(Y|X,e^{*}) means that only useful information can help reduce the target response’s entropy. I​(X,e∗;Y)I(X,e^{*};Y), I​(X,K;Y)I(X,K;Y) represents the mutual information between the input and useful reference knowledge, as well as the mutual information between the input and all external knowledge, respectively. I​(euseless)I(e_{\\text{useless}}) represents the useless information in the external knowledge that is discarded after model analysis. Therefore, when external knowledge is introduced as a condition, the information entropy of the generated answer decreases due to the useful knowledge extracted by the ensemble model (i.e., the accuracy of generating answer improves).\n\nAs shown in Figure 3(a), the entire white sphere represents the potential region for generating answers (i.e., the informational entropy of the answer), the gray area represents the useful information contained in the prompt, and each green circular area represents the external information introduced by an individual RAG system (including both useful and useless external knowledge). In Figure 3(b), the purple area represents the useful information extracted by generator from all external knowledge. We consider that due to the introduction of useful knowledge from both the prompt and all individual RAG systems, the uncertainty in generating the answer is reduced. That is, RAG ensemble can aggregate useful information from multiple systems to generate a correct output. The useful knowledge of individual RAG systems and the useful knowledge after extraction from all systems can be obtained as follows:\n\nwhere eie_{i} represents individual RAG’s useful knowledge, e∗e^{*} represents all the useful knowledge extracted, and gϕg_{\\phi} represents the process that the model ϕ\\phi extracts useful information. Then, the process by which ensemble model reduces the information entropy of the generated answer can be expressed by the following formula:\n\nwhere KK represents all the external knowledge provided, H​(Y|X,K)H(Y|X,K) represents the conditional entropy of the generated response when both the user prompt and external knowledge are introduced, H​(Y)H(Y) denotes the informational entropy of generating response. Moreover, H​(Y|X,K)=H​(Y|X,e∗)H(Y|X,K)=H(Y|X,e^{*}) means that only useful information can help reduce the target response’s entropy. I​(X,e∗;Y)I(X,e^{*};Y), I​(X,K;Y)I(X,K;Y) represents the mutual information between the input and useful reference knowledge, as well as the mutual information between the input and all external knowledge, respectively. I​(euseless)I(e_{\\text{useless}}) represents the useless information in the external knowledge that is discarded after model analysis. Therefore, when external knowledge is introduced as a condition, the information entropy of the generated answer decreases due to the useful knowledge extracted by the ensemble model (i.e., the accuracy of generating answer improves).\n\nFor RAG ensemble system, if the external knowledge base does not contain conflicting information (i.e., the external knowledge base cannot simultaneously contain knowledge such as “the sky is blue” and “the sky is green”), we can propose the following assumption:\n\nFor RAG ensemble system, if the external knowledge base does not contain conflicting information (i.e., the external knowledge base cannot simultaneously contain knowledge such as “the sky is blue” and “the sky is green”), we can propose the following assumption:\n\nAssumption When performing ensemble tasks, an ideal model tends to refine the collected information in a direction that increases the amount of correct knowledge.\n\nSuppose we have an ideal ensemble model ϕ∗\\phi^{*} that can perfectly extract useful information from the given external knowledge while ignoring all irrelevant information. In this case, the useful knowledge extracted from the input information of system ii can be represented as follows: (8) ei=gϕ∗​(q,di,ai).e_{i}=g_{\\phi^{*}}\\left(q,d_{i},a_{i}\\right).\n\nSuppose we have an ideal ensemble model ϕ∗\\phi^{*} that can perfectly extract useful information from the given external knowledge while ignoring all irrelevant information. In this case, the useful knowledge extracted from the input information of system ii can be represented as follows:\n\nMeanwhile, the final useful information extracted from all subsystems can be expressed as follows: (9) e∗=gϕ∗​(q,di,ai,S\\​i),e^{*}=g_{\\phi^{*}}\\left(q,d_{i},a_{i},S_{{\\textbackslash}i}\\right), here, S\\iS_{\\backslash i} represents the input information excluding system ii.\n\nMeanwhile, the final useful information extracted from all subsystems can be expressed as follows:\n\nhere, S\\iS_{\\backslash i} represents the input information excluding system ii.\n\nWhen more system information is obtained, for the initial useful information eie_{i}, the ensemble model can handle it in the following two ways:\n\nWhen more system information is obtained, for the initial useful information eie_{i}, the ensemble model can handle it in the following two ways:\n\n(1) ϕ∗\\phi^{*} retains all the information eie_{i}. Due to the introduction of information from other systems, the total information received by the ensemble model is at least not worse than the information received solely by the ii-th system. In other words, eie_{i} is included in e∗e^{*}, which means: (10) I1\\displaystyle I_{1} =I​(q,e∗;a)\\displaystyle=I(q,e^{*};a) =I​(q,ei;a)+I​(q,e\\​i∗;a),\\displaystyle=I(q,e_{i};a)+I(q,e_{{\\textbackslash}i}^{*};a),\n\n(1) ϕ∗\\phi^{*} retains all the information eie_{i}. Due to the introduction of information from other systems, the total information received by the ensemble model is at least not worse than the information received solely by the ii-th system. In other words, eie_{i} is included in e∗e^{*}, which means:\n\ne\\​i∗e_{{\\textbackslash}i}^{*} represents the useful information from other systems excluding system ii.\n\ne\\​i∗e_{{\\textbackslash}i}^{*} represents the useful information from other systems excluding system ii.\n\n(2) If ϕ∗\\phi^{*} only extracts partial information from eie_{i}, we define ei∗e_{i}^{*} represents the useful knowledge of system ii after refinement, then the ei∗e_{i}^{*} can be represented as: (11) I​(q,ei∗;a)=I​(q,ei;a)−I​(q,eiu​s​e​l​e​s​s;a),I(q,e_{i}^{*};a)=I(q,e_{i};a)-I(q,e_{\\text{i}}^{useless};a), here, eiu​s​e​l​e​s​se_{\\text{i}}^{useless} represents the part of the knowledge eie_{i} that the model considers useless after final refinement. At this point, the information entropy of generating the answer can be expressed as: (12) I2\\displaystyle I_{2} =I​(q,e∗;a)\\displaystyle=I(q,e^{*};a) =I​(q,ei∗;a)+I​(q,e\\​i∗;a)\\displaystyle=I(q,e_{i}^{*};a)+I(q,e_{{\\textbackslash}i}^{*};a) =I​(q,ei;a)−I​(q,eiu​s​e​l​e​s​s;a)\\displaystyle=I(q,e_{i};a)-I(q,e_{\\text{i}}^{useless};a) +I​(q,e\\​i∗;a).\\displaystyle+I(q,e_{{\\textbackslash}i}^{*};a).\n\n(2) If ϕ∗\\phi^{*} only extracts partial information from eie_{i}, we define ei∗e_{i}^{*} represents the useful knowledge of system ii after refinement, then the ei∗e_{i}^{*} can be represented as:\n\nhere, eiu​s​e​l​e​s​se_{\\text{i}}^{useless} represents the part of the knowledge eie_{i} that the model considers useless after final refinement. At this point, the information entropy of generating the answer can be expressed as:\n\nAt the same time, based on the assumption before, if the model chooses to discard some of the information from system ii, it must believe that other systems can provide more useful information, such that the total amount of effective information after refinement is greater than the information from system ii alone, that is: (13) I​(q,e\\​i∗;a)≥I​(q,eiu​s​e​l​e​s​s;a).I(q,e_{{\\textbackslash}i}^{*};a)\\geq I(q,e_{\\text{i}}^{useless};a).\n\nAt the same time, based on the assumption before, if the model chooses to discard some of the information from system ii, it must believe that other systems can provide more useful information, such that the total amount of effective information after refinement is greater than the information from system ii alone, that is:\n\nBased on all the derivations above, we can conclude the following relationship: (14) H​(a|q,e∗)\\displaystyle H(a|q,e^{*}) =H​(a)−min⁡(I1,I2)\\displaystyle=H(a)-\\min(I_{1},I_{2}) ≤H​(a)−I​(q,ei;a)\\displaystyle\\leq H(a)-I(q,e_{i};a) =H​(a|q,ei),\\displaystyle=H(a|q,e_{i}),\n\nBased on all the derivations above, we can conclude the following relationship:\n\nSo the ensemble knowledge contains more useful information, and it helps reduce the information entropy in generating answers. Therefore, we consider that the process of ensemble can introduce more helpful information than single system. This is the core of RAG ensemble. In the following sections, we conduct a large number of experiments, clearly demonstrating the effectiveness of RAG ensemble.\n\nSo the ensemble knowledge contains more useful information, and it helps reduce the information entropy in generating answers. Therefore, we consider that the process of ensemble can introduce more helpful information than single system. This is the core of RAG ensemble. In the following sections, we conduct a large number of experiments, clearly demonstrating the effectiveness of RAG ensemble.",
        "conclusion": "In this paper, we perform a thorough analysis of the method for aggregating information from multiple RAG systems to derive comprehensive answers. This is the first detailed analysis of the ensemble method about RAG ensemble framework. We establish a mathematical model that provides a solid formulation of the ensemble process. In addition, we conduct a lot of experiments at both the pipeline level and the module level, fully demonstrating the broad adaptability, effectiveness, and stability of the RAG ensemble framework. Moreover, we have drawn some important conclusions. For example, we find that the RAG system ensemble framework exhibits a scaling-up phenomenon, and that the ensemble model has different preferences for tasks of varying difficulty levels. These conclusions are supported by a series of our experiments. We hope this paper serves as a reference for research on the RAG system ensemble and encourages further work to optimize RAG system performance.\n\nIn this paper, we perform a thorough analysis of the method for aggregating information from multiple RAG systems to derive comprehensive answers. This is the first detailed analysis of the ensemble method about RAG ensemble framework. We establish a mathematical model that provides a solid formulation of the ensemble process. In addition, we conduct a lot of experiments at both the pipeline level and the module level, fully demonstrating the broad adaptability, effectiveness, and stability of the RAG ensemble framework. Moreover, we have drawn some important conclusions. For example, we find that the RAG system ensemble framework exhibits a scaling-up phenomenon, and that the ensemble model has different preferences for tasks of varying difficulty levels. These conclusions are supported by a series of our experiments. We hope this paper serves as a reference for research on the RAG system ensemble and encourages further work to optimize RAG system performance."
      }
    },
    {
      "arxiv_id": "2409.01666v1",
      "title": "In Defense of RAG in the Era of Long-Context Language Models",
      "authors": [
        "Tan Yu",
        "Anbang Xu",
        "Rama Akkiraju"
      ],
      "published": "2024-09-03T07:17:41Z",
      "updated": "2024-09-03T07:17:41Z",
      "summary": "Overcoming the limited context limitations in early-generation LLMs, retrieval-augmented generation (RAG) has been a reliable solution for context-based answer generation in the past. Recently, the emergence of long-context LLMs allows the models to incorporate much longer text sequences, making RAG less attractive. Recent studies show that long-context LLMs significantly outperform RAG in long-context applications. Unlike the existing works favoring the long-context LLM over RAG, we argue that the extremely long context in LLMs suffers from a diminished focus on relevant information and leads to potential degradation in answer quality. This paper revisits the RAG in long-context answer generation. We propose an order-preserve retrieval-augmented generation (OP-RAG) mechanism, which significantly improves the performance of RAG for long-context question-answer applications. With OP-RAG, as the number of retrieved chunks increases, the answer quality initially rises, and then declines, forming an inverted U-shaped curve. There exist sweet points where OP-RAG could achieve higher answer quality with much less tokens than long-context LLM taking the whole context as input. Extensive experiments on public benchmark demonstrate the superiority of our OP-RAG.",
      "html_url": "https://arxiv.org/html/2409.01666v1",
      "sections": {
        "introduction": "Due to the limited context window length (eg, 4096) of early-generation large language models (LLMs), retrieval augmented generation (RAG) Guu et al. (2020); Lewis et al. (2020) is an indispensable choice to handle a large-scale context corpus. Since the answer quality is heavily dependent on the performance of the retrieval model, a lot of efforts are devoted to improving the retrieval recall/precision when designing the RAG system.\n\nDue to the limited context window length (eg, 4096) of early-generation large language models (LLMs), retrieval augmented generation (RAG) Guu et al. (2020); Lewis et al. (2020) is an indispensable choice to handle a large-scale context corpus. Since the answer quality is heavily dependent on the performance of the retrieval model, a lot of efforts are devoted to improving the retrieval recall/precision when designing the RAG system.\n\n(a) F1 score. (b) Input token count.\n\n(b) Input token count.\n\nRecently, the state-of-art LLMs support much longer context windows. For example, GPT-4O OpenAI (2023), Claudi-3.5 Anthropic (2024), Llama3.1 Meta (2024b), Phi-3 Abdin et al. (2024), and Mistral-Large2 AI (2024) all support 128-K context. Gemini-1.5-pro even supports a 1M context window. The recent emergence of long-context LLMs naturally leads to the question: is RAG necessary in the age of long-context LLMs? Li et al. (2024) recently systematically compares RAG with long-context (LC) LLMs (w/o RAG) and demonstrates that LC LLMs consistently outperform RAG in terms of answer quality.\n\nRecently, the state-of-art LLMs support much longer context windows. For example, GPT-4O OpenAI (2023), Claudi-3.5 Anthropic (2024), Llama3.1 Meta (2024b), Phi-3 Abdin et al. (2024), and Mistral-Large2 AI (2024) all support 128-K context. Gemini-1.5-pro even supports a 1M context window. The recent emergence of long-context LLMs naturally leads to the question: is RAG necessary in the age of long-context LLMs? Li et al. (2024) recently systematically compares RAG with long-context (LC) LLMs (w/o RAG) and demonstrates that LC LLMs consistently outperform RAG in terms of answer quality.\n\nIn this work, we re-examine the effectiveness of RAG in long-context answer generation. We observe that the order of retrieved chunks in the context of LLM is vital for the answer quality. Different from traditional RAG which places the retrieved chunks in a relevance-descending order, we propose to preserve the order of retrieved chunks in the original text. Our experiments show that the proposed order-preserving mechanism significantly improves the answer quality of RAG.\n\nIn this work, we re-examine the effectiveness of RAG in long-context answer generation. We observe that the order of retrieved chunks in the context of LLM is vital for the answer quality. Different from traditional RAG which places the retrieved chunks in a relevance-descending order, we propose to preserve the order of retrieved chunks in the original text. Our experiments show that the proposed order-preserving mechanism significantly improves the answer quality of RAG.\n\nMeanwhile, using the proposed order-preserve RAG, as the number of retrieved chunks increases, the answer quality initially rises and then declines. This is because, with more retrieved chunks, the model has access to more potentially relevant information, which improves the chances of retrieving the correct context needed to generate a high-quality answer. However, as more chunks are retrieved, the likelihood of introducing irrelevant or distracting information also increases. This excess information can confuse the model, leading to a decline in answer quality. The trade-off, therefore, is between improving recall by retrieving more context and maintaining precision by limiting distractions. The optimal point is where the balance between relevant and irrelevant information maximizes the quality of the answer. Beyond this point, the introduction of too much irrelevant information degrades the model’s performance. It explains the inferior performance of the approach taking the whole long context as the input of LLM.\n\nMeanwhile, using the proposed order-preserve RAG, as the number of retrieved chunks increases, the answer quality initially rises and then declines. This is because, with more retrieved chunks, the model has access to more potentially relevant information, which improves the chances of retrieving the correct context needed to generate a high-quality answer. However, as more chunks are retrieved, the likelihood of introducing irrelevant or distracting information also increases. This excess information can confuse the model, leading to a decline in answer quality. The trade-off, therefore, is between improving recall by retrieving more context and maintaining precision by limiting distractions. The optimal point is where the balance between relevant and irrelevant information maximizes the quality of the answer. Beyond this point, the introduction of too much irrelevant information degrades the model’s performance. It explains the inferior performance of the approach taking the whole long context as the input of LLM.\n\nDifferent from the conclusion from Li et al. (2024), with the proposed order-preserving mechanism, RAG achieves higher answer quality compared with its counterparts that rely solely on Long-Context LLMs. As shown in Figure 4(a), On En.QA dataset of ∞\\infty∞Bench Zhang et al. (2024), using only 16161616K retrieved tokens, we achieve 44.4344.4344.4344.43 F1 score with Llama3.1-70B. In contrast, without RAG, Llama3.1-70B making full use of 128128128128K context only achieves 34.3234.3234.3234.32 F1 score, GPT-4O achieves only 32.3632.3632.3632.36 F1 score and Gemini-1.5-Pro obtains only 43.0843.0843.0843.08 F1 score as evaluated by Li et al. (2024). That is, RAG could achieve a higher F1 score even with a significant reduction on input length.\n\nDifferent from the conclusion from Li et al. (2024), with the proposed order-preserving mechanism, RAG achieves higher answer quality compared with its counterparts that rely solely on Long-Context LLMs. As shown in Figure 4(a), On En.QA dataset of ∞\\infty∞Bench Zhang et al. (2024), using only 16161616K retrieved tokens, we achieve 44.4344.4344.4344.43 F1 score with Llama3.1-70B. In contrast, without RAG, Llama3.1-70B making full use of 128128128128K context only achieves 34.3234.3234.3234.32 F1 score, GPT-4O achieves only 32.3632.3632.3632.36 F1 score and Gemini-1.5-Pro obtains only 43.0843.0843.0843.08 F1 score as evaluated by Li et al. (2024). That is, RAG could achieve a higher F1 score even with a significant reduction on input length.",
        "methodology": "EN.QA\nEN.MC\n\nF1 Score\nTokens\nAcc.\nTokens\n\nLong-context LLM w/o RAG\n\nLlama3.1-70B\n34.2634.2634.2634.26\n117K\n71.6271.6271.6271.62\n117K\n\nGPT-4O\n32.3632.3632.3632.36\n117K\n78.4278.4278.4278.42\n117K\n\nGemini-1.5-Pro\n43.0843.0843.0843.08\n196K\n85.5785.5785.5785.57\n188K\n\nSELF-ROUTE Li et al. (2024)\n\nGPT-4O\n34.9534.9534.9534.95\n85K\n77.2977.2977.2977.29\n62K\n\nGemini-1.5-Pro\n37.5137.5137.5137.51\n83K\n76.8676.8676.8676.86\n62K\n\nLlama3.1-70B order-preserve RAG (ours)\n\nOP-RAG-16K\n44.4344.4344.4344.43\n16K\n84.7284.7284.7284.72\n16K\n\nOP-RAG-24K\n45.4545.4545.4545.45\n24K\n88.6588.65\\mathbf{88.65}bold_88.65\n24K\n\nOP-RAG-48K\n47.2547.25\\mathbf{47.25}bold_47.25\n48K\n85.5985.5985.5985.59\n48K\n\nTable 1: Comparisons among the long-context LLM without RAG, SELF-ROUTE mechanism Li et al. (2024) and the proposed order-preserve (OP) RAG.",
        "conclusion": "In this paper, we have revisited the role of retrieval-augmented generation (RAG) in the era of long-context language models (LLMs). While recent trends have favored long-context LLMs over RAG for their ability to incorporate extensive text sequences, our research challenges this perspective. We argue that extremely long contexts in LLMs can lead to a diminished focus on relevant information, potentially degrading answer quality in question-answering tasks. To address this issue, we proposed the order-preserve retrieval-augmented generation (OP-RAG) mechanism. Our extensive experiments on public benchmarks have demonstrated that OP-RAG significantly improves the performance of RAG for long-context question-answer applications. OP-RAG’s superior performance suggests that efficient retrieval and focused context utilization can outperform the brute-force approach of processing extremely long contexts.\n\nIn this paper, we have revisited the role of retrieval-augmented generation (RAG) in the era of long-context language models (LLMs). While recent trends have favored long-context LLMs over RAG for their ability to incorporate extensive text sequences, our research challenges this perspective. We argue that extremely long contexts in LLMs can lead to a diminished focus on relevant information, potentially degrading answer quality in question-answering tasks. To address this issue, we proposed the order-preserve retrieval-augmented generation (OP-RAG) mechanism. Our extensive experiments on public benchmarks have demonstrated that OP-RAG significantly improves the performance of RAG for long-context question-answer applications. OP-RAG’s superior performance suggests that efficient retrieval and focused context utilization can outperform the brute-force approach of processing extremely long contexts."
      }
    },
    {
      "arxiv_id": "2403.09040v3",
      "title": "RAGGED: Towards Informed Design of Scalable and Stable RAG Systems",
      "authors": [
        "Jennifer Hsia",
        "Afreen Shaikh",
        "Zhiruo Wang",
        "Graham Neubig"
      ],
      "published": "2024-03-14T02:26:31Z",
      "updated": "2025-07-16T09:39:02Z",
      "summary": "Retrieval-augmented generation (RAG) enhances language models by integrating external knowledge, but its effectiveness is highly dependent on system configuration. Improper retrieval settings can degrade performance, making RAG less reliable than closed-book generation. In this work, we introduce RAGGED, a framework for systematically evaluating RAG systems across diverse retriever-reader configurations, retrieval depths, and datasets. Our analysis reveals that reader robustness to noise is the key determinant of RAG stability and scalability. Some readers benefit from increased retrieval depth, while others degrade due to their sensitivity to distracting content. Through large-scale experiments on open-domain, multi-hop, and specialized-domain datasets, we show that retrievers, rerankers, and prompts influence performance but do not fundamentally alter these reader-driven trends. By providing a principled framework and new metrics to assess RAG stability and scalability, RAGGED enables systematic evaluation of retrieval-augmented generation systems, guiding future research on optimizing retrieval depth and model robustness.",
      "html_url": "https://arxiv.org/html/2403.09040v3",
      "sections": {
        "introduction": "Retrieval-augmented generation (RAG) (Chen et al., 2017; Lewis et al., 2020) enhances large language models (LLMs) by retrieving relevant external contexts, enabling more specific and factually grounded responses. However, despite its promise, RAG’s effectiveness is not guaranteed. In fact, improper configurations can degrade model performance, leading to outputs that are worse than closed-book generation. Understanding when and why RAG helps or harms is critical for optimizing system design.\n\nRetrieval-augmented generation (RAG) (Chen et al., 2017; Lewis et al., 2020) enhances large language models (LLMs) by retrieving relevant external contexts, enabling more specific and factually grounded responses. However, despite its promise, RAG’s effectiveness is not guaranteed. In fact, improper configurations can degrade model performance, leading to outputs that are worse than closed-book generation. Understanding when and why RAG helps or harms is critical for optimizing system design.\n\nMost prior work evaluates RAG under controlled conditions and curated contexts (Liu et al., 2023; Cuconasu et al., 2024), which fail to reflect real-world retrieval challenges. In practice, retrieved contexts contain both relevant and irrelevant information, making the reader model’s ability to filter noise a critical factor in RAG success. Additionally, prior studies provide conflicting findings on retrieval depth (k𝑘kitalic_k)— while some suggest increasing k𝑘kitalic_k improves performance (Izacard & Grave, 2021), others observe diminishing returns (Liu et al., 2023) or even degradation at high k𝑘kitalic_k (Cuconasu et al., 2024; Jiang et al., 2024). This lack of consensus leaves practitioners without clear guidance on how to configure RAG systems for different tasks.\n\nMost prior work evaluates RAG under controlled conditions and curated contexts (Liu et al., 2023; Cuconasu et al., 2024), which fail to reflect real-world retrieval challenges. In practice, retrieved contexts contain both relevant and irrelevant information, making the reader model’s ability to filter noise a critical factor in RAG success. Additionally, prior studies provide conflicting findings on retrieval depth (k𝑘kitalic_k)— while some suggest increasing k𝑘kitalic_k improves performance (Izacard & Grave, 2021), others observe diminishing returns (Liu et al., 2023) or even degradation at high k𝑘kitalic_k (Cuconasu et al., 2024; Jiang et al., 2024). This lack of consensus leaves practitioners without clear guidance on how to configure RAG systems for different tasks.\n\nTo address these challenges, we introduce RAGGED (Retrieval-Augmented Generation Generalized Evaluation Device), a framework for systematically evaluating RAG performance across retrieval depths, model architectures, and retrieval conditions. Unlike prior work, which often relies on synthetic or manual retrieval modifications, RAGGED assesses models under realistic retrieval scenarios — analyzing performance on naturally retrieved top-k𝑘kitalic_k contexts rather than manually curated, oracle-aware contexts.\n\nTo address these challenges, we introduce RAGGED (Retrieval-Augmented Generation Generalized Evaluation Device), a framework for systematically evaluating RAG performance across retrieval depths, model architectures, and retrieval conditions. Unlike prior work, which often relies on synthetic or manual retrieval modifications, RAGGED assesses models under realistic retrieval scenarios — analyzing performance on naturally retrieved top-k𝑘kitalic_k contexts rather than manually curated, oracle-aware contexts.\n\nOur study reveals that reader robustness to noise is the primary factor driving RAG stability and scalability, rather than retriever quality alone. To quantify this, we introduce two new metrics: the RAG Stability Score (RSS) and RAG Scalability Coefficient (RSC), providing a principled framework for evaluating retrieval effectiveness across diverse configurations.\n\nOur study reveals that reader robustness to noise is the primary factor driving RAG stability and scalability, rather than retriever quality alone. To quantify this, we introduce two new metrics: the RAG Stability Score (RSS) and RAG Scalability Coefficient (RSC), providing a principled framework for evaluating retrieval effectiveness across diverse configurations.\n\nUsing RAGGED, we conduct a large-scale empirical study to answer four key questions (Figure 1), each corresponding to a core section of our paper:\n\nUsing RAGGED, we conduct a large-scale empirical study to answer four key questions (Figure 1), each corresponding to a core section of our paper:\n\n1. Under What Conditions Does Retrieval Outperform Closed-Book Generation? (§4) We analyze when retrieval improves performance and identify that some readers frequently benefit from RAG, particularly at large k𝑘kitalic_k, while others degrade due to noise sensitivity. 2. How Does Retrieval Depth Impact Stability and Scalability? (§5) We identify two distinct reader behaviors: improve-then-plateau models, which scale effectively, and peak-then-decline models, which degrade at higher k𝑘kitalic_k (Figure 2). 3. How Do Readers Handle Noisy Retrieval, and Is Prompting a Reliable Fix? (§6) We evaluate RAG performance under realistic retrieval conditions, showing that noise sensitivity—rather than retriever quality alone—determines downstream effectiveness. We also assess whether instructing readers to focus on relevant content mitigates noise sensitivity. 4. When Does a Better Retriever Actually Lead to Better Performance? (§7) While retriever choice shifts overall performance, it does not alter fundamental reader behaviors, thus highlighting the reader as the key driver of stability and scalability.\n\nUnder What Conditions Does Retrieval Outperform Closed-Book Generation? (§4) We analyze when retrieval improves performance and identify that some readers frequently benefit from RAG, particularly at large k𝑘kitalic_k, while others degrade due to noise sensitivity.\n\nUnder What Conditions Does Retrieval Outperform Closed-Book Generation? (§4) We analyze when retrieval improves performance and identify that some readers frequently benefit from RAG, particularly at large k𝑘kitalic_k, while others degrade due to noise sensitivity.\n\nHow Does Retrieval Depth Impact Stability and Scalability? (§5) We identify two distinct reader behaviors: improve-then-plateau models, which scale effectively, and peak-then-decline models, which degrade at higher k𝑘kitalic_k (Figure 2).\n\nHow Does Retrieval Depth Impact Stability and Scalability? (§5) We identify two distinct reader behaviors: improve-then-plateau models, which scale effectively, and peak-then-decline models, which degrade at higher k𝑘kitalic_k (Figure 2).\n\nHow Do Readers Handle Noisy Retrieval, and Is Prompting a Reliable Fix? (§6) We evaluate RAG performance under realistic retrieval conditions, showing that noise sensitivity—rather than retriever quality alone—determines downstream effectiveness. We also assess whether instructing readers to focus on relevant content mitigates noise sensitivity.\n\nHow Do Readers Handle Noisy Retrieval, and Is Prompting a Reliable Fix? (§6) We evaluate RAG performance under realistic retrieval conditions, showing that noise sensitivity—rather than retriever quality alone—determines downstream effectiveness. We also assess whether instructing readers to focus on relevant content mitigates noise sensitivity.\n\nWhen Does a Better Retriever Actually Lead to Better Performance? (§7) While retriever choice shifts overall performance, it does not alter fundamental reader behaviors, thus highlighting the reader as the key driver of stability and scalability.\n\nWhen Does a Better Retriever Actually Lead to Better Performance? (§7) While retriever choice shifts overall performance, it does not alter fundamental reader behaviors, thus highlighting the reader as the key driver of stability and scalability.\n\nBy introducing a structured and reproducible evaluation framework, our study provides foundational insights into the dynamics of RAG systems and guides future research toward optimizing retrieval-augmented generation for real-world applications.\n\nBy introducing a structured and reproducible evaluation framework, our study provides foundational insights into the dynamics of RAG systems and guides future research toward optimizing retrieval-augmented generation for real-world applications.",
        "methodology": "Section not found (searched for: methodology, method, approach)",
        "conclusion": "Retrieval-augmented generation (RAG) systems are widely used to enhance language models, but their performance hinges not just on retrieval quality, but on the reader’s ability to handle noise and uncertainty. Our study demonstrates that retrieval depth must be dynamically tuned for each model, and that reader robustness, not retriever strength, is the key driver of scalable and stable RAG performance.\n\nRetrieval-augmented generation (RAG) systems are widely used to enhance language models, but their performance hinges not just on retrieval quality, but on the reader’s ability to handle noise and uncertainty. Our study demonstrates that retrieval depth must be dynamically tuned for each model, and that reader robustness, not retriever strength, is the key driver of scalable and stable RAG performance.\n\nTo support this insight, we introduce RAGGED, a modular evaluation framework that systematically analyzes retrieval depth, noise sensitivity, and reader-retriever dynamics. Through two new metrics – RAG Stability Score (RSS) and RAG Scalability Coefficient (RSC) – RAGGED offers a principled way to assess how reliably and efficiently models use retrieved information across configurations and domains.\n\nTo support this insight, we introduce RAGGED, a modular evaluation framework that systematically analyzes retrieval depth, noise sensitivity, and reader-retriever dynamics. Through two new metrics – RAG Stability Score (RSS) and RAG Scalability Coefficient (RSC) – RAGGED offers a principled way to assess how reliably and efficiently models use retrieved information across configurations and domains.\n\nThese findings challenge the assumption that retrieval quality alone governs RAG success, and highlight the importance of tuning retrieval strategies around reader behavior. As models continue to evolve, RAGGED remains applicable as a model-agnostic harness for measuring retrieval sensitivity and guiding deployment decisions.\n\nThese findings challenge the assumption that retrieval quality alone governs RAG success, and highlight the importance of tuning retrieval strategies around reader behavior. As models continue to evolve, RAGGED remains applicable as a model-agnostic harness for measuring retrieval sensitivity and guiding deployment decisions.\n\nLooking ahead, expanding RAGGED to adversarial, outdated, or temporally shifting noise scenarios will further enhance its relevance to high-stakes, real-world settings. By formalizing how we evaluate reader robustness and retrieval utility, RAGGED lays a foundation for building more reliable, adaptive, and efficient retrieval-augmented generation systems.\n\nLooking ahead, expanding RAGGED to adversarial, outdated, or temporally shifting noise scenarios will further enhance its relevance to high-stakes, real-world settings. By formalizing how we evaluate reader robustness and retrieval utility, RAGGED lays a foundation for building more reliable, adaptive, and efficient retrieval-augmented generation systems."
      }
    },
    {
      "arxiv_id": "2504.07103v1",
      "title": "FG-RAG: Enhancing Query-Focused Summarization with Context-Aware   Fine-Grained Graph RAG",
      "authors": [
        "Yubin Hong",
        "Chaofan Li",
        "Jingyi Zhang",
        "Yingxia Shao"
      ],
      "published": "2025-03-13T17:42:07Z",
      "updated": "2025-03-13T17:42:07Z",
      "summary": "Retrieval-Augmented Generation (RAG) enables large language models to provide more precise and pertinent responses by incorporating external knowledge. In the Query-Focused Summarization (QFS) task, GraphRAG-based approaches have notably enhanced the comprehensiveness and diversity of generated responses. However, existing GraphRAG-based approaches predominantly focus on coarse-grained information summarization without being aware of the specific query, and the retrieved content lacks sufficient contextual information to generate comprehensive responses. To address the deficiencies of current RAG systems, we propose Context-Aware Fine-Grained Graph RAG (FG-RAG) to enhance the performance of the QFS task. FG-RAG employs Context-Aware Entity Expansion in graph retrieval to expand the coverage of retrieved entities in the graph, thus providing enough contextual information for the retrieved content. Furthermore, FG-RAG utilizes Query-Level Fine-Grained Summarization to incorporate fine-grained details during response generation, enhancing query awareness for the generated summarization. Our evaluation demonstrates that FG-RAG outperforms other RAG systems in multiple metrics of comprehensiveness, diversity, and empowerment when handling the QFS task. Our implementation is available at https://github.com/BuptWululu/FG-RAG.",
      "html_url": "https://arxiv.org/html/2504.07103v1",
      "sections": {
        "introduction": "Retrieval-Augmented Generation (RAG) systems enhance the capabilities of large language models (LLMs) by integrating external knowledge bases [?; ?; ?; ?]. The integration allows LLMs to access up-to-date information [?; ?; ?], thereby reducing the risk of generating inaccurate or “hallucinated” responses by using retrieved data as contexts [?; ?]. RAG systems improve the precision, currency, and clarity of the responses generated by LLMs.\n\nRetrieval-Augmented Generation (RAG) systems enhance the capabilities of large language models (LLMs) by integrating external knowledge bases [?; ?; ?; ?]. The integration allows LLMs to access up-to-date information [?; ?; ?], thereby reducing the risk of generating inaccurate or “hallucinated” responses by using retrieved data as contexts [?; ?]. RAG systems improve the precision, currency, and clarity of the responses generated by LLMs.\n\nQuery-Focused Summarization (QFS) task [?] aims to create summaries from documents by extracting or generating content relevant to a specific query. It involves identifying relevant entities from extensive documents and understanding and synthesizing the intricate relationships between them [?; ?; ?]. Consider the query, “How can beekeepers market and sell their honey and other hive products?” We need to sift through the mass of documents to find the contents that discuss how to bring bee products to market, such as utilizing social media, attending farmers’ markets, creating attractive packaging, or setting up an online store. It is challenging to accurately discern which pieces of information are relevant to marketing and selling activities and synthesize them into a coherent, concise summary that answers the query effectively.\n\nQuery-Focused Summarization (QFS) task [?] aims to create summaries from documents by extracting or generating content relevant to a specific query. It involves identifying relevant entities from extensive documents and understanding and synthesizing the intricate relationships between them [?; ?; ?]. Consider the query, “How can beekeepers market and sell their honey and other hive products?” We need to sift through the mass of documents to find the contents that discuss how to bring bee products to market, such as utilizing social media, attending farmers’ markets, creating attractive packaging, or setting up an online store. It is challenging to accurately discern which pieces of information are relevant to marketing and selling activities and synthesize them into a coherent, concise summary that answers the query effectively.\n\nIn order to enhance the capacity of RAG systems to handle the QFS task, an effective approach is to transform the external knowledge into a graph [?; ?], retrieve relevant contents for the query from the graph, and synthesize them into a coherent response using LLMs, such as GraphRAG [?] and LightRAG [?]. These methods simplify the analysis of complex relationships among query entities in the QFS task through the use of a graph structure, thus optimize the comprehensiveness and diversity of the responses. However, these GraphRAG-based approaches suffer from the following problems:\n\nIn order to enhance the capacity of RAG systems to handle the QFS task, an effective approach is to transform the external knowledge into a graph [?; ?], retrieve relevant contents for the query from the graph, and synthesize them into a coherent response using LLMs, such as GraphRAG [?] and LightRAG [?]. These methods simplify the analysis of complex relationships among query entities in the QFS task through the use of a graph structure, thus optimize the comprehensiveness and diversity of the responses. However, these GraphRAG-based approaches suffer from the following problems:\n\n1) Insufficient contexts for the graph retrieval. The QFS task requires the RAG system to perform extensive searches in the knowledge base to cover multiple perspectives and levels of information, thus ensuring that the generated answers are comprehensive and in-depth. We observe that the LLM has limited understanding of the domain while generating domain-specific responses [?]. It is challenging to produce coherent and organized answers to queries within the QFS task if the retrieved content from the graph does not contain enough contextual information. For example, when LightRAG retrieves relevant content for the previously mentioned query, it successfully finds a considerable amount of information directly related to honey, beekeepers, etc. However, this content lacks background information on the local history and geographical context of the honey’s place of origin, which would enable the LLM to address queries with a broader array of perspectives and insights.\n\n1) Insufficient contexts for the graph retrieval. The QFS task requires the RAG system to perform extensive searches in the knowledge base to cover multiple perspectives and levels of information, thus ensuring that the generated answers are comprehensive and in-depth. We observe that the LLM has limited understanding of the domain while generating domain-specific responses [?]. It is challenging to produce coherent and organized answers to queries within the QFS task if the retrieved content from the graph does not contain enough contextual information. For example, when LightRAG retrieves relevant content for the previously mentioned query, it successfully finds a considerable amount of information directly related to honey, beekeepers, etc. However, this content lacks background information on the local history and geographical context of the honey’s place of origin, which would enable the LLM to address queries with a broader array of perspectives and insights.\n\n2) Unawareness of the query in summarization. Existing GraphRAG-based approaches predominantly focus on coarse-grained information summarization, which is unaware of the specific query. For GraphRAG, the summarization of entities is a synthesis of the entities’ intrinsic information, but not all the information contributes meaningfully to a certain query, as it may introduce unnecessary noise. For instance, in the preceding example, GraphRAG might retrieve information about the nutritional content of honey, its processing, and so forth; however, this data is irrelevant to resolving the query. Such information, which is not useful for responding to queries, not only affects the quality of LLM generation, but also inevitably increases token overheads. Truly essential for query responses is the fine-grained information that is highly relevant to the query, highlighting a fundamental mismatch between generated summarization and query response requirements.\n\n2) Unawareness of the query in summarization. Existing GraphRAG-based approaches predominantly focus on coarse-grained information summarization, which is unaware of the specific query. For GraphRAG, the summarization of entities is a synthesis of the entities’ intrinsic information, but not all the information contributes meaningfully to a certain query, as it may introduce unnecessary noise. For instance, in the preceding example, GraphRAG might retrieve information about the nutritional content of honey, its processing, and so forth; however, this data is irrelevant to resolving the query. Such information, which is not useful for responding to queries, not only affects the quality of LLM generation, but also inevitably increases token overheads. Truly essential for query responses is the fine-grained information that is highly relevant to the query, highlighting a fundamental mismatch between generated summarization and query response requirements.\n\nIn this paper, we introduce Context-Aware Fine-Grained Graph RAG (FG-RAG) to enhance the performance of the QFS task. FG-RAG utilizes Context-Aware Entity Expansion during graph retrieval, employing retrieved entities to perform a complementary search for discovering additional related entities and exploring the relationships among them. Through this process, we expect to increase the coverage of retrieved entities in the graph, thus providing more contextual information for the retrieved content and improving the comprehensiveness of the subsequently generated summaries. Furthermore, Query-Level Fine-Grained Summarization utilized by FG-RAG is designed to consider the specific query, ultimately generating several highly relevant summaries of the query. This method improves response accuracy by synthesizing the coarse-grained information in the graph into summaries that are highly relevant to the query, reducing the incorporation of noise. Our contributions are summarized as follows:\n\nIn this paper, we introduce Context-Aware Fine-Grained Graph RAG (FG-RAG) to enhance the performance of the QFS task. FG-RAG utilizes Context-Aware Entity Expansion during graph retrieval, employing retrieved entities to perform a complementary search for discovering additional related entities and exploring the relationships among them. Through this process, we expect to increase the coverage of retrieved entities in the graph, thus providing more contextual information for the retrieved content and improving the comprehensiveness of the subsequently generated summaries. Furthermore, Query-Level Fine-Grained Summarization utilized by FG-RAG is designed to consider the specific query, ultimately generating several highly relevant summaries of the query. This method improves response accuracy by synthesizing the coarse-grained information in the graph into summaries that are highly relevant to the query, reducing the incorporation of noise. Our contributions are summarized as follows:\n\n• We propose incorporating Context-Aware Entity Expansion into the graph retrieval, which is a novel approach to enrich the contextual information of the retrieved content and provide more domain background for LLM generation, thus improving the quality of the output. • We propose an innovative method that effectively integrates coarse-grained information in the graph into fine-grained information that is highly relevant to the query, reduces the inclusion of noise, and improves the accuracy of LLM-generated responses. • Extensive experiments on the QFS task demonstrate that FG-RAG outperforms current state-of-the-art RAG methods in terms of comprehensiveness, diversity, and empowerment, while reducing token overhead, and also exhibits excellent adaptability in different tasks.\n\nWe propose incorporating Context-Aware Entity Expansion into the graph retrieval, which is a novel approach to enrich the contextual information of the retrieved content and provide more domain background for LLM generation, thus improving the quality of the output.\n\nWe propose incorporating Context-Aware Entity Expansion into the graph retrieval, which is a novel approach to enrich the contextual information of the retrieved content and provide more domain background for LLM generation, thus improving the quality of the output.\n\nWe propose an innovative method that effectively integrates coarse-grained information in the graph into fine-grained information that is highly relevant to the query, reduces the inclusion of noise, and improves the accuracy of LLM-generated responses.\n\nWe propose an innovative method that effectively integrates coarse-grained information in the graph into fine-grained information that is highly relevant to the query, reduces the inclusion of noise, and improves the accuracy of LLM-generated responses.\n\nExtensive experiments on the QFS task demonstrate that FG-RAG outperforms current state-of-the-art RAG methods in terms of comprehensiveness, diversity, and empowerment, while reducing token overhead, and also exhibits excellent adaptability in different tasks.\n\nExtensive experiments on the QFS task demonstrate that FG-RAG outperforms current state-of-the-art RAG methods in terms of comprehensiveness, diversity, and empowerment, while reducing token overhead, and also exhibits excellent adaptability in different tasks.",
        "methodology": "Metric NaiveRAG GraphRAG LightRAG FG-RAG Graph Indexing Tokens 0 7,707,566 5,111,680 4,863,801 Query Tokens 1,377,432 36,593,256 2,884,462 1,356,951\n\nWe compare the number of tokens consumed by FG-RAG and baselines when indexing the graph and responding to queries for the QFS task on the Mix dataset. The dataset contains approximately 600,000 tokens and a total of 125 queries. The results are presented in Table 5.\n\nWe compare the number of tokens consumed by FG-RAG and baselines when indexing the graph and responding to queries for the QFS task on the Mix dataset. The dataset contains approximately 600,000 tokens and a total of 125 queries. The results are presented in Table 5.\n\nCompared to GraphRAG, both LightRAG and FG-RAG significantly reduce the number of tokens that are used when indexing the graph and responding to queries. Since the graph indexing process of FG-RAG is similar to that of LightRAG, FG-RAG consumes only slightly fewer tokens111FG-RAG uses fewer examples in the prompt compared to LightRAG for extracting entities and relationships. than LightRAG when indexing a new dataset into the graph. FG-RAG processes the entities in the query at a fine-grained level, which can significantly decrease token overhead while improving generation quality. In response to queries, FG-RAG is more economical than LightRAG, reducing token usage by more than 50%. Although NaiveRAG uses a comparable number of tokens as FG-RAG in response to queries, its generation quality is significantly inferior to FG-RAG when solving the QFS task.\n\nCompared to GraphRAG, both LightRAG and FG-RAG significantly reduce the number of tokens that are used when indexing the graph and responding to queries. Since the graph indexing process of FG-RAG is similar to that of LightRAG, FG-RAG consumes only slightly fewer tokens111FG-RAG uses fewer examples in the prompt compared to LightRAG for extracting entities and relationships. than LightRAG when indexing a new dataset into the graph. FG-RAG processes the entities in the query at a fine-grained level, which can significantly decrease token overhead while improving generation quality. In response to queries, FG-RAG is more economical than LightRAG, reducing token usage by more than 50%. Although NaiveRAG uses a comparable number of tokens as FG-RAG in response to queries, its generation quality is significantly inferior to FG-RAG when solving the QFS task.",
        "conclusion": "In this work, we propose FG-RAG, an innovative RAG framework designed to handle the QFS task through the integration of the graph retrieval with Context-Aware Entity Expansion and Query-Level Fine-Grained Summarization. Our framework effectively converts the large amount of coarse-grained information in the knowledge graph from external documents into fine-grained information that is highly relevant to the query, and incorporates more contextual information related to the query entity during the retrieval process, thus improving the quality of the response in solving the QFS task.\n\nIn this work, we propose FG-RAG, an innovative RAG framework designed to handle the QFS task through the integration of the graph retrieval with Context-Aware Entity Expansion and Query-Level Fine-Grained Summarization. Our framework effectively converts the large amount of coarse-grained information in the knowledge graph from external documents into fine-grained information that is highly relevant to the query, and incorporates more contextual information related to the query entity during the retrieval process, thus improving the quality of the response in solving the QFS task."
      }
    },
    {
      "arxiv_id": "2506.20978v1",
      "title": "Response Quality Assessment for Retrieval-Augmented Generation via   Conditional Conformal Factuality",
      "authors": [
        "Naihe Feng",
        "Yi Sui",
        "Shiyi Hou",
        "Jesse C. Cresswell",
        "Ga Wu"
      ],
      "published": "2025-06-26T03:52:56Z",
      "updated": "2025-06-26T03:52:56Z",
      "summary": "Existing research on Retrieval-Augmented Generation (RAG) primarily focuses on improving overall question-answering accuracy, often overlooking the quality of sub-claims within generated responses. Recent methods that attempt to improve RAG trustworthiness, such as through auto-evaluation metrics, lack probabilistic guarantees or require ground truth answers. To address these limitations, we propose Conformal-RAG, a novel framework inspired by recent applications of conformal prediction (CP) on large language models (LLMs). Conformal-RAG leverages CP and internal information from the RAG mechanism to offer statistical guarantees on response quality. It ensures group-conditional coverage spanning multiple sub-domains without requiring manual labelling of conformal sets, making it suitable for complex RAG applications. Compared to existing RAG auto-evaluation methods, Conformal-RAG offers statistical guarantees on the quality of refined sub-claims, ensuring response reliability without the need for ground truth answers. Additionally, our experiments demonstrate that by leveraging information from the RAG system, Conformal-RAG retains up to 60\\% more high-quality sub-claims from the response compared to direct applications of CP to LLMs, while maintaining the same reliability guarantee.",
      "html_url": "https://arxiv.org/html/2506.20978v1",
      "sections": {
        "introduction": "Existing research in Retrieval-Augmented Generation (RAG) (Lewis et al., 2020; Gao et al., 2023) mostly focuses on improving overall question-answering accuracy (Yu et al., 2025), but often overlooks the quality of sub-claims within generated responses, leading to partially incorrect outputs and hard-to-detect errors (Min et al., 2023). Human evaluations reveal that RAG-based question-answering systems sometimes misinterpret user queries (Agrawal et al., 2024; Wu et al., 2024a), struggle with reasoning in unseen scenarios (Mirzadeh et al., 2025; Huang and Chang, 2023), and may generate claims that are irrelevant or even contradictory to the provided documents (Niu et al., 2024; Wu et al., 2024b).\n\nExisting research in Retrieval-Augmented Generation (RAG) (Lewis et al., 2020; Gao et al., 2023) mostly focuses on improving overall question-answering accuracy (Yu et al., 2025), but often overlooks the quality of sub-claims within generated responses, leading to partially incorrect outputs and hard-to-detect errors (Min et al., 2023). Human evaluations reveal that RAG-based question-answering systems sometimes misinterpret user queries (Agrawal et al., 2024; Wu et al., 2024a), struggle with reasoning in unseen scenarios (Mirzadeh et al., 2025; Huang and Chang, 2023), and may generate claims that are irrelevant or even contradictory to the provided documents (Niu et al., 2024; Wu et al., 2024b).\n\nEnsuring the trustworthiness of RAG systems remains a challenge, prompting research into various evaluation solutions. One straightforward way to quantify the trustworthiness of RAG systems is through auto-evaluation based on well-defined metrics. Unfortunately, popular auto-evaluation methods require ground truth answers at inference time, making them impractical in real applications (Song et al., 2025; Ru et al., 2024). While some research has addressed this problem (Es et al., 2024; Saad-Falcon et al., 2024), auto-evaluation methods still face criticism due to their lack of probabilistic guarantees. Compared to the evaluation techniques mentioned above, conformal prediction provides a stronger theoretical foundation for ensuring soundness of evaluations through statistical guarantees. In hallucination detection tasks, conformal factuality has provided remarkably robust guarantees on large language model (LLM) outputs, solely relying on the LLM’s parametric knowledge (Quach et al., 2024; Mohri and Hashimoto, 2024; Cherian et al., 2024). Although recent work has integrated conformal prediction into RAG systems (Kang et al., 2024), it primarily focuses on analyzing generation risks based on adjustable parameters rather than verifying the factuality of sub-claims, leaving a critical research gap unfilled.\n\nEnsuring the trustworthiness of RAG systems remains a challenge, prompting research into various evaluation solutions. One straightforward way to quantify the trustworthiness of RAG systems is through auto-evaluation based on well-defined metrics. Unfortunately, popular auto-evaluation methods require ground truth answers at inference time, making them impractical in real applications (Song et al., 2025; Ru et al., 2024). While some research has addressed this problem (Es et al., 2024; Saad-Falcon et al., 2024), auto-evaluation methods still face criticism due to their lack of probabilistic guarantees. Compared to the evaluation techniques mentioned above, conformal prediction provides a stronger theoretical foundation for ensuring soundness of evaluations through statistical guarantees. In hallucination detection tasks, conformal factuality has provided remarkably robust guarantees on large language model (LLM) outputs, solely relying on the LLM’s parametric knowledge (Quach et al., 2024; Mohri and Hashimoto, 2024; Cherian et al., 2024). Although recent work has integrated conformal prediction into RAG systems (Kang et al., 2024), it primarily focuses on analyzing generation risks based on adjustable parameters rather than verifying the factuality of sub-claims, leaving a critical research gap unfilled.\n\nThis paper presents Conformal-RAG, a conformal prediction (Vovk et al., 2005; Angelopoulos and Bates, 2021; Cresswell et al., 2024) framework tailored for RAG systems. The proposed framework leverages contextual information (retrieved external knowledge) from a RAG system, and a high-quality conformal scoring function, leading to substantially more retained response content compared to existing solutions when targeting the same factuality threshold. In particular, Conformal-RAG can ensure group-conditional factuality (Vovk et al., 2003; Lei and Wasserman, 2013; Foygel Barber et al., 2020) spanning multiple sub-domains without requiring manual annotation of conformal set validity, making it highly adaptable for complex RAG applications. We empirically evaluate Conformal-RAG on four benchmark datasets from two domains, Wikipedia (Mallen et al., 2023; Min et al., 2023; Yang et al., 2018) and medicine (Jeong et al., 2024). The experimental results show that Conformal-RAG retains up to 60%percent6060\\%60 % more sub-claims from the output in question-answering tasks for the same factuality level compared to existing baselines.\n\nThis paper presents Conformal-RAG, a conformal prediction (Vovk et al., 2005; Angelopoulos and Bates, 2021; Cresswell et al., 2024) framework tailored for RAG systems. The proposed framework leverages contextual information (retrieved external knowledge) from a RAG system, and a high-quality conformal scoring function, leading to substantially more retained response content compared to existing solutions when targeting the same factuality threshold. In particular, Conformal-RAG can ensure group-conditional factuality (Vovk et al., 2003; Lei and Wasserman, 2013; Foygel Barber et al., 2020) spanning multiple sub-domains without requiring manual annotation of conformal set validity, making it highly adaptable for complex RAG applications. We empirically evaluate Conformal-RAG on four benchmark datasets from two domains, Wikipedia (Mallen et al., 2023; Min et al., 2023; Yang et al., 2018) and medicine (Jeong et al., 2024). The experimental results show that Conformal-RAG retains up to 60%percent6060\\%60 % more sub-claims from the output in question-answering tasks for the same factuality level compared to existing baselines.",
        "methodology": "We introduce Conformal-RAG, a framework leveraging CP and the RAG mechanism to offer statistical guarantees on response quality while remaining grounded in documents containing domain knowledge. Below we discuss the end-to-end application of the framework, followed by an in-depth examination of how concepts from CP are applied.\n\nWe introduce Conformal-RAG, a framework leveraging CP and the RAG mechanism to offer statistical guarantees on response quality while remaining grounded in documents containing domain knowledge. Below we discuss the end-to-end application of the framework, followed by an in-depth examination of how concepts from CP are applied.\n\nGiven a query x∈X𝑥𝑋x\\in Xitalic_x ∈ italic_X, a RAG model retrieves a set of m𝑚mitalic_m relevant documents D={d1,d2,…,dm}𝐷subscript𝑑1subscript𝑑2…subscript𝑑𝑚D=\\{d_{1},d_{2},...,d_{m}\\}italic_D = { italic_d start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_d start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , italic_d start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT } from its knowledge corpus. The model then generates an answer y^^𝑦\\hat{y}over^ start_ARG italic_y end_ARG composed of p𝑝pitalic_p sub-claims y^={c1,c2,….,cp}\\hat{y}=\\{c_{1},c_{2},....,c_{p}\\}over^ start_ARG italic_y end_ARG = { italic_c start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_c start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … . , italic_c start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT }. The goal of Conformal-RAG is to modify y^^𝑦\\hat{y}over^ start_ARG italic_y end_ARG by filtering out sub-claims, producing y𝑦yitalic_y which satisfies eq. 3 where α𝛼\\alphaitalic_α is the predefined error tolerance level, and y𝑦yitalic_y consists of a subset of claims from y^^𝑦\\hat{y}over^ start_ARG italic_y end_ARG, i.e. y⊆y^={c1,c2,….,cp}y\\subseteq\\hat{y}=\\{c_{1},c_{2},....,c_{p}\\}italic_y ⊆ over^ start_ARG italic_y end_ARG = { italic_c start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_c start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … . , italic_c start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT }.\n\nGiven a query x∈X𝑥𝑋x\\in Xitalic_x ∈ italic_X, a RAG model retrieves a set of m𝑚mitalic_m relevant documents D={d1,d2,…,dm}𝐷subscript𝑑1subscript𝑑2…subscript𝑑𝑚D=\\{d_{1},d_{2},...,d_{m}\\}italic_D = { italic_d start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_d start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , italic_d start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT } from its knowledge corpus. The model then generates an answer y^^𝑦\\hat{y}over^ start_ARG italic_y end_ARG composed of p𝑝pitalic_p sub-claims y^={c1,c2,….,cp}\\hat{y}=\\{c_{1},c_{2},....,c_{p}\\}over^ start_ARG italic_y end_ARG = { italic_c start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_c start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … . , italic_c start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT }. The goal of Conformal-RAG is to modify y^^𝑦\\hat{y}over^ start_ARG italic_y end_ARG by filtering out sub-claims, producing y𝑦yitalic_y which satisfies eq. 3 where α𝛼\\alphaitalic_α is the predefined error tolerance level, and y𝑦yitalic_y consists of a subset of claims from y^^𝑦\\hat{y}over^ start_ARG italic_y end_ARG, i.e. y⊆y^={c1,c2,….,cp}y\\subseteq\\hat{y}=\\{c_{1},c_{2},....,c_{p}\\}italic_y ⊆ over^ start_ARG italic_y end_ARG = { italic_c start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_c start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … . , italic_c start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT }.\n\nThe first step of our method is to design and calibrate a function to score the relevance of claims. For each query x𝑥xitalic_x in the calibration set, we obtain the generated answer from RAG as y^={c1,c2,….,cp}\\hat{y}=\\{c_{1},c_{2},....,c_{p}\\}over^ start_ARG italic_y end_ARG = { italic_c start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_c start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … . , italic_c start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT }. Our scoring function R⁢(c∈y^)𝑅𝑐^𝑦R(c\\in\\hat{y})italic_R ( italic_c ∈ over^ start_ARG italic_y end_ARG ) assigns each claim c𝑐citalic_c a relevance score as shown in algorithm 1. First we compute the cosine similarity between the claim and each of the m𝑚mitalic_m retrieved documents. These similarity scores are then multiplied by the cosine similarity between the corresponding document and the original query. Finally, the relevance score R⁢(c)𝑅𝑐R(c)italic_R ( italic_c ) takes the maximum of these values across all m𝑚mitalic_m documents (or zero if all scores are negative).\n\nThe first step of our method is to design and calibrate a function to score the relevance of claims. For each query x𝑥xitalic_x in the calibration set, we obtain the generated answer from RAG as y^={c1,c2,….,cp}\\hat{y}=\\{c_{1},c_{2},....,c_{p}\\}over^ start_ARG italic_y end_ARG = { italic_c start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_c start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … . , italic_c start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT }. Our scoring function R⁢(c∈y^)𝑅𝑐^𝑦R(c\\in\\hat{y})italic_R ( italic_c ∈ over^ start_ARG italic_y end_ARG ) assigns each claim c𝑐citalic_c a relevance score as shown in algorithm 1. First we compute the cosine similarity between the claim and each of the m𝑚mitalic_m retrieved documents. These similarity scores are then multiplied by the cosine similarity between the corresponding document and the original query. Finally, the relevance score R⁢(c)𝑅𝑐R(c)italic_R ( italic_c ) takes the maximum of these values across all m𝑚mitalic_m documents (or zero if all scores are negative).\n\n1:Query x𝑥xitalic_x, retrieved documents D={d1,d2,…⁢dm}𝐷subscript𝑑1subscript𝑑2…subscript𝑑𝑚D=\\{d_{1},d_{2},...d_{m}\\}italic_D = { italic_d start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_d start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … italic_d start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT }, generated answer y^={c1,c2,….,cp}\\hat{y}=\\{c_{1},c_{2},....,c_{p}\\}over^ start_ARG italic_y end_ARG = { italic_c start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_c start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … . , italic_c start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT }. 2:for ck∈y^subscript𝑐𝑘^𝑦c_{k}\\in\\hat{y}italic_c start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ∈ over^ start_ARG italic_y end_ARG do 3: for dj∈Dsubscript𝑑𝑗𝐷d_{j}\\in Ditalic_d start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ∈ italic_D do 4: sk⁢j=CosineSimilarity⁢(x,dj)⋅CosineSimilarity⁢(ck,dj)subscript𝑠𝑘𝑗⋅CosineSimilarity𝑥subscript𝑑𝑗CosineSimilaritysubscript𝑐𝑘subscript𝑑𝑗s_{kj}=\\text{CosineSimilarity}(x,d_{j})\\cdot\\text{CosineSimilarity}(c_{k},d_{j})italic_s start_POSTSUBSCRIPT italic_k italic_j end_POSTSUBSCRIPT = CosineSimilarity ( italic_x , italic_d start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) ⋅ CosineSimilarity ( italic_c start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , italic_d start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) 5: end for 6: rk=max⁡({sk⁢j}j=1m∪0)subscript𝑟𝑘superscriptsubscriptsubscript𝑠𝑘𝑗𝑗1𝑚0r_{k}=\\max(\\{s_{kj}\\}_{j=1}^{m}\\cup 0)italic_r start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = roman_max ( { italic_s start_POSTSUBSCRIPT italic_k italic_j end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ∪ 0 ) ▷▷\\triangleright▷ Sub-claim relevance scores 7:end for 8:return {rk}k=1psuperscriptsubscriptsubscript𝑟𝑘𝑘1𝑝\\{r_{k}\\}_{k=1}^{p}{ italic_r start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_p end_POSTSUPERSCRIPT\n\n1:Query x𝑥xitalic_x, retrieved documents D={d1,d2,…⁢dm}𝐷subscript𝑑1subscript𝑑2…subscript𝑑𝑚D=\\{d_{1},d_{2},...d_{m}\\}italic_D = { italic_d start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_d start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … italic_d start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT }, generated answer y^={c1,c2,….,cp}\\hat{y}=\\{c_{1},c_{2},....,c_{p}\\}over^ start_ARG italic_y end_ARG = { italic_c start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_c start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … . , italic_c start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT }.\n\n2:for ck∈y^subscript𝑐𝑘^𝑦c_{k}\\in\\hat{y}italic_c start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ∈ over^ start_ARG italic_y end_ARG do\n\n3: for dj∈Dsubscript𝑑𝑗𝐷d_{j}\\in Ditalic_d start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ∈ italic_D do\n\n4: sk⁢j=CosineSimilarity⁢(x,dj)⋅CosineSimilarity⁢(ck,dj)subscript𝑠𝑘𝑗⋅CosineSimilarity𝑥subscript𝑑𝑗CosineSimilaritysubscript𝑐𝑘subscript𝑑𝑗s_{kj}=\\text{CosineSimilarity}(x,d_{j})\\cdot\\text{CosineSimilarity}(c_{k},d_{j})italic_s start_POSTSUBSCRIPT italic_k italic_j end_POSTSUBSCRIPT = CosineSimilarity ( italic_x , italic_d start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) ⋅ CosineSimilarity ( italic_c start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , italic_d start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT )\n\n6: rk=max⁡({sk⁢j}j=1m∪0)subscript𝑟𝑘superscriptsubscriptsubscript𝑠𝑘𝑗𝑗1𝑚0r_{k}=\\max(\\{s_{kj}\\}_{j=1}^{m}\\cup 0)italic_r start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = roman_max ( { italic_s start_POSTSUBSCRIPT italic_k italic_j end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ∪ 0 ) ▷▷\\triangleright▷ Sub-claim relevance scores\n\n8:return {rk}k=1psuperscriptsubscriptsubscript𝑟𝑘𝑘1𝑝\\{r_{k}\\}_{k=1}^{p}{ italic_r start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_p end_POSTSUPERSCRIPT\n\nThe second step is to design an annotation function which takes advantage of the ground-truth answers from the calibration set to judge the factuality of claims. Specifically, we prompt an LLM (Gu et al., 2024) to annotate if a given sub-claim is factual by providing the query x𝑥xitalic_x, ground-truth answer y∗superscript𝑦y^{*}italic_y start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT, as well as the retrieved documents D𝐷Ditalic_D. The annotation function A⁢(c∈y^,x,y∗,D)=1𝐴𝑐^𝑦𝑥superscript𝑦𝐷1A(c\\in\\hat{y},x,y^{*},D)=1italic_A ( italic_c ∈ over^ start_ARG italic_y end_ARG , italic_x , italic_y start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT , italic_D ) = 1 when the sub-claim c𝑐citalic_c is factual and A⁢(c∈y^,x,y∗,D)=0𝐴𝑐^𝑦𝑥superscript𝑦𝐷0A(c\\in\\hat{y},x,y^{*},D)=0italic_A ( italic_c ∈ over^ start_ARG italic_y end_ARG , italic_x , italic_y start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT , italic_D ) = 0 when it is non-factual.\n\nThe second step is to design an annotation function which takes advantage of the ground-truth answers from the calibration set to judge the factuality of claims. Specifically, we prompt an LLM (Gu et al., 2024) to annotate if a given sub-claim is factual by providing the query x𝑥xitalic_x, ground-truth answer y∗superscript𝑦y^{*}italic_y start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT, as well as the retrieved documents D𝐷Ditalic_D. The annotation function A⁢(c∈y^,x,y∗,D)=1𝐴𝑐^𝑦𝑥superscript𝑦𝐷1A(c\\in\\hat{y},x,y^{*},D)=1italic_A ( italic_c ∈ over^ start_ARG italic_y end_ARG , italic_x , italic_y start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT , italic_D ) = 1 when the sub-claim c𝑐citalic_c is factual and A⁢(c∈y^,x,y∗,D)=0𝐴𝑐^𝑦𝑥superscript𝑦𝐷0A(c\\in\\hat{y},x,y^{*},D)=0italic_A ( italic_c ∈ over^ start_ARG italic_y end_ARG , italic_x , italic_y start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT , italic_D ) = 0 when it is non-factual.\n\nBased on the relevance scores and annotations generated for each claim across queries in the calibration dataset, we apply CP to calibrate a threshold q^^𝑞\\hat{q}over^ start_ARG italic_q end_ARG. Details on the marginal and conditional CP approaches are given below in section 3.2 and section 3.3. At inference, only queries and documents are available. Sub-claims and relevance scores are generated in the same way as during calibration. Then, claims are removed from the generated answer if their relevance is below the calibrated threshold, creating the conformally factual output y⁢(x;q^)={c∈y^∣R⁢(c)≥q^}𝑦𝑥^𝑞conditional-set𝑐^𝑦𝑅𝑐^𝑞{y}(x;\\hat{q})=\\{c\\in\\hat{y}\\mid R(c)\\geq\\hat{q}\\}italic_y ( italic_x ; over^ start_ARG italic_q end_ARG ) = { italic_c ∈ over^ start_ARG italic_y end_ARG ∣ italic_R ( italic_c ) ≥ over^ start_ARG italic_q end_ARG }.\n\nBased on the relevance scores and annotations generated for each claim across queries in the calibration dataset, we apply CP to calibrate a threshold q^^𝑞\\hat{q}over^ start_ARG italic_q end_ARG. Details on the marginal and conditional CP approaches are given below in section 3.2 and section 3.3. At inference, only queries and documents are available. Sub-claims and relevance scores are generated in the same way as during calibration. Then, claims are removed from the generated answer if their relevance is below the calibrated threshold, creating the conformally factual output y⁢(x;q^)={c∈y^∣R⁢(c)≥q^}𝑦𝑥^𝑞conditional-set𝑐^𝑦𝑅𝑐^𝑞{y}(x;\\hat{q})=\\{c\\in\\hat{y}\\mid R(c)\\geq\\hat{q}\\}italic_y ( italic_x ; over^ start_ARG italic_q end_ARG ) = { italic_c ∈ over^ start_ARG italic_y end_ARG ∣ italic_R ( italic_c ) ≥ over^ start_ARG italic_q end_ARG }.\n\nNote that LLM-generated answers may not always be in the form of clearly separated sub-claims. Following previous work (Mohri and Hashimoto, 2024), we use an LLM to decompose the answer into sub-claims. Similarly, since removing sub-claims may affect the grammatical structure of the overall answer, the final set of claims is fed back into an LLM, which is prompted to merge them into a coherent response.\n\nNote that LLM-generated answers may not always be in the form of clearly separated sub-claims. Following previous work (Mohri and Hashimoto, 2024), we use an LLM to decompose the answer into sub-claims. Similarly, since removing sub-claims may affect the grammatical structure of the overall answer, the final set of claims is fed back into an LLM, which is prompted to merge them into a coherent response.\n\nOur marginal CP calibration builds off of work by Mohri and Hashimoto (2024), but takes advantage of the RAG mechanism through our relevance scoring function. Our aim is to guarantee factuality of generated answers in the sense that the final generated output is entailed by the ground truth answer ytest∗subscriptsuperscript𝑦testy^{*}_{\\text{test}}italic_y start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT test end_POSTSUBSCRIPT with high probability, satisfying eq. 3.\n\nOur marginal CP calibration builds off of work by Mohri and Hashimoto (2024), but takes advantage of the RAG mechanism through our relevance scoring function. Our aim is to guarantee factuality of generated answers in the sense that the final generated output is entailed by the ground truth answer ytest∗subscriptsuperscript𝑦testy^{*}_{\\text{test}}italic_y start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT test end_POSTSUBSCRIPT with high probability, satisfying eq. 3.\n\nWe introduce a filtering function Fq⁢({c})subscript𝐹𝑞𝑐F_{q}(\\{c\\})italic_F start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT ( { italic_c } ) acting on a set of claims, and satisfying both F0⁢({c})={c}subscript𝐹0𝑐𝑐F_{0}(\\{c\\})=\\{c\\}italic_F start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ( { italic_c } ) = { italic_c } and F∞⁢({c})=∅subscript𝐹𝑐F_{\\infty}(\\{c\\})=\\emptysetitalic_F start_POSTSUBSCRIPT ∞ end_POSTSUBSCRIPT ( { italic_c } ) = ∅. As the threshold q𝑞qitalic_q increases from 0, Fqsubscript𝐹𝑞F_{q}italic_F start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT progressively filters out more of the claims, and hence satisfies a nesting property: Fq⁢({c})⊆Fq′⁢({c})subscript𝐹𝑞𝑐subscript𝐹superscript𝑞′𝑐F_{q}(\\{c\\})\\subseteq F_{q^{\\prime}}(\\{c\\})italic_F start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT ( { italic_c } ) ⊆ italic_F start_POSTSUBSCRIPT italic_q start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( { italic_c } ) for q≥q′𝑞superscript𝑞′q\\geq q^{\\prime}italic_q ≥ italic_q start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT (Gupta et al., 2022). The filtering function is constructed using the relevance scores R⁢(c)𝑅𝑐R(c)italic_R ( italic_c ) described in algorithm 1 as (4) Fq⁢(y^)={c∈y^∣R⁢(c)≥q}.subscript𝐹𝑞^𝑦conditional-set𝑐^𝑦𝑅𝑐𝑞F_{q}(\\hat{y})=\\{c\\in\\hat{y}\\mid R(c)\\geq q\\}.italic_F start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT ( over^ start_ARG italic_y end_ARG ) = { italic_c ∈ over^ start_ARG italic_y end_ARG ∣ italic_R ( italic_c ) ≥ italic_q } . To determine the appropriate threshold q𝑞qitalic_q we use CP calibration over the conformal scores (5) S⁢(xi,yi∗):=inf{q∈ℝ+∣∀q′≥q,∀c∈Fq′⁢(y^i),A⁢(c,xi,yi∗,D)=1}.assign𝑆subscript𝑥𝑖superscriptsubscript𝑦𝑖infimumconditional-set𝑞superscriptℝformulae-sequencefor-allsuperscript𝑞′𝑞formulae-sequencefor-all𝑐subscript𝐹superscript𝑞′subscript^𝑦𝑖𝐴𝑐subscript𝑥𝑖superscriptsubscript𝑦𝑖𝐷1S(x_{i},y_{i}^{*}):=\\inf\\{q\\in\\mathbb{R}^{+}\\mid\\forall q^{\\prime}\\geq q,% \\forall c\\in F_{q^{\\prime}}(\\hat{y}_{i}),A(c,x_{i},y_{i}^{*},D)=1\\}.italic_S ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ) := roman_inf { italic_q ∈ blackboard_R start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT ∣ ∀ italic_q start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ≥ italic_q , ∀ italic_c ∈ italic_F start_POSTSUBSCRIPT italic_q start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( over^ start_ARG italic_y end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) , italic_A ( italic_c , italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT , italic_D ) = 1 } . That is, the score S𝑆Sitalic_S is the smallest threshold q𝑞qitalic_q such that all retained claims are considered factual by the annotation function A𝐴Aitalic_A from section 3.1. Then, the conformal threshold q^^𝑞\\hat{q}over^ start_ARG italic_q end_ARG is set as the ⌈(n+1)⁢(1−α)⌉n𝑛11𝛼𝑛\\frac{\\lceil(n+1)(1-\\alpha)\\rceil}{n}divide start_ARG ⌈ ( italic_n + 1 ) ( 1 - italic_α ) ⌉ end_ARG start_ARG italic_n end_ARG quantile of the conformal scores over the calibration set.\n\nWe introduce a filtering function Fq⁢({c})subscript𝐹𝑞𝑐F_{q}(\\{c\\})italic_F start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT ( { italic_c } ) acting on a set of claims, and satisfying both F0⁢({c})={c}subscript𝐹0𝑐𝑐F_{0}(\\{c\\})=\\{c\\}italic_F start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ( { italic_c } ) = { italic_c } and F∞⁢({c})=∅subscript𝐹𝑐F_{\\infty}(\\{c\\})=\\emptysetitalic_F start_POSTSUBSCRIPT ∞ end_POSTSUBSCRIPT ( { italic_c } ) = ∅. As the threshold q𝑞qitalic_q increases from 0, Fqsubscript𝐹𝑞F_{q}italic_F start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT progressively filters out more of the claims, and hence satisfies a nesting property: Fq⁢({c})⊆Fq′⁢({c})subscript𝐹𝑞𝑐subscript𝐹superscript𝑞′𝑐F_{q}(\\{c\\})\\subseteq F_{q^{\\prime}}(\\{c\\})italic_F start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT ( { italic_c } ) ⊆ italic_F start_POSTSUBSCRIPT italic_q start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( { italic_c } ) for q≥q′𝑞superscript𝑞′q\\geq q^{\\prime}italic_q ≥ italic_q start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT (Gupta et al., 2022). The filtering function is constructed using the relevance scores R⁢(c)𝑅𝑐R(c)italic_R ( italic_c ) described in algorithm 1 as\n\nTo determine the appropriate threshold q𝑞qitalic_q we use CP calibration over the conformal scores\n\nThat is, the score S𝑆Sitalic_S is the smallest threshold q𝑞qitalic_q such that all retained claims are considered factual by the annotation function A𝐴Aitalic_A from section 3.1. Then, the conformal threshold q^^𝑞\\hat{q}over^ start_ARG italic_q end_ARG is set as the ⌈(n+1)⁢(1−α)⌉n𝑛11𝛼𝑛\\frac{\\lceil(n+1)(1-\\alpha)\\rceil}{n}divide start_ARG ⌈ ( italic_n + 1 ) ( 1 - italic_α ) ⌉ end_ARG start_ARG italic_n end_ARG quantile of the conformal scores over the calibration set.\n\nOn inference data we filter out claims with relevance score R⁢(c)𝑅𝑐R(c)italic_R ( italic_c ) less than q^^𝑞\\hat{q}over^ start_ARG italic_q end_ARG, i.e. we return ytest=Fq^⁢(y^)subscript𝑦testsubscript𝐹^𝑞^𝑦y_{\\text{test}}=F_{\\hat{q}}(\\hat{y})italic_y start_POSTSUBSCRIPT test end_POSTSUBSCRIPT = italic_F start_POSTSUBSCRIPT over^ start_ARG italic_q end_ARG end_POSTSUBSCRIPT ( over^ start_ARG italic_y end_ARG ). Under the assumption that the annotation function is correct on the calibration data, these sets of filtered claims will satisfy eq. 3 by Theorem 4.1 of Mohri and Hashimoto (2024). The core differences between Conformal-RAG and (Mohri and Hashimoto, 2024) are the relevance function R⁢(c)𝑅𝑐R(c)italic_R ( italic_c ) used for filtering which incorporates similarity information from the RAG mechanism, and the use of automatic annotation to provide ground truth on sub-claim factuality.\n\nOn inference data we filter out claims with relevance score R⁢(c)𝑅𝑐R(c)italic_R ( italic_c ) less than q^^𝑞\\hat{q}over^ start_ARG italic_q end_ARG, i.e. we return ytest=Fq^⁢(y^)subscript𝑦testsubscript𝐹^𝑞^𝑦y_{\\text{test}}=F_{\\hat{q}}(\\hat{y})italic_y start_POSTSUBSCRIPT test end_POSTSUBSCRIPT = italic_F start_POSTSUBSCRIPT over^ start_ARG italic_q end_ARG end_POSTSUBSCRIPT ( over^ start_ARG italic_y end_ARG ). Under the assumption that the annotation function is correct on the calibration data, these sets of filtered claims will satisfy eq. 3 by Theorem 4.1 of Mohri and Hashimoto (2024). The core differences between Conformal-RAG and (Mohri and Hashimoto, 2024) are the relevance function R⁢(c)𝑅𝑐R(c)italic_R ( italic_c ) used for filtering which incorporates similarity information from the RAG mechanism, and the use of automatic annotation to provide ground truth on sub-claim factuality.\n\nPrevious research (Foygel Barber et al., 2020; Gibbs et al., 2023) shows that marginal CP can undercover some groups within the data, while overcovering others, leading to fairness concerns (Romano et al., 2020; Cresswell et al., 2025). To address this, one can aim to provide group-conditional coverage over a pre-specified grouping g:X→G={1⁢…⁢ng}:𝑔→𝑋𝐺1…subscript𝑛𝑔g:X\\to G=\\{1\\dots n_{g}\\}italic_g : italic_X → italic_G = { 1 … italic_n start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT }: (6) ℙ⁢(ytest∗∈Cq^a⁢(xtest)∣g⁢(xtest)=a)≥1−α∀a∈G.formulae-sequenceℙsubscriptsuperscript𝑦testconditionalsubscript𝐶subscript^𝑞𝑎subscript𝑥test𝑔subscript𝑥test𝑎1𝛼for-all𝑎𝐺\\mathbb{P}(y^{*}_{\\text{test}}\\in C_{\\hat{q}_{a}}(x_{\\text{test}})\\mid g(x_{% \\text{test}})=a)\\geq 1-\\alpha\\quad\\forall\\ a\\in G.blackboard_P ( italic_y start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT test end_POSTSUBSCRIPT ∈ italic_C start_POSTSUBSCRIPT over^ start_ARG italic_q end_ARG start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT test end_POSTSUBSCRIPT ) ∣ italic_g ( italic_x start_POSTSUBSCRIPT test end_POSTSUBSCRIPT ) = italic_a ) ≥ 1 - italic_α ∀ italic_a ∈ italic_G . Correspondingly, the conformal threshold q^asubscript^𝑞𝑎\\hat{q}_{a}over^ start_ARG italic_q end_ARG start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT needs to depend on the group attribute a𝑎aitalic_a (e.g. topic category or difficulty of the query).\n\nPrevious research (Foygel Barber et al., 2020; Gibbs et al., 2023) shows that marginal CP can undercover some groups within the data, while overcovering others, leading to fairness concerns (Romano et al., 2020; Cresswell et al., 2025). To address this, one can aim to provide group-conditional coverage over a pre-specified grouping g:X→G={1⁢…⁢ng}:𝑔→𝑋𝐺1…subscript𝑛𝑔g:X\\to G=\\{1\\dots n_{g}\\}italic_g : italic_X → italic_G = { 1 … italic_n start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT }:\n\nCorrespondingly, the conformal threshold q^asubscript^𝑞𝑎\\hat{q}_{a}over^ start_ARG italic_q end_ARG start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT needs to depend on the group attribute a𝑎aitalic_a (e.g. topic category or difficulty of the query).\n\nCherian et al. (2024) proposed to adapt the threshold per test datapoint. First, define the pinball loss ℓα⁢(r):=(1−α)⁢[r]++α⁢[r]−assignsubscriptℓ𝛼𝑟1𝛼subscriptdelimited-[]𝑟𝛼subscriptdelimited-[]𝑟\\ell_{\\alpha}(r):=(1-\\alpha)[r]_{+}+\\alpha[r]_{-}roman_ℓ start_POSTSUBSCRIPT italic_α end_POSTSUBSCRIPT ( italic_r ) := ( 1 - italic_α ) [ italic_r ] start_POSTSUBSCRIPT + end_POSTSUBSCRIPT + italic_α [ italic_r ] start_POSTSUBSCRIPT - end_POSTSUBSCRIPT. Then, the threshold specific to datapoint xtestsubscript𝑥testx_{\\text{test}}italic_x start_POSTSUBSCRIPT test end_POSTSUBSCRIPT is determined by the function ftest:G→ℝ:subscript𝑓test→𝐺ℝf_{\\text{test}}:G\\to\\mathbb{R}italic_f start_POSTSUBSCRIPT test end_POSTSUBSCRIPT : italic_G → blackboard_R defined as (7) ftest=arg⁡minf∈ℱ⁢1n+1⁢[∑i=1nℓα⁢(Si−f⁢(g⁢(xi)))+ℓα⁢(Stest−f⁢(g⁢(xtest)))]subscript𝑓test𝑓ℱ1𝑛1delimited-[]superscriptsubscript𝑖1𝑛subscriptℓ𝛼subscript𝑆𝑖𝑓𝑔subscript𝑥𝑖subscriptℓ𝛼subscript𝑆test𝑓𝑔subscript𝑥testf_{\\text{test}}=\\underset{f\\in\\mathcal{F}}{\\arg\\min}\\frac{1}{n+1}\\big{[}\\sum_{% i=1}^{n}\\ell_{\\alpha}\\big{(}S_{i}-f(g(x_{i}))\\big{)}+\\ell_{\\alpha}\\big{(}S_{% \\text{test}}-f(g(x_{\\text{test}}))\\big{)}\\big{]}italic_f start_POSTSUBSCRIPT test end_POSTSUBSCRIPT = start_UNDERACCENT italic_f ∈ caligraphic_F end_UNDERACCENT start_ARG roman_arg roman_min end_ARG divide start_ARG 1 end_ARG start_ARG italic_n + 1 end_ARG [ ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT roman_ℓ start_POSTSUBSCRIPT italic_α end_POSTSUBSCRIPT ( italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - italic_f ( italic_g ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) ) ) + roman_ℓ start_POSTSUBSCRIPT italic_α end_POSTSUBSCRIPT ( italic_S start_POSTSUBSCRIPT test end_POSTSUBSCRIPT - italic_f ( italic_g ( italic_x start_POSTSUBSCRIPT test end_POSTSUBSCRIPT ) ) ) ] where Si=S⁢(xi,y^i)subscript𝑆𝑖𝑆subscript𝑥𝑖subscript^𝑦𝑖S_{i}=S(x_{i},\\hat{y}_{i})italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_S ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , over^ start_ARG italic_y end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) (eq. 5), Stestsubscript𝑆testS_{\\text{test}}italic_S start_POSTSUBSCRIPT test end_POSTSUBSCRIPT is imputed using quantile regression, and the optimization is over the family of linear functions ℱ={f⁢(a)=β⊤⁢ea}ℱ𝑓𝑎superscript𝛽topsubscript𝑒𝑎\\mathcal{F}=\\{f(a)=\\beta^{\\top}e_{a}\\}caligraphic_F = { italic_f ( italic_a ) = italic_β start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT italic_e start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT } for β∈ℝ|G|𝛽superscriptℝ𝐺\\beta\\in\\mathbb{R}^{|G|}italic_β ∈ blackboard_R start_POSTSUPERSCRIPT | italic_G | end_POSTSUPERSCRIPT and easubscript𝑒𝑎e_{a}italic_e start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT a basis vector of ℝ|G|superscriptℝ𝐺\\mathbb{R}^{|G|}blackboard_R start_POSTSUPERSCRIPT | italic_G | end_POSTSUPERSCRIPT. The learned function ftestsubscript𝑓testf_{\\text{test}}italic_f start_POSTSUBSCRIPT test end_POSTSUBSCRIPT provides the adapted conformal quantile q^test=ftest⁢(xtest)subscript^𝑞testsubscript𝑓testsubscript𝑥test\\hat{q}_{\\text{test}}=f_{\\text{test}}(x_{\\text{test}})over^ start_ARG italic_q end_ARG start_POSTSUBSCRIPT test end_POSTSUBSCRIPT = italic_f start_POSTSUBSCRIPT test end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT test end_POSTSUBSCRIPT ) which is used to filter out claims, i.e. the method returns ytest=Fq^test⁢(y^)subscript𝑦testsubscript𝐹subscript^𝑞test^𝑦y_{\\text{test}}=F_{\\hat{q}_{\\text{test}}}(\\hat{y})italic_y start_POSTSUBSCRIPT test end_POSTSUBSCRIPT = italic_F start_POSTSUBSCRIPT over^ start_ARG italic_q end_ARG start_POSTSUBSCRIPT test end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( over^ start_ARG italic_y end_ARG ) as in eq. 4. This procedure satisfies group-conditional factuality (Cherian et al., 2024), (8) ℙ⁢(ytest∗⇒ytest⁢(xtest;q^)∣g⁢(xtest)=a)≥1−α∀a∈G.formulae-sequenceℙ⇒subscriptsuperscript𝑦testconditionalsubscript𝑦testsubscript𝑥test^𝑞𝑔subscript𝑥test𝑎1𝛼for-all𝑎𝐺\\mathbb{P}(y^{*}_{\\text{test}}\\Rightarrow y_{\\text{test}}(x_{\\text{test}};\\hat% {q})\\mid g(x_{\\text{test}})=a)\\geq 1-\\alpha\\quad\\forall\\ a\\in G.blackboard_P ( italic_y start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT test end_POSTSUBSCRIPT ⇒ italic_y start_POSTSUBSCRIPT test end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT test end_POSTSUBSCRIPT ; over^ start_ARG italic_q end_ARG ) ∣ italic_g ( italic_x start_POSTSUBSCRIPT test end_POSTSUBSCRIPT ) = italic_a ) ≥ 1 - italic_α ∀ italic_a ∈ italic_G . However, this method borders on impractical as it requires both a quantile regression to impute Stestsubscript𝑆testS_{\\text{test}}italic_S start_POSTSUBSCRIPT test end_POSTSUBSCRIPT, and an optimization over ℱℱ\\mathcal{F}caligraphic_F for every inference datapoint. To simplify these procedures, Conformal-RAG follows the Mondrian CP paradigm (Vovk et al., 2003, 2005) which first partitions the calibration data by groups using g𝑔gitalic_g, then calibrates a distinct threshold q^asubscript^𝑞𝑎\\hat{q}_{a}over^ start_ARG italic_q end_ARG start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT for each a∈G𝑎𝐺a\\in Gitalic_a ∈ italic_G using the procedure in section 3.2. At inference time, the threshold for group atest=g⁢(xtest)subscript𝑎test𝑔subscript𝑥testa_{\\text{test}}=g(x_{\\text{test}})italic_a start_POSTSUBSCRIPT test end_POSTSUBSCRIPT = italic_g ( italic_x start_POSTSUBSCRIPT test end_POSTSUBSCRIPT ) is used for filtering out claims, i.e. we return ytest=Fq^atest⁢(y^)subscript𝑦testsubscript𝐹subscript^𝑞subscript𝑎test^𝑦y_{\\text{test}}=F_{\\hat{q}_{a_{\\text{test}}}}(\\hat{y})italic_y start_POSTSUBSCRIPT test end_POSTSUBSCRIPT = italic_F start_POSTSUBSCRIPT over^ start_ARG italic_q end_ARG start_POSTSUBSCRIPT italic_a start_POSTSUBSCRIPT test end_POSTSUBSCRIPT end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( over^ start_ARG italic_y end_ARG ). Since each group is calibrated independently, eq. 3 holds for each group, which implies eq. 8.\n\nCherian et al. (2024) proposed to adapt the threshold per test datapoint. First, define the pinball loss ℓα⁢(r):=(1−α)⁢[r]++α⁢[r]−assignsubscriptℓ𝛼𝑟1𝛼subscriptdelimited-[]𝑟𝛼subscriptdelimited-[]𝑟\\ell_{\\alpha}(r):=(1-\\alpha)[r]_{+}+\\alpha[r]_{-}roman_ℓ start_POSTSUBSCRIPT italic_α end_POSTSUBSCRIPT ( italic_r ) := ( 1 - italic_α ) [ italic_r ] start_POSTSUBSCRIPT + end_POSTSUBSCRIPT + italic_α [ italic_r ] start_POSTSUBSCRIPT - end_POSTSUBSCRIPT. Then, the threshold specific to datapoint xtestsubscript𝑥testx_{\\text{test}}italic_x start_POSTSUBSCRIPT test end_POSTSUBSCRIPT is determined by the function ftest:G→ℝ:subscript𝑓test→𝐺ℝf_{\\text{test}}:G\\to\\mathbb{R}italic_f start_POSTSUBSCRIPT test end_POSTSUBSCRIPT : italic_G → blackboard_R defined as\n\nwhere Si=S⁢(xi,y^i)subscript𝑆𝑖𝑆subscript𝑥𝑖subscript^𝑦𝑖S_{i}=S(x_{i},\\hat{y}_{i})italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_S ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , over^ start_ARG italic_y end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) (eq. 5), Stestsubscript𝑆testS_{\\text{test}}italic_S start_POSTSUBSCRIPT test end_POSTSUBSCRIPT is imputed using quantile regression, and the optimization is over the family of linear functions ℱ={f⁢(a)=β⊤⁢ea}ℱ𝑓𝑎superscript𝛽topsubscript𝑒𝑎\\mathcal{F}=\\{f(a)=\\beta^{\\top}e_{a}\\}caligraphic_F = { italic_f ( italic_a ) = italic_β start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT italic_e start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT } for β∈ℝ|G|𝛽superscriptℝ𝐺\\beta\\in\\mathbb{R}^{|G|}italic_β ∈ blackboard_R start_POSTSUPERSCRIPT | italic_G | end_POSTSUPERSCRIPT and easubscript𝑒𝑎e_{a}italic_e start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT a basis vector of ℝ|G|superscriptℝ𝐺\\mathbb{R}^{|G|}blackboard_R start_POSTSUPERSCRIPT | italic_G | end_POSTSUPERSCRIPT. The learned function ftestsubscript𝑓testf_{\\text{test}}italic_f start_POSTSUBSCRIPT test end_POSTSUBSCRIPT provides the adapted conformal quantile q^test=ftest⁢(xtest)subscript^𝑞testsubscript𝑓testsubscript𝑥test\\hat{q}_{\\text{test}}=f_{\\text{test}}(x_{\\text{test}})over^ start_ARG italic_q end_ARG start_POSTSUBSCRIPT test end_POSTSUBSCRIPT = italic_f start_POSTSUBSCRIPT test end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT test end_POSTSUBSCRIPT ) which is used to filter out claims, i.e. the method returns ytest=Fq^test⁢(y^)subscript𝑦testsubscript𝐹subscript^𝑞test^𝑦y_{\\text{test}}=F_{\\hat{q}_{\\text{test}}}(\\hat{y})italic_y start_POSTSUBSCRIPT test end_POSTSUBSCRIPT = italic_F start_POSTSUBSCRIPT over^ start_ARG italic_q end_ARG start_POSTSUBSCRIPT test end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( over^ start_ARG italic_y end_ARG ) as in eq. 4. This procedure satisfies group-conditional factuality (Cherian et al., 2024),\n\nHowever, this method borders on impractical as it requires both a quantile regression to impute Stestsubscript𝑆testS_{\\text{test}}italic_S start_POSTSUBSCRIPT test end_POSTSUBSCRIPT, and an optimization over ℱℱ\\mathcal{F}caligraphic_F for every inference datapoint. To simplify these procedures, Conformal-RAG follows the Mondrian CP paradigm (Vovk et al., 2003, 2005) which first partitions the calibration data by groups using g𝑔gitalic_g, then calibrates a distinct threshold q^asubscript^𝑞𝑎\\hat{q}_{a}over^ start_ARG italic_q end_ARG start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT for each a∈G𝑎𝐺a\\in Gitalic_a ∈ italic_G using the procedure in section 3.2. At inference time, the threshold for group atest=g⁢(xtest)subscript𝑎test𝑔subscript𝑥testa_{\\text{test}}=g(x_{\\text{test}})italic_a start_POSTSUBSCRIPT test end_POSTSUBSCRIPT = italic_g ( italic_x start_POSTSUBSCRIPT test end_POSTSUBSCRIPT ) is used for filtering out claims, i.e. we return ytest=Fq^atest⁢(y^)subscript𝑦testsubscript𝐹subscript^𝑞subscript𝑎test^𝑦y_{\\text{test}}=F_{\\hat{q}_{a_{\\text{test}}}}(\\hat{y})italic_y start_POSTSUBSCRIPT test end_POSTSUBSCRIPT = italic_F start_POSTSUBSCRIPT over^ start_ARG italic_q end_ARG start_POSTSUBSCRIPT italic_a start_POSTSUBSCRIPT test end_POSTSUBSCRIPT end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( over^ start_ARG italic_y end_ARG ). Since each group is calibrated independently, eq. 3 holds for each group, which implies eq. 8.",
        "conclusion": "This paper introduced Conformal-RAG, a novel framework that applies conformal prediction (CP) to enhance RAG systems. An extension of Conformal-RAG to conditional CP ensures group-conditional coverage across multiple sub-domains without requiring manual annotation of conformal sets, making it well-suited for complex RAG applications. Experimental results showed that Conformal-RAG and its conditional extension retain up to 60% more high-quality sub-claims than direct applications of CP to LLMs, while maintaining the same factuality guarantees.\n\nThis paper introduced Conformal-RAG, a novel framework that applies conformal prediction (CP) to enhance RAG systems. An extension of Conformal-RAG to conditional CP ensures group-conditional coverage across multiple sub-domains without requiring manual annotation of conformal sets, making it well-suited for complex RAG applications. Experimental results showed that Conformal-RAG and its conditional extension retain up to 60% more high-quality sub-claims than direct applications of CP to LLMs, while maintaining the same factuality guarantees."
      }
    }
  ]
}